{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "import optuna.integration.lightgbm as optuna_lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp070_hyp_opt\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.1, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.prep_dir + 'cat_col_lists.pkl', 'rb') as f:\n",
    "    cat_col_lists = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTrain:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"session_id\", \"level_group\", \"elapsed_time\"], ignore_index=True)\n",
    "        self.features = self.sessions_df[[\"session_id\", \"level_group\"]].drop_duplicates().copy()\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].shift(1)\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_record_cnt\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].agg([max,min]).reset_index()\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[\"max\"] - add_features[\"min\"]\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[f\"{self.group}_group_elapsed_time\"].astype(np.float32)\n",
    "        add_features = add_features[[\"session_id\", \"level_group\", f\"{self.group}_group_elapsed_time\"]].copy()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[\"index\"].count().reset_index().rename(columns={\"index\":\"cnt\"})\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            tmp = add_features[add_features[cat_col]==cat][[\"session_id\", \"level_group\", \"cnt\"]].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp = tmp.rename(columns={\"cnt\": feat_name})\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[feat_name] = self.features[feat_name].fillna(0)\n",
    "            else:\n",
    "                self.features[feat_name] = 0\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.dropna(subset=[cat_col]).drop_duplicates([\"session_id\", \"level_group\", cat_col])\n",
    "        add_features = add_features.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_{cat_col}_nunique\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")        \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        new_cols = [f\"{self.group}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{self.group}_{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "    def get_train(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "        \n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        \n",
    "        self.result = self.result.merge(self.features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesInf:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"elapsed_time\"], ignore_index=True)\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.use_cols = [\n",
    "            \"elapsed_time\", \"event_name\", \"name\", \"level\", \"page\", \"index\",\n",
    "            \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\",\n",
    "            \"hover_duration\", \"text\", \"fqid\", \"room_fqid\", \"text_fqid\"\n",
    "        ]\n",
    "\n",
    "    def _prep(self):\n",
    "        # dataframeの各列をnumpy arrayで保持\n",
    "        self.sessions = {}\n",
    "        for c in self.use_cols:\n",
    "            self.sessions[c] = self.sessions_df[c].values\n",
    "        self.sessions[\"time_diff\"] = self.sessions[\"elapsed_time\"] - self.sessions_df[\"elapsed_time\"].shift(1).values\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_feature = len(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_record_cnt\"] = add_feature\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_feature = np.max(self.sessions[\"elapsed_time\"]) - np.min(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_group_elapsed_time\"] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            add_feature = (self.sessions[cat_col] == cat).astype(int).sum()\n",
    "            self.result[feat_name] = add_feature\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        self.result[f\"{self.group}_{cat_col}_nunique\"] = self.sessions_df[cat_col].dropna().nunique()       \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        for val_col, agg in itertools.product(val_cols, aggs):\n",
    "            feat_name = f\"{self.group}_{val_col}_{agg}\"\n",
    "            if agg == \"mean\":\n",
    "                add_feature = np.nanmean(self.sessions[val_col])\n",
    "            elif agg == \"max\":\n",
    "                add_feature = np.nanmax(self.sessions[val_col])\n",
    "            elif agg == \"min\":\n",
    "                add_feature = np.nanmin(self.sessions[val_col])\n",
    "            elif agg == \"std\":\n",
    "                add_feature = np.nanstd(self.sessions[val_col], ddof=1)\n",
    "            elif agg == \"sum\":\n",
    "                add_feature = np.nansum(self.sessions[val_col])\n",
    "            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    tmp = self.sessions[val_col][idx]\n",
    "                    if agg == \"mean\":\n",
    "                        add_feature = np.nanmean(tmp)\n",
    "                    elif agg == \"max\":\n",
    "                        add_feature = np.nanmax(tmp)\n",
    "                    elif agg == \"min\":\n",
    "                        add_feature = np.nanmin(tmp)\n",
    "                    elif agg == \"std\":\n",
    "                        add_feature = np.nanstd(tmp, ddof=1)\n",
    "                    elif agg == \"sum\":\n",
    "                        add_feature = np.nansum(tmp)\n",
    "                    if np.isnan(add_feature):\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def get_test(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "\n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesTrain(sessions, labels)\n",
    "    train = feat.get_train()\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesInf(sessions, labels)\n",
    "    test = feat.get_test()\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    dfs = []\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train_group = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train_group = train_group.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train_group[\"0-4_question_duration_time\"] = train_group[\"5-12_elapsed_time_min\"] - train_group[\"0-4_elapsed_time_max\"]\n",
    "            train_group[\"0-4_question_duration_index\"] = train_group[\"5-12_index_min\"] - train_group[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train_group[\"5-12_question_duration_time\"] = train_group[\"13-22_elapsed_time_min\"] - train_group[\"5-12_elapsed_time_max\"]\n",
    "            train_group[\"5-12_question_duration_index\"] = train_group[\"13-22_index_min\"] - train_group[\"5-12_index_max\"]\n",
    "    \n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train_group.columns if c not in not_use_cols]\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train_group[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "\n",
    "        dfs.append(train_group)\n",
    "    train = pd.concat(dfs, ignore_index=True)\n",
    "    # concatするとcategory型がリセットされてしまうので再度cast\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    target = \"correct\"\n",
    "    not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "    features = [c for c in train.columns if c not in not_use_cols]    \n",
    "\n",
    "    gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "    fis = []\n",
    "    \n",
    "    for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "        oof_groups = []\n",
    "        print(f\"fold : {i}\")\n",
    "        tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "        vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)\n",
    "        # モデル出力\n",
    "        model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{i}.lgb\")\n",
    "    \n",
    "        # valid_pred\n",
    "        oof_fold = train.iloc[vl_idx].copy()\n",
    "        oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oofs.append(oof_fold)\n",
    "\n",
    "        # 特徴量重要度\n",
    "        fi_fold = pd.DataFrame()\n",
    "        fi_fold[\"feature\"] = model.feature_name()\n",
    "        fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "        fi_fold[\"fold\"] = i\n",
    "        fis.append(fi_fold)\n",
    "\n",
    "    fi = pd.concat(fis)    \n",
    "    fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "    fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi.csv\", index=False)\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"level_group2\"] = test[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"level_group2\"] = sub[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in test.groupby(\"level_group2\")]\n",
    "    subs = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in sub.groupby(\"level_group2\")]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    models = []\n",
    "    for i in range(cfg.n_splits):\n",
    "        if mode == \"local_cv\":\n",
    "            model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{i}.lgb\"\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_path = f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{i}.lgb\"\n",
    "        models.append(lgb.Booster(model_file=model_path))\n",
    "    use_features = models[0].feature_name()\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in test.columns if c not in not_use_cols]\n",
    "\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        \n",
    "        # そのlevel_group時点で存在しない列を追加\n",
    "        complement_features = list(set(use_features) - set(test.columns.tolist()))\n",
    "        test[complement_features] = np.nan\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = models[i]\n",
    "            preds += model.predict(test[use_features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        test[\"pred\"] = preds\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4\n",
      "5-12\n",
      "fold : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 23:57:11,566]\u001b[0m A new study created in memory with name: no-name-ffd5848f-ff56-4e9e-a063-4c503780a156\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.469526 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.507636\tvalid_1's binary_logloss: 0.535095\n",
      "[200]\tvalid_0's binary_logloss: 0.485936\tvalid_1's binary_logloss: 0.534311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.534095:  14%|#4        | 1/7 [03:43<22:23, 223.93s/it]\u001b[32m[I 2023-05-31 00:00:55,535]\u001b[0m Trial 0 finished with value: 0.5340947129714599 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.5340947129714599.\u001b[0m\n",
      "feature_fraction, val_score: 0.534095:  14%|#4        | 1/7 [03:43<22:23, 223.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[153]\tvalid_0's binary_logloss: 0.494665\tvalid_1's binary_logloss: 0.534095\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.063732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.505179\tvalid_1's binary_logloss: 0.535473\n",
      "[200]\tvalid_0's binary_logloss: 0.483392\tvalid_1's binary_logloss: 0.534903\n",
      "[300]\tvalid_0's binary_logloss: 0.466536\tvalid_1's binary_logloss: 0.53531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.534095:  29%|##8       | 2/7 [10:41<28:08, 337.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[202]\tvalid_0's binary_logloss: 0.483022\tvalid_1's binary_logloss: 0.534846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 00:07:53,525]\u001b[0m Trial 1 finished with value: 0.5348459448227766 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.5340947129714599.\u001b[0m\n",
      "feature_fraction, val_score: 0.534095:  29%|##8       | 2/7 [10:41<28:08, 337.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.267919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.505704\tvalid_1's binary_logloss: 0.535801\n",
      "[200]\tvalid_0's binary_logloss: 0.484094\tvalid_1's binary_logloss: 0.535541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.534095:  43%|####2     | 3/7 [15:54<21:45, 326.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's binary_logloss: 0.4947\tvalid_1's binary_logloss: 0.535146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 00:13:06,153]\u001b[0m Trial 2 finished with value: 0.5351458536456757 and parameters: {'feature_fraction': 0.8}. Best is trial 0 with value: 0.5340947129714599.\u001b[0m\n",
      "feature_fraction, val_score: 0.534095:  43%|####2     | 3/7 [15:54<21:45, 326.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.892761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.506268\tvalid_1's binary_logloss: 0.535571\n",
      "[200]\tvalid_0's binary_logloss: 0.484312\tvalid_1's binary_logloss: 0.53501\n",
      "[300]\tvalid_0's binary_logloss: 0.46826\tvalid_1's binary_logloss: 0.53566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.534095:  57%|#####7    | 4/7 [21:07<16:03, 321.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[212]\tvalid_0's binary_logloss: 0.482165\tvalid_1's binary_logloss: 0.534997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 00:18:19,255]\u001b[0m Trial 3 finished with value: 0.534996699955248 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.5340947129714599.\u001b[0m\n",
      "feature_fraction, val_score: 0.534095:  57%|#####7    | 4/7 [21:07<16:03, 321.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.348156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.506043\tvalid_1's binary_logloss: 0.535823\n",
      "[200]\tvalid_0's binary_logloss: 0.483963\tvalid_1's binary_logloss: 0.535058\n",
      "[300]\tvalid_0's binary_logloss: 0.467876\tvalid_1's binary_logloss: 0.535592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.534095:  71%|#######1  | 5/7 [27:54<11:44, 352.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[218]\tvalid_0's binary_logloss: 0.480838\tvalid_1's binary_logloss: 0.534967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 00:25:06,588]\u001b[0m Trial 4 finished with value: 0.5349667318015209 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.5340947129714599.\u001b[0m\n",
      "feature_fraction, val_score: 0.534095:  71%|#######1  | 5/7 [27:55<11:44, 352.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.840837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.504972\tvalid_1's binary_logloss: 0.53576\n",
      "[200]\tvalid_0's binary_logloss: 0.482762\tvalid_1's binary_logloss: 0.535058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.534095:  86%|########5 | 6/7 [33:40<05:50, 350.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's binary_logloss: 0.492894\tvalid_1's binary_logloss: 0.534905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 00:30:52,891]\u001b[0m Trial 5 finished with value: 0.5349049289675214 and parameters: {'feature_fraction': 1.0}. Best is trial 0 with value: 0.5340947129714599.\u001b[0m\n",
      "feature_fraction, val_score: 0.534095:  86%|########5 | 6/7 [33:41<05:50, 350.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.732887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.50672\tvalid_1's binary_logloss: 0.535851\n",
      "[200]\tvalid_0's binary_logloss: 0.48504\tvalid_1's binary_logloss: 0.53574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.534095: 100%|##########| 7/7 [37:46<00:00, 316.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's binary_logloss: 0.499076\tvalid_1's binary_logloss: 0.535511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 00:34:58,659]\u001b[0m Trial 6 finished with value: 0.5355109116637381 and parameters: {'feature_fraction': 0.5}. Best is trial 0 with value: 0.5340947129714599.\u001b[0m\n",
      "feature_fraction, val_score: 0.534095: 100%|##########| 7/7 [37:47<00:00, 323.87s/it]\n",
      "num_leaves, val_score: 0.534095:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.364256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.424789\tvalid_1's binary_logloss: 0.537046\n",
      "[200]\tvalid_0's binary_logloss: 0.378527\tvalid_1's binary_logloss: 0.539578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.534095:   5%|5         | 1/20 [12:04<3:49:34, 724.95s/it]\u001b[32m[I 2023-05-31 00:47:03,651]\u001b[0m Trial 7 finished with value: 0.5368133979903189 and parameters: {'num_leaves': 195}. Best is trial 7 with value: 0.5368133979903189.\u001b[0m\n",
      "num_leaves, val_score: 0.534095:   5%|5         | 1/20 [12:04<3:49:34, 724.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[109]\tvalid_0's binary_logloss: 0.419574\tvalid_1's binary_logloss: 0.536813\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.471452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.467977\tvalid_1's binary_logloss: 0.53586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.534095:  10%|#         | 2/20 [17:09<2:23:13, 477.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's binary_logloss: 0.430946\tvalid_1's binary_logloss: 0.53712\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.467977\tvalid_1's binary_logloss: 0.53586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 00:52:08,213]\u001b[0m Trial 8 finished with value: 0.5358598323393428 and parameters: {'num_leaves': 91}. Best is trial 8 with value: 0.5358598323393428.\u001b[0m\n",
      "num_leaves, val_score: 0.534095:  10%|#         | 2/20 [17:09<2:23:13, 477.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.553062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.424254\tvalid_1's binary_logloss: 0.537654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.534095:  15%|#5        | 3/20 [24:30<2:10:36, 460.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's binary_logloss: 0.445161\tvalid_1's binary_logloss: 0.537467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 00:59:29,518]\u001b[0m Trial 9 finished with value: 0.5374671113773618 and parameters: {'num_leaves': 196}. Best is trial 8 with value: 0.5358598323393428.\u001b[0m\n",
      "num_leaves, val_score: 0.534095:  15%|#5        | 3/20 [24:30<2:10:36, 460.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.287051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.418101\tvalid_1's binary_logloss: 0.537375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.534095:  20%|##        | 4/20 [31:42<1:59:51, 449.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[74]\tvalid_0's binary_logloss: 0.438247\tvalid_1's binary_logloss: 0.537162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 01:06:41,763]\u001b[0m Trial 10 finished with value: 0.5371615552603703 and parameters: {'num_leaves': 216}. Best is trial 8 with value: 0.5358598323393428.\u001b[0m\n",
      "num_leaves, val_score: 0.534095:  20%|##        | 4/20 [31:43<1:59:51, 449.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.661838 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.474756\tvalid_1's binary_logloss: 0.535694\n",
      "[200]\tvalid_0's binary_logloss: 0.439759\tvalid_1's binary_logloss: 0.536089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.534095:  25%|##5       | 5/20 [36:23<1:37:11, 388.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[143]\tvalid_0's binary_logloss: 0.456833\tvalid_1's binary_logloss: 0.535439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 01:11:22,670]\u001b[0m Trial 11 finished with value: 0.5354389282588291 and parameters: {'num_leaves': 78}. Best is trial 11 with value: 0.5354389282588291.\u001b[0m\n",
      "num_leaves, val_score: 0.534095:  25%|##5       | 5/20 [36:24<1:37:11, 388.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 2.089761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.516156\tvalid_1's binary_logloss: 0.535869\n",
      "[200]\tvalid_0's binary_logloss: 0.498177\tvalid_1's binary_logloss: 0.534384\n",
      "[300]\tvalid_0's binary_logloss: 0.485693\tvalid_1's binary_logloss: 0.534599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.534095:  30%|###       | 6/20 [42:33<1:29:14, 382.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[214]\tvalid_0's binary_logloss: 0.496206\tvalid_1's binary_logloss: 0.534184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 01:17:32,790]\u001b[0m Trial 12 finished with value: 0.5341838679562143 and parameters: {'num_leaves': 22}. Best is trial 12 with value: 0.5341838679562143.\u001b[0m\n",
      "num_leaves, val_score: 0.534095:  30%|###       | 6/20 [42:34<1:29:14, 382.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.958341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.506767\tvalid_1's binary_logloss: 0.534953\n",
      "[200]\tvalid_0's binary_logloss: 0.484515\tvalid_1's binary_logloss: 0.533585\n",
      "[300]\tvalid_0's binary_logloss: 0.468642\tvalid_1's binary_logloss: 0.533368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533245:  35%|###5      | 7/20 [50:42<1:30:24, 417.24s/it]\u001b[32m[I 2023-05-31 01:25:41,201]\u001b[0m Trial 13 finished with value: 0.5332450356212636 and parameters: {'num_leaves': 32}. Best is trial 13 with value: 0.5332450356212636.\u001b[0m\n",
      "num_leaves, val_score: 0.533245:  35%|###5      | 7/20 [50:42<1:30:24, 417.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[235]\tvalid_0's binary_logloss: 0.478202\tvalid_1's binary_logloss: 0.533245\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.411014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.43626\tvalid_1's binary_logloss: 0.536241\n",
      "[200]\tvalid_0's binary_logloss: 0.392696\tvalid_1's binary_logloss: 0.539063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533245:  40%|####      | 8/20 [1:01:05<1:36:33, 482.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[101]\tvalid_0's binary_logloss: 0.435862\tvalid_1's binary_logloss: 0.536142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 01:36:04,744]\u001b[0m Trial 14 finished with value: 0.5361417367024051 and parameters: {'num_leaves': 163}. Best is trial 13 with value: 0.5332450356212636.\u001b[0m\n",
      "num_leaves, val_score: 0.533245:  40%|####      | 8/20 [1:01:06<1:36:33, 482.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.398988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.520349\tvalid_1's binary_logloss: 0.536278\n",
      "[200]\tvalid_0's binary_logloss: 0.504503\tvalid_1's binary_logloss: 0.535177\n",
      "[300]\tvalid_0's binary_logloss: 0.492918\tvalid_1's binary_logloss: 0.534706\n",
      "[400]\tvalid_0's binary_logloss: 0.483439\tvalid_1's binary_logloss: 0.534974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533245:  45%|####5     | 9/20 [1:08:16<1:25:30, 466.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[303]\tvalid_0's binary_logloss: 0.492632\tvalid_1's binary_logloss: 0.534678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 01:43:15,136]\u001b[0m Trial 15 finished with value: 0.5346775983945998 and parameters: {'num_leaves': 18}. Best is trial 13 with value: 0.5332450356212636.\u001b[0m\n",
      "num_leaves, val_score: 0.533245:  45%|####5     | 9/20 [1:08:16<1:25:30, 466.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.816636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.465717\tvalid_1's binary_logloss: 0.535042\n",
      "[200]\tvalid_0's binary_logloss: 0.42795\tvalid_1's binary_logloss: 0.536257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533245:  50%|#####     | 10/20 [1:13:27<1:09:44, 418.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's binary_logloss: 0.454007\tvalid_1's binary_logloss: 0.534946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-31 01:48:26,123]\u001b[0m Trial 16 finished with value: 0.5349458040387735 and parameters: {'num_leaves': 95}. Best is trial 13 with value: 0.5332450356212636.\u001b[0m\n",
      "num_leaves, val_score: 0.533245:  50%|#####     | 10/20 [1:13:27<1:09:44, 418.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.847147 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466141\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009:  55%|#####5    | 11/20 [1:17:50<55:38, 370.97s/it]  \u001b[32m[I 2023-05-31 01:52:49,079]\u001b[0m Trial 17 finished with value: 0.5330088247443111 and parameters: {'num_leaves': 48}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009:  55%|#####5    | 11/20 [1:17:50<55:38, 370.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.47225\tvalid_1's binary_logloss: 0.533009\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.851475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.487281\tvalid_1's binary_logloss: 0.535772\n",
      "[200]\tvalid_0's binary_logloss: 0.456103\tvalid_1's binary_logloss: 0.535933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009:  60%|######    | 12/20 [1:22:17<45:13, 339.22s/it]\u001b[32m[I 2023-05-31 01:57:15,706]\u001b[0m Trial 18 finished with value: 0.5355907648661028 and parameters: {'num_leaves': 58}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009:  60%|######    | 12/20 [1:22:17<45:13, 339.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's binary_logloss: 0.478264\tvalid_1's binary_logloss: 0.535591\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.414072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.542749\tvalid_1's binary_logloss: 0.544969\n",
      "[200]\tvalid_0's binary_logloss: 0.535549\tvalid_1's binary_logloss: 0.540074\n",
      "[300]\tvalid_0's binary_logloss: 0.531398\tvalid_1's binary_logloss: 0.53848\n",
      "[400]\tvalid_0's binary_logloss: 0.528022\tvalid_1's binary_logloss: 0.53764\n",
      "[500]\tvalid_0's binary_logloss: 0.525018\tvalid_1's binary_logloss: 0.537292\n",
      "[600]\tvalid_0's binary_logloss: 0.522332\tvalid_1's binary_logloss: 0.536898\n",
      "[700]\tvalid_0's binary_logloss: 0.519843\tvalid_1's binary_logloss: 0.536652\n",
      "[800]\tvalid_0's binary_logloss: 0.51745\tvalid_1's binary_logloss: 0.536383\n",
      "[900]\tvalid_0's binary_logloss: 0.515286\tvalid_1's binary_logloss: 0.536266\n",
      "[1000]\tvalid_0's binary_logloss: 0.513252\tvalid_1's binary_logloss: 0.53606\n",
      "[1100]\tvalid_0's binary_logloss: 0.511277\tvalid_1's binary_logloss: 0.536051\n",
      "[1200]\tvalid_0's binary_logloss: 0.509416\tvalid_1's binary_logloss: 0.535992\n",
      "[1300]\tvalid_0's binary_logloss: 0.507564\tvalid_1's binary_logloss: 0.535889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009:  65%|######5   | 13/20 [1:32:15<48:44, 417.82s/it]\u001b[32m[I 2023-05-31 02:07:14,369]\u001b[0m Trial 19 finished with value: 0.5358669599321315 and parameters: {'num_leaves': 4}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009:  65%|######5   | 13/20 [1:32:15<48:44, 417.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1241]\tvalid_0's binary_logloss: 0.50862\tvalid_1's binary_logloss: 0.535867\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.001880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.4521\tvalid_1's binary_logloss: 0.536011\n",
      "[200]\tvalid_0's binary_logloss: 0.410878\tvalid_1's binary_logloss: 0.537622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009:  70%|#######   | 14/20 [1:37:30<38:40, 386.83s/it]\u001b[32m[I 2023-05-31 02:12:29,588]\u001b[0m Trial 20 finished with value: 0.5359761789224295 and parameters: {'num_leaves': 123}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009:  70%|#######   | 14/20 [1:37:30<38:40, 386.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[101]\tvalid_0's binary_logloss: 0.451654\tvalid_1's binary_logloss: 0.535976\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.752045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.493501\tvalid_1's binary_logloss: 0.534881\n",
      "[200]\tvalid_0's binary_logloss: 0.464937\tvalid_1's binary_logloss: 0.53409\n",
      "[300]\tvalid_0's binary_logloss: 0.44521\tvalid_1's binary_logloss: 0.53449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009:  75%|#######5  | 15/20 [1:42:56<30:42, 368.44s/it]\u001b[32m[I 2023-05-31 02:17:55,426]\u001b[0m Trial 21 finished with value: 0.5339669167328602 and parameters: {'num_leaves': 49}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009:  75%|#######5  | 15/20 [1:42:56<30:42, 368.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[213]\tvalid_0's binary_logloss: 0.461923\tvalid_1's binary_logloss: 0.533967\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 6.834079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.450458\tvalid_1's binary_logloss: 0.535928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009:  80%|########  | 16/20 [1:47:22<22:29, 337.41s/it]\u001b[32m[I 2023-05-31 02:22:20,750]\u001b[0m Trial 22 finished with value: 0.5358236669686496 and parameters: {'num_leaves': 127}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009:  80%|########  | 16/20 [1:47:22<22:29, 337.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's binary_logloss: 0.455074\tvalid_1's binary_logloss: 0.535824\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.398352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466141\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009:  85%|########5 | 17/20 [1:54:35<18:18, 366.32s/it]\u001b[32m[I 2023-05-31 02:29:34,306]\u001b[0m Trial 23 finished with value: 0.5330088247443111 and parameters: {'num_leaves': 48}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009:  85%|########5 | 17/20 [1:54:35<18:18, 366.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.47225\tvalid_1's binary_logloss: 0.533009\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.113440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.406982\tvalid_1's binary_logloss: 0.537309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009:  90%|######### | 18/20 [2:02:57<13:34, 407.18s/it]\u001b[32m[I 2023-05-31 02:37:56,604]\u001b[0m Trial 24 finished with value: 0.5370257129106043 and parameters: {'num_leaves': 255}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009:  90%|######### | 18/20 [2:02:57<13:34, 407.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's binary_logloss: 0.422388\tvalid_1's binary_logloss: 0.537026\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.658443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.486575\tvalid_1's binary_logloss: 0.534936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009:  95%|#########5| 19/20 [2:06:54<05:56, 356.05s/it]\u001b[32m[I 2023-05-31 02:41:53,556]\u001b[0m Trial 25 finished with value: 0.5349355885113765 and parameters: {'num_leaves': 59}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009:  95%|#########5| 19/20 [2:06:54<05:56, 356.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\tvalid_0's binary_logloss: 0.45496\tvalid_1's binary_logloss: 0.535644\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's binary_logloss: 0.486575\tvalid_1's binary_logloss: 0.534936\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.714844 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.458435\tvalid_1's binary_logloss: 0.535654\n",
      "[200]\tvalid_0's binary_logloss: 0.419811\tvalid_1's binary_logloss: 0.53645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.533009: 100%|##########| 20/20 [2:11:59<00:00, 340.60s/it]\u001b[32m[I 2023-05-31 02:46:58,135]\u001b[0m Trial 26 finished with value: 0.5353040696245952 and parameters: {'num_leaves': 110}. Best is trial 17 with value: 0.5330088247443111.\u001b[0m\n",
      "num_leaves, val_score: 0.533009: 100%|##########| 20/20 [2:11:59<00:00, 395.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's binary_logloss: 0.448715\tvalid_1's binary_logloss: 0.535304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.428158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494319\tvalid_1's binary_logloss: 0.535851\n",
      "[200]\tvalid_0's binary_logloss: 0.466548\tvalid_1's binary_logloss: 0.535487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:  10%|#         | 1/10 [05:15<47:19, 315.52s/it]\u001b[32m[I 2023-05-31 02:52:13,699]\u001b[0m Trial 27 finished with value: 0.5353405121998125 and parameters: {'bagging_fraction': 0.8244333618923735, 'bagging_freq': 1}. Best is trial 27 with value: 0.5353405121998125.\u001b[0m\n",
      "bagging, val_score: 0.533009:  10%|#         | 1/10 [05:15<47:19, 315.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[192]\tvalid_0's binary_logloss: 0.46836\tvalid_1's binary_logloss: 0.535341\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.903639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.495711\tvalid_1's binary_logloss: 0.536795\n",
      "[200]\tvalid_0's binary_logloss: 0.469076\tvalid_1's binary_logloss: 0.53784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:  20%|##        | 2/10 [08:13<31:19, 234.90s/it]\u001b[32m[I 2023-05-31 02:55:12,167]\u001b[0m Trial 28 finished with value: 0.536728035972462 and parameters: {'bagging_fraction': 0.5189924746422235, 'bagging_freq': 7}. Best is trial 27 with value: 0.5353405121998125.\u001b[0m\n",
      "bagging, val_score: 0.533009:  20%|##        | 2/10 [08:14<31:19, 234.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's binary_logloss: 0.489184\tvalid_1's binary_logloss: 0.536728\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.791409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.535322\n",
      "[200]\tvalid_0's binary_logloss: 0.466194\tvalid_1's binary_logloss: 0.534875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:  30%|###       | 3/10 [13:20<31:13, 267.63s/it]\u001b[32m[I 2023-05-31 03:00:18,736]\u001b[0m Trial 29 finished with value: 0.5345457409823166 and parameters: {'bagging_fraction': 0.95687119283154, 'bagging_freq': 7}. Best is trial 29 with value: 0.5345457409823166.\u001b[0m\n",
      "bagging, val_score: 0.533009:  30%|###       | 3/10 [13:20<31:13, 267.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[162]\tvalid_0's binary_logloss: 0.475331\tvalid_1's binary_logloss: 0.534546\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.479779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.495984\tvalid_1's binary_logloss: 0.537791\n",
      "[200]\tvalid_0's binary_logloss: 0.470246\tvalid_1's binary_logloss: 0.539492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:  40%|####      | 4/10 [16:03<22:38, 226.41s/it]\u001b[32m[I 2023-05-31 03:03:01,960]\u001b[0m Trial 30 finished with value: 0.5377056576536495 and parameters: {'bagging_fraction': 0.4185398883666168, 'bagging_freq': 4}. Best is trial 29 with value: 0.5345457409823166.\u001b[0m\n",
      "bagging, val_score: 0.533009:  40%|####      | 4/10 [16:03<22:38, 226.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's binary_logloss: 0.487406\tvalid_1's binary_logloss: 0.537706\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.731515 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494372\tvalid_1's binary_logloss: 0.535475\n",
      "[200]\tvalid_0's binary_logloss: 0.466174\tvalid_1's binary_logloss: 0.535548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:  50%|#####     | 5/10 [21:04<21:05, 253.07s/it]\u001b[32m[I 2023-05-31 03:08:02,296]\u001b[0m Trial 31 finished with value: 0.535188430634139 and parameters: {'bagging_fraction': 0.9092961359244282, 'bagging_freq': 1}. Best is trial 29 with value: 0.5345457409823166.\u001b[0m\n",
      "bagging, val_score: 0.533009:  50%|#####     | 5/10 [21:04<21:05, 253.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[152]\tvalid_0's binary_logloss: 0.477895\tvalid_1's binary_logloss: 0.535188\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.491087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494366\tvalid_1's binary_logloss: 0.535302\n",
      "[200]\tvalid_0's binary_logloss: 0.46676\tvalid_1's binary_logloss: 0.535378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:  60%|######    | 6/10 [24:31<15:49, 237.40s/it]\u001b[32m[I 2023-05-31 03:11:29,281]\u001b[0m Trial 32 finished with value: 0.5349168164279218 and parameters: {'bagging_fraction': 0.8518794259079707, 'bagging_freq': 2}. Best is trial 29 with value: 0.5345457409823166.\u001b[0m\n",
      "bagging, val_score: 0.533009:  60%|######    | 6/10 [24:31<15:49, 237.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[119]\tvalid_0's binary_logloss: 0.487974\tvalid_1's binary_logloss: 0.534917\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.834234 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494218\tvalid_1's binary_logloss: 0.535145\n",
      "[200]\tvalid_0's binary_logloss: 0.466395\tvalid_1's binary_logloss: 0.534448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:  70%|#######   | 7/10 [29:05<12:28, 249.42s/it]\u001b[32m[I 2023-05-31 03:16:03,449]\u001b[0m Trial 33 finished with value: 0.5342380107212333 and parameters: {'bagging_fraction': 0.8852695324765545, 'bagging_freq': 6}. Best is trial 33 with value: 0.5342380107212333.\u001b[0m\n",
      "bagging, val_score: 0.533009:  70%|#######   | 7/10 [29:05<12:28, 249.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[167]\tvalid_0's binary_logloss: 0.474162\tvalid_1's binary_logloss: 0.534238\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.737386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.495321\tvalid_1's binary_logloss: 0.53739\n",
      "[200]\tvalid_0's binary_logloss: 0.46874\tvalid_1's binary_logloss: 0.537915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:  80%|########  | 8/10 [32:23<07:46, 233.17s/it]\u001b[32m[I 2023-05-31 03:19:21,841]\u001b[0m Trial 34 finished with value: 0.5370851343683625 and parameters: {'bagging_fraction': 0.5507662302556335, 'bagging_freq': 3}. Best is trial 33 with value: 0.5342380107212333.\u001b[0m\n",
      "bagging, val_score: 0.533009:  80%|########  | 8/10 [32:23<07:46, 233.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's binary_logloss: 0.488625\tvalid_1's binary_logloss: 0.537085\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.794430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.496033\tvalid_1's binary_logloss: 0.538481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009:  90%|######### | 9/10 [35:32<03:39, 219.37s/it]\u001b[32m[I 2023-05-31 03:22:30,862]\u001b[0m Trial 35 finished with value: 0.5381850268247642 and parameters: {'bagging_fraction': 0.4170659406660186, 'bagging_freq': 2}. Best is trial 33 with value: 0.5342380107212333.\u001b[0m\n",
      "bagging, val_score: 0.533009:  90%|######### | 9/10 [35:32<03:39, 219.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[82]\tvalid_0's binary_logloss: 0.50262\tvalid_1's binary_logloss: 0.538185\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.806687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49509\tvalid_1's binary_logloss: 0.535008\n",
      "[200]\tvalid_0's binary_logloss: 0.467966\tvalid_1's binary_logloss: 0.535498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.533009: 100%|##########| 10/10 [41:16<00:00, 257.68s/it]\u001b[32m[I 2023-05-31 03:28:14,333]\u001b[0m Trial 36 finished with value: 0.5345146988476355 and parameters: {'bagging_fraction': 0.6906578134945391, 'bagging_freq': 7}. Best is trial 33 with value: 0.5342380107212333.\u001b[0m\n",
      "bagging, val_score: 0.533009: 100%|##########| 10/10 [41:16<00:00, 247.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's binary_logloss: 0.485148\tvalid_1's binary_logloss: 0.534515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.533009:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.156007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.493475\tvalid_1's binary_logloss: 0.535248\n",
      "[200]\tvalid_0's binary_logloss: 0.465153\tvalid_1's binary_logloss: 0.535179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.533009:  33%|###3      | 1/3 [05:01<10:02, 301.31s/it]\u001b[32m[I 2023-05-31 03:33:15,675]\u001b[0m Trial 37 finished with value: 0.5348178662824535 and parameters: {'feature_fraction': 0.44800000000000006}. Best is trial 37 with value: 0.5348178662824535.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.533009:  33%|###3      | 1/3 [05:01<10:02, 301.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[144]\tvalid_0's binary_logloss: 0.479476\tvalid_1's binary_logloss: 0.534818\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.146448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.493241\tvalid_1's binary_logloss: 0.535585\n",
      "[200]\tvalid_0's binary_logloss: 0.464464\tvalid_1's binary_logloss: 0.535216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.533009:  67%|######6   | 2/3 [09:58<04:59, 299.12s/it]\u001b[32m[I 2023-05-31 03:38:13,267]\u001b[0m Trial 38 finished with value: 0.5351360315439739 and parameters: {'feature_fraction': 0.48000000000000004}. Best is trial 37 with value: 0.5348178662824535.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.533009:  67%|######6   | 2/3 [09:58<04:59, 299.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[193]\tvalid_0's binary_logloss: 0.466012\tvalid_1's binary_logloss: 0.535136\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.091321 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.493684\tvalid_1's binary_logloss: 0.534928\n",
      "[200]\tvalid_0's binary_logloss: 0.465534\tvalid_1's binary_logloss: 0.534726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.533009: 100%|##########| 3/3 [17:22<00:00, 364.94s/it]\u001b[32m[I 2023-05-31 03:45:36,530]\u001b[0m Trial 39 finished with value: 0.5346574431221167 and parameters: {'feature_fraction': 0.41600000000000004}. Best is trial 39 with value: 0.5346574431221167.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.533009: 100%|##########| 3/3 [17:22<00:00, 347.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[190]\tvalid_0's binary_logloss: 0.467793\tvalid_1's binary_logloss: 0.534657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.081528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494122\tvalid_1's binary_logloss: 0.534869\n",
      "[200]\tvalid_0's binary_logloss: 0.465943\tvalid_1's binary_logloss: 0.53471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:   5%|5         | 1/20 [04:15<1:20:51, 255.35s/it]\u001b[32m[I 2023-05-31 03:49:51,911]\u001b[0m Trial 40 finished with value: 0.5343071899806007 and parameters: {'lambda_l1': 0.0604554962111354, 'lambda_l2': 3.869954801736793e-08}. Best is trial 40 with value: 0.5343071899806007.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:   5%|5         | 1/20 [04:15<1:20:51, 255.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[132]\tvalid_0's binary_logloss: 0.483392\tvalid_1's binary_logloss: 0.534307\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.350906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.496355\tvalid_1's binary_logloss: 0.534797\n",
      "[200]\tvalid_0's binary_logloss: 0.469875\tvalid_1's binary_logloss: 0.534344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  10%|#         | 2/20 [08:07<1:12:30, 241.67s/it]\u001b[32m[I 2023-05-31 03:53:44,010]\u001b[0m Trial 41 finished with value: 0.5343145393423903 and parameters: {'lambda_l1': 4.9975706902437915, 'lambda_l2': 1.9012424041685485e-06}. Best is trial 40 with value: 0.5343071899806007.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  10%|#         | 2/20 [08:07<1:12:30, 241.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[146]\tvalid_0's binary_logloss: 0.48222\tvalid_1's binary_logloss: 0.534315\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.811634 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494152\tvalid_1's binary_logloss: 0.534225\n",
      "[200]\tvalid_0's binary_logloss: 0.466165\tvalid_1's binary_logloss: 0.534286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  15%|#5        | 3/20 [12:20<1:09:53, 246.69s/it]\u001b[32m[I 2023-05-31 03:57:56,673]\u001b[0m Trial 42 finished with value: 0.5338199521903195 and parameters: {'lambda_l1': 0.00039376838225038495, 'lambda_l2': 0.002499037071924892}. Best is trial 42 with value: 0.5338199521903195.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  15%|#5        | 3/20 [12:20<1:09:53, 246.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's binary_logloss: 0.485836\tvalid_1's binary_logloss: 0.53382\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.639361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494159\tvalid_1's binary_logloss: 0.53474\n",
      "[200]\tvalid_0's binary_logloss: 0.466644\tvalid_1's binary_logloss: 0.534017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  20%|##        | 4/20 [20:06<1:28:56, 333.52s/it]\u001b[32m[I 2023-05-31 04:05:43,307]\u001b[0m Trial 43 finished with value: 0.5339580689095228 and parameters: {'lambda_l1': 1.6327842809598889e-06, 'lambda_l2': 0.12222223687206561}. Best is trial 42 with value: 0.5338199521903195.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  20%|##        | 4/20 [20:06<1:28:56, 333.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[195]\tvalid_0's binary_logloss: 0.467679\tvalid_1's binary_logloss: 0.533958\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.376665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466141\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  25%|##5       | 5/20 [26:57<1:30:20, 361.38s/it]\u001b[32m[I 2023-05-31 04:12:34,067]\u001b[0m Trial 44 finished with value: 0.5330088082772017 and parameters: {'lambda_l1': 0.00015569864847923652, 'lambda_l2': 1.972108672067178e-05}. Best is trial 44 with value: 0.5330088082772017.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  25%|##5       | 5/20 [26:57<1:30:20, 361.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.47225\tvalid_1's binary_logloss: 0.533009\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.252395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494372\tvalid_1's binary_logloss: 0.534432\n",
      "[200]\tvalid_0's binary_logloss: 0.46632\tvalid_1's binary_logloss: 0.53375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  30%|###       | 6/20 [31:49<1:18:46, 337.63s/it]\u001b[32m[I 2023-05-31 04:17:25,615]\u001b[0m Trial 45 finished with value: 0.5335700065176225 and parameters: {'lambda_l1': 0.340061975132354, 'lambda_l2': 0.18377744502257684}. Best is trial 44 with value: 0.5330088082772017.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  30%|###       | 6/20 [31:49<1:18:46, 337.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[156]\tvalid_0's binary_logloss: 0.477009\tvalid_1's binary_logloss: 0.53357\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.231076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494152\tvalid_1's binary_logloss: 0.534225\n",
      "[200]\tvalid_0's binary_logloss: 0.466094\tvalid_1's binary_logloss: 0.534324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  35%|###5      | 7/20 [35:53<1:06:33, 307.17s/it]\u001b[32m[I 2023-05-31 04:21:30,062]\u001b[0m Trial 46 finished with value: 0.5338199699103042 and parameters: {'lambda_l1': 2.8664020497040848e-05, 'lambda_l2': 0.002540819891025607}. Best is trial 44 with value: 0.5330088082772017.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  35%|###5      | 7/20 [35:53<1:06:33, 307.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's binary_logloss: 0.485836\tvalid_1's binary_logloss: 0.53382\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.755663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49432\tvalid_1's binary_logloss: 0.534541\n",
      "[200]\tvalid_0's binary_logloss: 0.466085\tvalid_1's binary_logloss: 0.534406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  40%|####      | 8/20 [42:31<1:07:11, 335.98s/it]\u001b[32m[I 2023-05-31 04:28:07,716]\u001b[0m Trial 47 finished with value: 0.5340980247287268 and parameters: {'lambda_l1': 0.3631575115070118, 'lambda_l2': 2.565058627813059e-07}. Best is trial 44 with value: 0.5330088082772017.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  40%|####      | 8/20 [42:31<1:07:11, 335.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[140]\tvalid_0's binary_logloss: 0.481153\tvalid_1's binary_logloss: 0.534098\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.659757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466142\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  45%|####5     | 9/20 [47:18<58:49, 320.84s/it]  \u001b[32m[I 2023-05-31 04:32:55,277]\u001b[0m Trial 48 finished with value: 0.5330087828067551 and parameters: {'lambda_l1': 0.0004765648129632002, 'lambda_l2': 5.355668054611892e-06}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  45%|####5     | 9/20 [47:18<58:49, 320.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.472251\tvalid_1's binary_logloss: 0.533009\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.629821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49576\tvalid_1's binary_logloss: 0.534248\n",
      "[200]\tvalid_0's binary_logloss: 0.469615\tvalid_1's binary_logloss: 0.533533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  50%|#####     | 10/20 [54:28<59:05, 354.56s/it]\u001b[32m[I 2023-05-31 04:40:05,346]\u001b[0m Trial 49 finished with value: 0.5335060741159852 and parameters: {'lambda_l1': 5.0585963953619845e-06, 'lambda_l2': 7.694730802665455}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  50%|#####     | 10/20 [54:28<59:05, 354.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[157]\tvalid_0's binary_logloss: 0.479152\tvalid_1's binary_logloss: 0.533506\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.519491 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466141\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  55%|#####5    | 11/20 [59:06<49:39, 331.08s/it]\u001b[32m[I 2023-05-31 04:44:43,200]\u001b[0m Trial 50 finished with value: 0.5330088247405984 and parameters: {'lambda_l1': 1.783781087925491e-08, 'lambda_l2': 1.0588809037037295e-08}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  55%|#####5    | 11/20 [59:06<49:39, 331.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.47225\tvalid_1's binary_logloss: 0.533009\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.133704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494284\tvalid_1's binary_logloss: 0.534009\n",
      "[200]\tvalid_0's binary_logloss: 0.466159\tvalid_1's binary_logloss: 0.533866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  60%|######    | 12/20 [1:03:12<40:40, 305.10s/it]\u001b[32m[I 2023-05-31 04:48:48,858]\u001b[0m Trial 51 finished with value: 0.5333409497547723 and parameters: {'lambda_l1': 0.0018428821961065651, 'lambda_l2': 5.790872075589118e-06}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  60%|######    | 12/20 [1:03:12<40:40, 305.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[153]\tvalid_0's binary_logloss: 0.477721\tvalid_1's binary_logloss: 0.533341\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.379932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494241\tvalid_1's binary_logloss: 0.534137\n",
      "[200]\tvalid_0's binary_logloss: 0.466246\tvalid_1's binary_logloss: 0.533916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  65%|######5   | 13/20 [1:07:38<34:12, 293.27s/it]\u001b[32m[I 2023-05-31 04:53:14,921]\u001b[0m Trial 52 finished with value: 0.5335338955179935 and parameters: {'lambda_l1': 0.003142473301171557, 'lambda_l2': 3.168975415346845e-05}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  65%|######5   | 13/20 [1:07:38<34:12, 293.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[165]\tvalid_0's binary_logloss: 0.474338\tvalid_1's binary_logloss: 0.533534\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.160388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466141\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  70%|#######   | 14/20 [1:11:49<28:03, 280.54s/it]\u001b[32m[I 2023-05-31 04:57:26,036]\u001b[0m Trial 53 finished with value: 0.5330088073872115 and parameters: {'lambda_l1': 8.954444081826908e-05, 'lambda_l2': 6.246467255033147e-05}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  70%|#######   | 14/20 [1:11:49<28:03, 280.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.47225\tvalid_1's binary_logloss: 0.533009\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.369144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494271\tvalid_1's binary_logloss: 0.534305\n",
      "[200]\tvalid_0's binary_logloss: 0.46599\tvalid_1's binary_logloss: 0.534114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  75%|#######5  | 15/20 [1:17:51<25:24, 305.00s/it]\u001b[32m[I 2023-05-31 05:03:27,721]\u001b[0m Trial 54 finished with value: 0.533749693563732 and parameters: {'lambda_l1': 0.004541415190106224, 'lambda_l2': 0.0005286712575090093}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  75%|#######5  | 15/20 [1:17:51<25:24, 305.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's binary_logloss: 0.483885\tvalid_1's binary_logloss: 0.53375\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.494177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466141\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  80%|########  | 16/20 [1:25:23<23:17, 349.38s/it]\u001b[32m[I 2023-05-31 05:11:00,160]\u001b[0m Trial 55 finished with value: 0.5330088216614665 and parameters: {'lambda_l1': 3.4739364682166064e-05, 'lambda_l2': 5.949031026001032e-07}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  80%|########  | 16/20 [1:25:23<23:17, 349.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.47225\tvalid_1's binary_logloss: 0.533009\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.405188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466141\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  85%|########5 | 17/20 [1:33:02<19:06, 382.25s/it]\u001b[32m[I 2023-05-31 05:18:38,853]\u001b[0m Trial 56 finished with value: 0.533008817024269 and parameters: {'lambda_l1': 6.880018130833114e-07, 'lambda_l2': 4.97176851039701e-05}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  85%|########5 | 17/20 [1:33:02<19:06, 382.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.47225\tvalid_1's binary_logloss: 0.533009\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.140586 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466142\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  90%|######### | 18/20 [1:36:48<11:10, 335.46s/it]\u001b[32m[I 2023-05-31 05:22:25,388]\u001b[0m Trial 57 finished with value: 0.5330087910340476 and parameters: {'lambda_l1': 0.00039042774207132403, 'lambda_l2': 2.1400000378852668e-07}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  90%|######### | 18/20 [1:36:48<11:10, 335.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.472251\tvalid_1's binary_logloss: 0.533009\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.477681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49414\tvalid_1's binary_logloss: 0.534703\n",
      "[200]\tvalid_0's binary_logloss: 0.465648\tvalid_1's binary_logloss: 0.534954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009:  95%|#########5| 19/20 [1:43:04<05:47, 347.53s/it]\u001b[32m[I 2023-05-31 05:28:41,045]\u001b[0m Trial 58 finished with value: 0.5345640148440186 and parameters: {'lambda_l1': 0.020259121814818877, 'lambda_l2': 1.856907255001664e-07}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009:  95%|#########5| 19/20 [1:43:04<05:47, 347.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[112]\tvalid_0's binary_logloss: 0.48992\tvalid_1's binary_logloss: 0.534564\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.398726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.49429\tvalid_1's binary_logloss: 0.534004\n",
      "[200]\tvalid_0's binary_logloss: 0.466142\tvalid_1's binary_logloss: 0.533015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.533009: 100%|##########| 20/20 [1:47:30<00:00, 323.20s/it]\u001b[32m[I 2023-05-31 05:33:07,526]\u001b[0m Trial 59 finished with value: 0.5330087902477733 and parameters: {'lambda_l1': 0.0003956555200109906, 'lambda_l2': 2.3882055188566842e-06}. Best is trial 48 with value: 0.5330087828067551.\u001b[0m\n",
      "regularization_factors, val_score: 0.533009: 100%|##########| 20/20 [1:47:30<00:00, 322.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[174]\tvalid_0's binary_logloss: 0.472251\tvalid_1's binary_logloss: 0.533009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.533009:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.397970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494481\tvalid_1's binary_logloss: 0.534517\n",
      "[200]\tvalid_0's binary_logloss: 0.466739\tvalid_1's binary_logloss: 0.533905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.533009:  20%|##        | 1/5 [07:11<28:46, 431.55s/it]\u001b[32m[I 2023-05-31 05:40:19,109]\u001b[0m Trial 60 finished with value: 0.5337455757712775 and parameters: {'min_child_samples': 100}. Best is trial 60 with value: 0.5337455757712775.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.533009:  20%|##        | 1/5 [07:11<28:46, 431.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[190]\tvalid_0's binary_logloss: 0.468856\tvalid_1's binary_logloss: 0.533746\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.471242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494221\tvalid_1's binary_logloss: 0.5344\n",
      "[200]\tvalid_0's binary_logloss: 0.466351\tvalid_1's binary_logloss: 0.533855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.533009:  40%|####      | 2/5 [11:35<16:38, 332.71s/it]\u001b[32m[I 2023-05-31 05:44:42,635]\u001b[0m Trial 61 finished with value: 0.5336968987105085 and parameters: {'min_child_samples': 25}. Best is trial 61 with value: 0.5336968987105085.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.533009:  40%|####      | 2/5 [11:35<16:38, 332.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[141]\tvalid_0's binary_logloss: 0.480927\tvalid_1's binary_logloss: 0.533697\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.541893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494221\tvalid_1's binary_logloss: 0.534532\n",
      "[200]\tvalid_0's binary_logloss: 0.466187\tvalid_1's binary_logloss: 0.534321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.533009:  60%|######    | 3/5 [17:54<11:48, 354.23s/it]\u001b[32m[I 2023-05-31 05:51:02,472]\u001b[0m Trial 62 finished with value: 0.5338224270090719 and parameters: {'min_child_samples': 50}. Best is trial 61 with value: 0.5336968987105085.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.533009:  60%|######    | 3/5 [17:54<11:48, 354.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[128]\tvalid_0's binary_logloss: 0.48487\tvalid_1's binary_logloss: 0.533822\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.432873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494142\tvalid_1's binary_logloss: 0.534797\n",
      "[200]\tvalid_0's binary_logloss: 0.466068\tvalid_1's binary_logloss: 0.534519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.533009:  80%|########  | 4/5 [22:01<05:11, 311.58s/it]\u001b[32m[I 2023-05-31 05:55:08,673]\u001b[0m Trial 63 finished with value: 0.5342254577526487 and parameters: {'min_child_samples': 5}. Best is trial 61 with value: 0.5336968987105085.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.533009:  80%|########  | 4/5 [22:01<05:11, 311.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.481901\tvalid_1's binary_logloss: 0.534225\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.549104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 472995\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5185\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.494275\tvalid_1's binary_logloss: 0.534388\n",
      "[200]\tvalid_0's binary_logloss: 0.46577\tvalid_1's binary_logloss: 0.534298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.533009: 100%|##########| 5/5 [29:18<00:00, 356.87s/it]\u001b[32m[I 2023-05-31 06:02:25,834]\u001b[0m Trial 64 finished with value: 0.5338909136644174 and parameters: {'min_child_samples': 10}. Best is trial 61 with value: 0.5336968987105085.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.533009: 100%|##########| 5/5 [29:18<00:00, 351.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[143]\tvalid_0's binary_logloss: 0.48013\tvalid_1's binary_logloss: 0.533891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "dfs = []\n",
    "for group in [\"0-4\", \"5-12\"]:\n",
    "    print(group)\n",
    "    # データ読み込み\n",
    "    train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}.csv\")\n",
    "    labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "    train_group = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "    # 一つ前のlevel_groupの特徴量を追加\n",
    "    if prev_features_df is not None:\n",
    "        train_group = train_group.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "    if group == \"5-12\":\n",
    "        train_group[\"0-4_question_duration_time\"] = train_group[\"5-12_elapsed_time_min\"] - train_group[\"0-4_elapsed_time_max\"]\n",
    "        train_group[\"0-4_question_duration_index\"] = train_group[\"5-12_index_min\"] - train_group[\"0-4_index_max\"]\n",
    "    elif group == \"13-22\":\n",
    "        train_group[\"5-12_question_duration_time\"] = train_group[\"13-22_elapsed_time_min\"] - train_group[\"5-12_elapsed_time_max\"]\n",
    "        train_group[\"5-12_question_duration_index\"] = train_group[\"13-22_index_min\"] - train_group[\"5-12_index_max\"]\n",
    "\n",
    "    target = \"correct\"\n",
    "    not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "    features = [c for c in train_group.columns if c not in not_use_cols]\n",
    "\n",
    "    # 次のlevel_groupで使う用に特徴量を保持\n",
    "    prev_features_df = train_group[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "\n",
    "    dfs.append(train_group)\n",
    "train = pd.concat(dfs, ignore_index=True)\n",
    "# concatするとcategory型がリセットされてしまうので再度cast\n",
    "train_group[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "target = \"correct\"\n",
    "not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "features = [c for c in train_group.columns if c not in not_use_cols]\n",
    "gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "\n",
    "for i, (tr_idx, vl_idx) in enumerate(gkf.split(train_group[features], train_group[target], train_group[\"session_id\"])):\n",
    "    oof_groups = []\n",
    "    print(f\"fold : {i}\")\n",
    "    tr_x, tr_y = train_group.iloc[tr_idx][features], train_group.iloc[tr_idx][target]\n",
    "    vl_x, vl_y = train_group.iloc[vl_idx][features], train_group.iloc[vl_idx][target]\n",
    "    tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "    break\n",
    "\n",
    "model = optuna_lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                         num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary', 'boosting': 'gbdt', 'learning_rate': 0.1, 'metric': 'binary_logloss', 'seed': 42, 'feature_pre_filter': False, 'lambda_l1': 0.0004765648129632002, 'lambda_l2': 5.355668054611892e-06, 'num_leaves': 48, 'feature_fraction': 0.4, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'num_iterations': 20000, 'early_stopping_round': 100, 'categorical_column': [0]}\n"
     ]
    }
   ],
   "source": [
    "print(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
