{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp072"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp069ベース。 minigame特徴量追加\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp072\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.01, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    with open(cfg.prep_dir + 'cat_col_lists_v2.pkl', 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) \n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    with open(\"/kaggle/input/psp-cat-col-lists/cat_col_lists_v2.pkl\", 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTrain:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"session_id\", \"level_group\", \"index\"], ignore_index=True)\n",
    "        self.features = self.sessions_df[[\"session_id\", \"level_group\"]].drop_duplicates().copy()\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].shift(1)\n",
    "        self.sessions_df[\"time_diff\"] = np.where(self.sessions_df[\"time_diff\"]<0, 0, self.sessions_df[\"time_diff\"])\n",
    "        self.sessions_df[\"time_diff\"] = np.nan_to_num(self.sessions_df[\"time_diff\"], 0)\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_record_cnt\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].agg([max,min]).reset_index()\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[\"max\"] - add_features[\"min\"]\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[f\"{self.group}_group_elapsed_time\"].astype(np.float32)\n",
    "        add_features = add_features[[\"session_id\", \"level_group\", f\"{self.group}_group_elapsed_time\"]].copy()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[\"index\"].count().reset_index().rename(columns={\"index\":\"cnt\"})\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            tmp = add_features[add_features[cat_col]==cat][[\"session_id\", \"level_group\", \"cnt\"]].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp = tmp.rename(columns={\"cnt\": feat_name})\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[feat_name] = self.features[feat_name].fillna(0)\n",
    "            else:\n",
    "                self.features[feat_name] = 0\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.dropna(subset=[cat_col]).drop_duplicates([\"session_id\", \"level_group\", cat_col])\n",
    "        add_features = add_features.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_{cat_col}_nunique\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")        \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        new_cols = [f\"{self.group}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{self.group}_{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "    def _cat_change_cnt(self, cat_col):\n",
    "        \"\"\"cat_colの変化回数\n",
    "        \"\"\"\n",
    "        tmp = self.sessions_df[[\"session_id\", \"level_group\", cat_col]].copy()\n",
    "        tmp[cat_col] = tmp[cat_col].fillna(\"nan\")\n",
    "        tmp[f\"{cat_col}_change_cnt\"] = (tmp[cat_col] != tmp.groupby([\"session_id\", \"level_group\"])[cat_col].shift(1)).astype(int)\n",
    "        add_features = tmp.groupby([\"session_id\", \"level_group\"])[f\"{cat_col}_change_cnt\"].sum().reset_index()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "\n",
    "    def _add_minigame_features(self, start_fqid, end_fqid):\n",
    "        game_name = start_fqid\n",
    "        dfs = []\n",
    "        for session_id in tqdm(self.sessions_df[\"session_id\"].unique()):\n",
    "            tmp = self.sessions_df[self.sessions_df[\"session_id\"]==session_id].copy()\n",
    "            start_indexes = tmp[(tmp[\"event_name\"]==\"navigate_click\")&(tmp[\"fqid\"]==start_fqid)][\"index\"].values\n",
    "            end_indexes = tmp[(tmp[\"event_name\"]==\"object_click\")&(tmp[\"fqid\"]==end_fqid)][\"index\"].values\n",
    "            if len(start_indexes) > 0:\n",
    "                start_index = start_indexes[0]\n",
    "            else:\n",
    "                start_index = np.nan\n",
    "            if len(end_indexes) > 0:\n",
    "                end_index = end_indexes[0]\n",
    "            else:\n",
    "                end_index = np.nan\n",
    "\n",
    "            if start_index < end_index:\n",
    "                mini_game_sessions = tmp[(tmp[\"index\"]>start_index)&(tmp[\"index\"]<=end_index)].copy()\n",
    "                record_cnt = len(mini_game_sessions)\n",
    "                total_duration = mini_game_sessions[\"time_diff\"].sum()\n",
    "                total_hover_duration = mini_game_sessions[\"hover_duration\"].sum()\n",
    "\n",
    "                hover_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_hover\"].copy()\n",
    "                if len(hover_sessions) > 0:\n",
    "                    hover_cnt = len(hover_sessions)\n",
    "                else:\n",
    "                    hover_cnt = 0\n",
    "\n",
    "                click_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_click\"].copy()\n",
    "                if len(click_sessions) > 0:\n",
    "                    click_cnt = len(click_sessions)\n",
    "                else:\n",
    "                    click_cnt = 0\n",
    "\n",
    "                feature_tmp = pd.DataFrame([[session_id, record_cnt, total_duration, total_hover_duration, hover_cnt, click_cnt]],\n",
    "                                            columns=[\"session_id\", f\"{self.group}_{game_name}_record_cnt\", f\"{self.group}_minigame_{game_name}_total_duration\", f\"{self.group}_minigame_{game_name}_total_hover_duration\",\n",
    "                                                    f\"{self.group}_minigame_{game_name}_hover_cnt\", f\"{self.group}_minigame_{game_name}_click_cnt\"]\n",
    "                                        )\n",
    "            else:\n",
    "                feature_tmp = pd.DataFrame([[session_id, 0, 0, 0, 0, 0]],\n",
    "                                            columns=[\"session_id\", f\"{self.group}_{game_name}_record_cnt\", f\"{self.group}_minigame_{game_name}_total_duration\", f\"{self.group}_minigame_{game_name}_total_hover_duration\",\n",
    "                                                    f\"{self.group}_minigame_{game_name}_hover_cnt\", f\"{self.group}_minigame_{game_name}_click_cnt\"]\n",
    "                                        )\n",
    "            dfs.append(feature_tmp)\n",
    "        add_features = pd.concat(dfs, ignore_index=True)\n",
    "        self.features = self.features.merge(add_features, on=\"session_id\", how=\"left\")\n",
    "\n",
    "\n",
    "    def get_train(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "        \n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        \n",
    "        self._cat_change_cnt(\"text_fqid\")\n",
    "        self._cat_change_cnt(\"room_fqid\")\n",
    "\n",
    "        if self.group == \"0-4\":\n",
    "            self._add_minigame_features(\"tunic\", \"tunic.hub.slip\")\n",
    "            self._add_minigame_features(\"plaque\", \"plaque.face.date\")\n",
    "        \n",
    "        elif self.group == \"5-12\":\n",
    "            self._add_minigame_features(\"businesscards\", \"businesscards.card_bingo.bingo\")\n",
    "            self._add_minigame_features(\"logbook\", \"logbook.page.bingo\")\n",
    "            self._add_minigame_features(\"reader\", \"reader.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals\", \"journals.pic_2.bingo\")\n",
    "        \n",
    "        elif self.group == \"13-22\":\n",
    "            self._add_minigame_features(\"tracks\", \"tracks.hub.deer\")\n",
    "            self._add_minigame_features(\"reader_flag\", \"reader_flag.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals_flag\", \"journals_flag.pic_0.bingo\")\n",
    "        \n",
    "        self.result = self.result.merge(self.features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesInf:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"index\"], ignore_index=True)\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.use_cols = [\n",
    "            \"elapsed_time\", \"event_name\", \"name\", \"level\", \"page\", \"index\",\n",
    "            \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\",\n",
    "            \"hover_duration\", \"text\", \"fqid\", \"room_fqid\", \"text_fqid\"\n",
    "        ]\n",
    "\n",
    "    def _prep(self):\n",
    "        # dataframeの各列をnumpy arrayで保持\n",
    "        self.sessions = {}\n",
    "        for c in self.use_cols:\n",
    "            self.sessions[c] = self.sessions_df[c].values\n",
    "        self.sessions[\"time_diff\"] = self.sessions[\"elapsed_time\"] - self.sessions_df[\"elapsed_time\"].shift(1).values\n",
    "        self.sessions[\"time_diff\"] = np.where(self.sessions[\"time_diff\"]<0, 0, self.sessions[\"time_diff\"])\n",
    "        self.sessions[\"time_diff\"] = np.nan_to_num(self.sessions[\"time_diff\"], 0)\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_feature = len(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_record_cnt\"] = add_feature\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_feature = np.max(self.sessions[\"elapsed_time\"]) - np.min(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_group_elapsed_time\"] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            add_feature = (self.sessions[cat_col] == cat).astype(int).sum()\n",
    "            self.result[feat_name] = add_feature\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        self.result[f\"{self.group}_{cat_col}_nunique\"] = self.sessions_df[cat_col].dropna().nunique()       \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        for val_col, agg in itertools.product(val_cols, aggs):\n",
    "            feat_name = f\"{self.group}_{val_col}_{agg}\"\n",
    "            if agg == \"mean\":\n",
    "                add_feature = np.nanmean(self.sessions[val_col])\n",
    "            elif agg == \"max\":\n",
    "                add_feature = np.nanmax(self.sessions[val_col])\n",
    "            elif agg == \"min\":\n",
    "                add_feature = np.nanmin(self.sessions[val_col])\n",
    "            elif agg == \"std\":\n",
    "                add_feature = np.nanstd(self.sessions[val_col], ddof=1)\n",
    "            elif agg == \"sum\":\n",
    "                add_feature = np.nansum(self.sessions[val_col])\n",
    "            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    tmp = self.sessions[val_col][idx]\n",
    "                    if agg == \"mean\":\n",
    "                        add_feature = np.nanmean(tmp)\n",
    "                    elif agg == \"max\":\n",
    "                        add_feature = np.nanmax(tmp)\n",
    "                    elif agg == \"min\":\n",
    "                        add_feature = np.nanmin(tmp)\n",
    "                    elif agg == \"std\":\n",
    "                        add_feature = np.nanstd(tmp, ddof=1)\n",
    "                    elif agg == \"sum\":\n",
    "                        add_feature = np.nansum(tmp)\n",
    "                    if np.isnan(add_feature):\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_change_cnt(self, cat_col):\n",
    "        \"\"\"cat_colの変化回数\n",
    "        \"\"\"\n",
    "        feat_name = f\"{cat_col}_change_cnt\"\n",
    "        tmp = self.sessions_df[cat_col].copy()\n",
    "        tmp = tmp.fillna(\"nan\")\n",
    "        self.result[feat_name] = (tmp != tmp.shift(1)).sum()\n",
    "\n",
    "    def get_test(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "\n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        self._cat_change_cnt(\"text_fqid\")\n",
    "        self._cat_change_cnt(\"room_fqid\")\n",
    "        \n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesTrain(sessions, labels)\n",
    "    train = feat.get_train()\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesInf(sessions, labels)\n",
    "    test = feat.get_test()\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}_cleaned.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "    \n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "        gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "        fis = []\n",
    "        \n",
    "        for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "            oof_groups = []\n",
    "            print(f\"fold : {i}\")\n",
    "            tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "            vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "            tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "            model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                            num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)\n",
    "            # モデル出力\n",
    "            model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\")\n",
    "        \n",
    "            # valid_pred\n",
    "            oof_fold = train.iloc[vl_idx].copy()\n",
    "            oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "            oof_groups.append(oof_fold)\n",
    "\n",
    "            # 特徴量重要度\n",
    "            fi_fold = pd.DataFrame()\n",
    "            fi_fold[\"feature\"] = model.feature_name()\n",
    "            fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "            fi_fold[\"fold\"] = i\n",
    "            fis.append(fi_fold)\n",
    "\n",
    "        fi = pd.concat(fis)    \n",
    "        fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "        fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "        fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi_{group}.csv\", index=False)\n",
    "\n",
    "        oof_group = pd.concat(oof_groups)\n",
    "        oofs.append(oof_group)\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"level_group2\"] = test[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"level_group2\"] = sub[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in test.groupby(\"level_group2\")]\n",
    "    subs = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in sub.groupby(\"level_group2\")]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    model_dict = {}\n",
    "    features_dict = {}\n",
    "    for g in level_group_list:\n",
    "        if mode == \"local_cv\":\n",
    "            model_paths = [cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_paths = [f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        model_dict[g] = [lgb.Booster(model_file=p) for p in model_paths]\n",
    "        features_dict[g] = model_dict[g][0].feature_name()\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "        features = features_dict[level_group]\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = model_dict[level_group][i]\n",
    "            preds += model.predict(test[features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        test[\"pred\"] = preds\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = test.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{level_group}_pred_mean\", \"max\":f\"{level_group}_pred_max\", \"min\":f\"{level_group}_pred_min\", \"std\":f\"{level_group}_pred_std\"})\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23562/23562 [01:42<00:00, 231.00it/s]\n",
      "100%|██████████| 23562/23562 [01:43<00:00, 228.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49821, number of negative: 6726\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.753041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 172955\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 806\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.881055 -> initscore=2.002456\n",
      "[LightGBM] [Info] Start training from score 2.002456\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.291063\tvalid_1's binary_logloss: 0.312844\n",
      "[200]\ttraining's binary_logloss: 0.25791\tvalid_1's binary_logloss: 0.289878\n",
      "[300]\ttraining's binary_logloss: 0.237361\tvalid_1's binary_logloss: 0.278952\n",
      "[400]\ttraining's binary_logloss: 0.222716\tvalid_1's binary_logloss: 0.273914\n",
      "[500]\ttraining's binary_logloss: 0.211127\tvalid_1's binary_logloss: 0.271618\n",
      "[600]\ttraining's binary_logloss: 0.200893\tvalid_1's binary_logloss: 0.270466\n",
      "[700]\ttraining's binary_logloss: 0.191797\tvalid_1's binary_logloss: 0.269874\n",
      "[800]\ttraining's binary_logloss: 0.183821\tvalid_1's binary_logloss: 0.269897\n",
      "Early stopping, best iteration is:\n",
      "[743]\ttraining's binary_logloss: 0.188286\tvalid_1's binary_logloss: 0.269746\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49692, number of negative: 6855\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.208708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 172958\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 806\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.878773 -> initscore=1.980866\n",
      "[LightGBM] [Info] Start training from score 1.980866\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.294083\tvalid_1's binary_logloss: 0.299899\n",
      "[200]\ttraining's binary_logloss: 0.260306\tvalid_1's binary_logloss: 0.278572\n",
      "[300]\ttraining's binary_logloss: 0.239465\tvalid_1's binary_logloss: 0.268856\n",
      "[400]\ttraining's binary_logloss: 0.224556\tvalid_1's binary_logloss: 0.264606\n",
      "[500]\ttraining's binary_logloss: 0.212749\tvalid_1's binary_logloss: 0.262936\n",
      "[600]\ttraining's binary_logloss: 0.202482\tvalid_1's binary_logloss: 0.262035\n",
      "[700]\ttraining's binary_logloss: 0.193311\tvalid_1's binary_logloss: 0.261649\n",
      "[800]\ttraining's binary_logloss: 0.185121\tvalid_1's binary_logloss: 0.261515\n",
      "[900]\ttraining's binary_logloss: 0.177766\tvalid_1's binary_logloss: 0.261474\n",
      "Early stopping, best iteration is:\n",
      "[884]\ttraining's binary_logloss: 0.178872\tvalid_1's binary_logloss: 0.26145\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49772, number of negative: 6778\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.325066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173078\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 806\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880141 -> initscore=1.993771\n",
      "[LightGBM] [Info] Start training from score 1.993771\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.292485\tvalid_1's binary_logloss: 0.305211\n",
      "[200]\ttraining's binary_logloss: 0.259073\tvalid_1's binary_logloss: 0.282142\n",
      "[300]\ttraining's binary_logloss: 0.238328\tvalid_1's binary_logloss: 0.2711\n",
      "[400]\ttraining's binary_logloss: 0.22345\tvalid_1's binary_logloss: 0.265772\n",
      "[500]\ttraining's binary_logloss: 0.211736\tvalid_1's binary_logloss: 0.263299\n",
      "[600]\ttraining's binary_logloss: 0.201423\tvalid_1's binary_logloss: 0.262213\n",
      "[700]\ttraining's binary_logloss: 0.192264\tvalid_1's binary_logloss: 0.261624\n",
      "[800]\ttraining's binary_logloss: 0.184106\tvalid_1's binary_logloss: 0.261423\n",
      "[900]\ttraining's binary_logloss: 0.176775\tvalid_1's binary_logloss: 0.261258\n",
      "Early stopping, best iteration is:\n",
      "[861]\ttraining's binary_logloss: 0.179554\tvalid_1's binary_logloss: 0.261214\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49755, number of negative: 6795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173037\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 806\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.879841 -> initscore=1.990924\n",
      "[LightGBM] [Info] Start training from score 1.990924\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.291292\tvalid_1's binary_logloss: 0.306838\n",
      "[200]\ttraining's binary_logloss: 0.257364\tvalid_1's binary_logloss: 0.285624\n",
      "[300]\ttraining's binary_logloss: 0.236249\tvalid_1's binary_logloss: 0.27559\n",
      "[400]\ttraining's binary_logloss: 0.221245\tvalid_1's binary_logloss: 0.270977\n",
      "[500]\ttraining's binary_logloss: 0.20936\tvalid_1's binary_logloss: 0.269073\n",
      "[600]\ttraining's binary_logloss: 0.198852\tvalid_1's binary_logloss: 0.268124\n",
      "[700]\ttraining's binary_logloss: 0.1895\tvalid_1's binary_logloss: 0.267825\n",
      "[800]\ttraining's binary_logloss: 0.181291\tvalid_1's binary_logloss: 0.267655\n",
      "[900]\ttraining's binary_logloss: 0.173873\tvalid_1's binary_logloss: 0.267765\n",
      "Early stopping, best iteration is:\n",
      "[833]\ttraining's binary_logloss: 0.178783\tvalid_1's binary_logloss: 0.267585\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49804, number of negative: 6746\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 172866\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 806\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880707 -> initscore=1.999146\n",
      "[LightGBM] [Info] Start training from score 1.999146\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.291113\tvalid_1's binary_logloss: 0.308357\n",
      "[200]\ttraining's binary_logloss: 0.257776\tvalid_1's binary_logloss: 0.284433\n",
      "[300]\ttraining's binary_logloss: 0.236973\tvalid_1's binary_logloss: 0.272733\n",
      "[400]\ttraining's binary_logloss: 0.222153\tvalid_1's binary_logloss: 0.267323\n",
      "[500]\ttraining's binary_logloss: 0.21041\tvalid_1's binary_logloss: 0.2648\n",
      "[600]\ttraining's binary_logloss: 0.200004\tvalid_1's binary_logloss: 0.263444\n",
      "[700]\ttraining's binary_logloss: 0.19071\tvalid_1's binary_logloss: 0.262748\n",
      "[800]\ttraining's binary_logloss: 0.182543\tvalid_1's binary_logloss: 0.262582\n",
      "[900]\ttraining's binary_logloss: 0.17516\tvalid_1's binary_logloss: 0.262467\n",
      "[1000]\ttraining's binary_logloss: 0.168495\tvalid_1's binary_logloss: 0.262508\n",
      "Early stopping, best iteration is:\n",
      "[927]\ttraining's binary_logloss: 0.173271\tvalid_1's binary_logloss: 0.262442\n",
      "5-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23562/23562 [02:52<00:00, 136.33it/s]\n",
      "100%|██████████| 23562/23562 [02:50<00:00, 138.10it/s]\n",
      "100%|██████████| 23562/23562 [02:54<00:00, 135.35it/s]\n",
      "100%|██████████| 23562/23562 [02:49<00:00, 139.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.098376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 465956\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 2144\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.571843\tvalid_1's binary_logloss: 0.57991\n",
      "[200]\ttraining's binary_logloss: 0.540206\tvalid_1's binary_logloss: 0.553859\n",
      "[300]\ttraining's binary_logloss: 0.524212\tvalid_1's binary_logloss: 0.543656\n",
      "[400]\ttraining's binary_logloss: 0.513304\tvalid_1's binary_logloss: 0.538434\n",
      "[500]\ttraining's binary_logloss: 0.505086\tvalid_1's binary_logloss: 0.536042\n",
      "[600]\ttraining's binary_logloss: 0.498142\tvalid_1's binary_logloss: 0.534523\n",
      "[700]\ttraining's binary_logloss: 0.492049\tvalid_1's binary_logloss: 0.533657\n",
      "[800]\ttraining's binary_logloss: 0.486517\tvalid_1's binary_logloss: 0.533014\n",
      "[900]\ttraining's binary_logloss: 0.481508\tvalid_1's binary_logloss: 0.53254\n",
      "[1000]\ttraining's binary_logloss: 0.476896\tvalid_1's binary_logloss: 0.532185\n",
      "[1100]\ttraining's binary_logloss: 0.472634\tvalid_1's binary_logloss: 0.531954\n",
      "[1200]\ttraining's binary_logloss: 0.468779\tvalid_1's binary_logloss: 0.531812\n",
      "[1300]\ttraining's binary_logloss: 0.465004\tvalid_1's binary_logloss: 0.5316\n",
      "[1400]\ttraining's binary_logloss: 0.461462\tvalid_1's binary_logloss: 0.531446\n",
      "[1500]\ttraining's binary_logloss: 0.458295\tvalid_1's binary_logloss: 0.531427\n",
      "[1600]\ttraining's binary_logloss: 0.455235\tvalid_1's binary_logloss: 0.531387\n",
      "[1700]\ttraining's binary_logloss: 0.45251\tvalid_1's binary_logloss: 0.531304\n",
      "[1800]\ttraining's binary_logloss: 0.449582\tvalid_1's binary_logloss: 0.531321\n",
      "Early stopping, best iteration is:\n",
      "[1720]\ttraining's binary_logloss: 0.451874\tvalid_1's binary_logloss: 0.531283\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122423, number of negative: 66067\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.240345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 465732\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 2144\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649493 -> initscore=0.616813\n",
      "[LightGBM] [Info] Start training from score 0.616813\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.571934\tvalid_1's binary_logloss: 0.578999\n",
      "[200]\ttraining's binary_logloss: 0.540037\tvalid_1's binary_logloss: 0.553962\n",
      "[300]\ttraining's binary_logloss: 0.523919\tvalid_1's binary_logloss: 0.544306\n",
      "[400]\ttraining's binary_logloss: 0.512931\tvalid_1's binary_logloss: 0.539595\n",
      "[500]\ttraining's binary_logloss: 0.504745\tvalid_1's binary_logloss: 0.537406\n",
      "[600]\ttraining's binary_logloss: 0.497816\tvalid_1's binary_logloss: 0.536055\n",
      "[700]\ttraining's binary_logloss: 0.491738\tvalid_1's binary_logloss: 0.535271\n",
      "[800]\ttraining's binary_logloss: 0.486242\tvalid_1's binary_logloss: 0.534812\n",
      "[900]\ttraining's binary_logloss: 0.481236\tvalid_1's binary_logloss: 0.534418\n",
      "[1000]\ttraining's binary_logloss: 0.476567\tvalid_1's binary_logloss: 0.534166\n",
      "[1100]\ttraining's binary_logloss: 0.472305\tvalid_1's binary_logloss: 0.533987\n",
      "[1200]\ttraining's binary_logloss: 0.468371\tvalid_1's binary_logloss: 0.533874\n",
      "[1300]\ttraining's binary_logloss: 0.464605\tvalid_1's binary_logloss: 0.533794\n",
      "[1400]\ttraining's binary_logloss: 0.461144\tvalid_1's binary_logloss: 0.533792\n",
      "Early stopping, best iteration is:\n",
      "[1306]\ttraining's binary_logloss: 0.464398\tvalid_1's binary_logloss: 0.533785\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122249, number of negative: 66251\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.605064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 466028\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 2144\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.648536 -> initscore=0.612609\n",
      "[LightGBM] [Info] Start training from score 0.612609\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572366\tvalid_1's binary_logloss: 0.577259\n",
      "[200]\ttraining's binary_logloss: 0.540412\tvalid_1's binary_logloss: 0.552655\n",
      "[300]\ttraining's binary_logloss: 0.524327\tvalid_1's binary_logloss: 0.543298\n",
      "[400]\ttraining's binary_logloss: 0.51339\tvalid_1's binary_logloss: 0.538527\n",
      "[500]\ttraining's binary_logloss: 0.505166\tvalid_1's binary_logloss: 0.536249\n",
      "[600]\ttraining's binary_logloss: 0.498241\tvalid_1's binary_logloss: 0.534933\n",
      "[700]\ttraining's binary_logloss: 0.492206\tvalid_1's binary_logloss: 0.534115\n",
      "[800]\ttraining's binary_logloss: 0.486741\tvalid_1's binary_logloss: 0.533672\n",
      "[900]\ttraining's binary_logloss: 0.481774\tvalid_1's binary_logloss: 0.533361\n",
      "[1000]\ttraining's binary_logloss: 0.477176\tvalid_1's binary_logloss: 0.533051\n",
      "[1100]\ttraining's binary_logloss: 0.472948\tvalid_1's binary_logloss: 0.532859\n",
      "[1200]\ttraining's binary_logloss: 0.469089\tvalid_1's binary_logloss: 0.532636\n",
      "[1300]\ttraining's binary_logloss: 0.465361\tvalid_1's binary_logloss: 0.532555\n",
      "[1400]\ttraining's binary_logloss: 0.461901\tvalid_1's binary_logloss: 0.532451\n",
      "[1500]\ttraining's binary_logloss: 0.45867\tvalid_1's binary_logloss: 0.532424\n",
      "[1600]\ttraining's binary_logloss: 0.455606\tvalid_1's binary_logloss: 0.532366\n",
      "[1700]\ttraining's binary_logloss: 0.452947\tvalid_1's binary_logloss: 0.532286\n",
      "[1800]\ttraining's binary_logloss: 0.450052\tvalid_1's binary_logloss: 0.532245\n",
      "[1900]\ttraining's binary_logloss: 0.447379\tvalid_1's binary_logloss: 0.532202\n",
      "[2000]\ttraining's binary_logloss: 0.444827\tvalid_1's binary_logloss: 0.532252\n",
      "Early stopping, best iteration is:\n",
      "[1932]\ttraining's binary_logloss: 0.446508\tvalid_1's binary_logloss: 0.532178\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122566, number of negative: 65934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.862139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 466028\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 2143\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650218 -> initscore=0.619995\n",
      "[LightGBM] [Info] Start training from score 0.619995\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.576642\tvalid_1's binary_logloss: 0.584753\n",
      "[200]\ttraining's binary_logloss: 0.543917\tvalid_1's binary_logloss: 0.557736\n",
      "[300]\ttraining's binary_logloss: 0.525439\tvalid_1's binary_logloss: 0.544645\n",
      "[400]\ttraining's binary_logloss: 0.514218\tvalid_1's binary_logloss: 0.53894\n",
      "[500]\ttraining's binary_logloss: 0.505775\tvalid_1's binary_logloss: 0.535844\n",
      "[600]\ttraining's binary_logloss: 0.498797\tvalid_1's binary_logloss: 0.534131\n",
      "[700]\ttraining's binary_logloss: 0.492755\tvalid_1's binary_logloss: 0.533049\n",
      "[800]\ttraining's binary_logloss: 0.487251\tvalid_1's binary_logloss: 0.532337\n",
      "[900]\ttraining's binary_logloss: 0.482235\tvalid_1's binary_logloss: 0.531852\n",
      "[1000]\ttraining's binary_logloss: 0.477606\tvalid_1's binary_logloss: 0.531479\n",
      "[1100]\ttraining's binary_logloss: 0.473328\tvalid_1's binary_logloss: 0.531228\n",
      "[1200]\ttraining's binary_logloss: 0.469369\tvalid_1's binary_logloss: 0.530955\n",
      "[1300]\ttraining's binary_logloss: 0.465674\tvalid_1's binary_logloss: 0.530706\n",
      "[1400]\ttraining's binary_logloss: 0.462213\tvalid_1's binary_logloss: 0.530559\n",
      "[1500]\ttraining's binary_logloss: 0.45898\tvalid_1's binary_logloss: 0.530455\n",
      "[1600]\ttraining's binary_logloss: 0.455902\tvalid_1's binary_logloss: 0.530396\n",
      "[1700]\ttraining's binary_logloss: 0.453071\tvalid_1's binary_logloss: 0.530413\n",
      "[1800]\ttraining's binary_logloss: 0.4503\tvalid_1's binary_logloss: 0.530389\n",
      "[1900]\ttraining's binary_logloss: 0.447769\tvalid_1's binary_logloss: 0.530327\n",
      "[2000]\ttraining's binary_logloss: 0.445089\tvalid_1's binary_logloss: 0.530213\n",
      "[2100]\ttraining's binary_logloss: 0.442564\tvalid_1's binary_logloss: 0.53016\n",
      "[2200]\ttraining's binary_logloss: 0.440163\tvalid_1's binary_logloss: 0.530196\n",
      "Early stopping, best iteration is:\n",
      "[2123]\ttraining's binary_logloss: 0.441975\tvalid_1's binary_logloss: 0.530135\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122635, number of negative: 65865\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.243746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 465671\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 2144\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650584 -> initscore=0.621605\n",
      "[LightGBM] [Info] Start training from score 0.621605\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.571554\tvalid_1's binary_logloss: 0.580609\n",
      "[200]\ttraining's binary_logloss: 0.539961\tvalid_1's binary_logloss: 0.554838\n",
      "[300]\ttraining's binary_logloss: 0.523978\tvalid_1's binary_logloss: 0.544863\n",
      "[400]\ttraining's binary_logloss: 0.513106\tvalid_1's binary_logloss: 0.53968\n",
      "[500]\ttraining's binary_logloss: 0.504879\tvalid_1's binary_logloss: 0.537029\n",
      "[600]\ttraining's binary_logloss: 0.497918\tvalid_1's binary_logloss: 0.53541\n",
      "[700]\ttraining's binary_logloss: 0.491854\tvalid_1's binary_logloss: 0.534486\n",
      "[800]\ttraining's binary_logloss: 0.486343\tvalid_1's binary_logloss: 0.533847\n",
      "[900]\ttraining's binary_logloss: 0.481325\tvalid_1's binary_logloss: 0.533458\n",
      "[1000]\ttraining's binary_logloss: 0.476738\tvalid_1's binary_logloss: 0.53313\n",
      "[1100]\ttraining's binary_logloss: 0.47251\tvalid_1's binary_logloss: 0.532943\n",
      "[1200]\ttraining's binary_logloss: 0.468615\tvalid_1's binary_logloss: 0.532811\n",
      "[1300]\ttraining's binary_logloss: 0.464889\tvalid_1's binary_logloss: 0.532689\n",
      "[1400]\ttraining's binary_logloss: 0.461429\tvalid_1's binary_logloss: 0.532559\n",
      "[1500]\ttraining's binary_logloss: 0.458186\tvalid_1's binary_logloss: 0.532491\n",
      "[1600]\ttraining's binary_logloss: 0.455093\tvalid_1's binary_logloss: 0.532446\n",
      "[1700]\ttraining's binary_logloss: 0.452328\tvalid_1's binary_logloss: 0.532383\n",
      "[1800]\ttraining's binary_logloss: 0.449454\tvalid_1's binary_logloss: 0.532298\n",
      "[1900]\ttraining's binary_logloss: 0.446732\tvalid_1's binary_logloss: 0.53229\n",
      "Early stopping, best iteration is:\n",
      "[1869]\ttraining's binary_logloss: 0.447546\tvalid_1's binary_logloss: 0.532265\n",
      "13-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23562/23562 [04:35<00:00, 85.47it/s] \n",
      "100%|██████████| 23562/23562 [04:27<00:00, 88.14it/s]\n",
      "100%|██████████| 23562/23562 [04:23<00:00, 89.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67313, number of negative: 26932\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.115711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 840781\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 3826\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.714234 -> initscore=0.916038\n",
      "[LightGBM] [Info] Start training from score 0.916038\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.54431\tvalid_1's binary_logloss: 0.555453\n",
      "[200]\ttraining's binary_logloss: 0.514347\tvalid_1's binary_logloss: 0.530436\n",
      "[300]\ttraining's binary_logloss: 0.494513\tvalid_1's binary_logloss: 0.517125\n",
      "[400]\ttraining's binary_logloss: 0.481761\tvalid_1's binary_logloss: 0.512185\n",
      "[500]\ttraining's binary_logloss: 0.471192\tvalid_1's binary_logloss: 0.509368\n",
      "[600]\ttraining's binary_logloss: 0.462283\tvalid_1's binary_logloss: 0.508017\n",
      "[700]\ttraining's binary_logloss: 0.45409\tvalid_1's binary_logloss: 0.507139\n",
      "[800]\ttraining's binary_logloss: 0.446567\tvalid_1's binary_logloss: 0.506565\n",
      "[900]\ttraining's binary_logloss: 0.439853\tvalid_1's binary_logloss: 0.506251\n",
      "[1000]\ttraining's binary_logloss: 0.433967\tvalid_1's binary_logloss: 0.506071\n",
      "[1100]\ttraining's binary_logloss: 0.428608\tvalid_1's binary_logloss: 0.505904\n",
      "[1200]\ttraining's binary_logloss: 0.423579\tvalid_1's binary_logloss: 0.505783\n",
      "[1300]\ttraining's binary_logloss: 0.418774\tvalid_1's binary_logloss: 0.505659\n",
      "[1400]\ttraining's binary_logloss: 0.414275\tvalid_1's binary_logloss: 0.50571\n",
      "Early stopping, best iteration is:\n",
      "[1336]\ttraining's binary_logloss: 0.416985\tvalid_1's binary_logloss: 0.505571\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 66951, number of negative: 27294\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.343711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 840667\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 3826\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710393 -> initscore=0.897294\n",
      "[LightGBM] [Info] Start training from score 0.897294\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.547618\tvalid_1's binary_logloss: 0.543354\n",
      "[200]\ttraining's binary_logloss: 0.517526\tvalid_1's binary_logloss: 0.519397\n",
      "[300]\ttraining's binary_logloss: 0.497666\tvalid_1's binary_logloss: 0.506579\n",
      "[400]\ttraining's binary_logloss: 0.484876\tvalid_1's binary_logloss: 0.501701\n",
      "[500]\ttraining's binary_logloss: 0.474399\tvalid_1's binary_logloss: 0.499109\n",
      "[600]\ttraining's binary_logloss: 0.465618\tvalid_1's binary_logloss: 0.497928\n",
      "[700]\ttraining's binary_logloss: 0.457287\tvalid_1's binary_logloss: 0.496749\n",
      "[800]\ttraining's binary_logloss: 0.449796\tvalid_1's binary_logloss: 0.496106\n",
      "[900]\ttraining's binary_logloss: 0.443095\tvalid_1's binary_logloss: 0.495635\n",
      "[1000]\ttraining's binary_logloss: 0.437134\tvalid_1's binary_logloss: 0.495398\n",
      "[1100]\ttraining's binary_logloss: 0.431623\tvalid_1's binary_logloss: 0.495377\n",
      "[1200]\ttraining's binary_logloss: 0.426556\tvalid_1's binary_logloss: 0.495259\n",
      "[1300]\ttraining's binary_logloss: 0.421761\tvalid_1's binary_logloss: 0.495088\n",
      "[1400]\ttraining's binary_logloss: 0.41745\tvalid_1's binary_logloss: 0.495137\n",
      "Early stopping, best iteration is:\n",
      "[1311]\ttraining's binary_logloss: 0.421219\tvalid_1's binary_logloss: 0.495087\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlocal_cv\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     run_train()\n\u001b[1;32m      3\u001b[0m \u001b[39m#inference(cfg.mode)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m tr_data \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(tr_x, label\u001b[39m=\u001b[39mtr_y)\n\u001b[1;32m     38\u001b[0m vl_data \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(vl_x, label\u001b[39m=\u001b[39mvl_y)\n\u001b[0;32m---> 40\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(params, tr_data, valid_sets\u001b[39m=\u001b[39;49m[tr_data, vl_data],\n\u001b[1;32m     41\u001b[0m                 num_boost_round\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m, early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, verbose_eval\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m     42\u001b[0m \u001b[39m# モデル出力\u001b[39;00m\n\u001b[1;32m     43\u001b[0m model\u001b[39m.\u001b[39msave_model(cfg\u001b[39m.\u001b[39moutput_dir \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mexp_name\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mexp_name\u001b[39m}\u001b[39;00m\u001b[39m_model_\u001b[39m\u001b[39m{\u001b[39;00mgroup\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.lgb\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[39m=\u001b[39m Booster(params\u001b[39m=\u001b[39;49mparams, train_set\u001b[39m=\u001b[39;49mtrain_set)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[39m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_network(\n\u001b[1;32m   2599\u001b[0m         machines\u001b[39m=\u001b[39mmachines,\n\u001b[1;32m   2600\u001b[0m         local_listen_port\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mlocal_listen_port\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   2601\u001b[0m         listen_time_out\u001b[39m=\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_out\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m120\u001b[39m),\n\u001b[1;32m   2602\u001b[0m         num_machines\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mnum_machines\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2603\u001b[0m     )\n\u001b[1;32m   2604\u001b[0m \u001b[39m# construct booster object\u001b[39;00m\n\u001b[0;32m-> 2605\u001b[0m train_set\u001b[39m.\u001b[39;49mconstruct()\n\u001b[1;32m   2606\u001b[0m \u001b[39m# copy the parameters from train_set\u001b[39;00m\n\u001b[1;32m   2607\u001b[0m params\u001b[39m.\u001b[39mupdate(train_set\u001b[39m.\u001b[39mget_params())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_init_score_by_predictor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predictor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, used_indices)\n\u001b[1;32m   1813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[39m# create train\u001b[39;00m\n\u001b[0;32m-> 1815\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel,\n\u001b[1;32m   1816\u001b[0m                     weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, group\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup,\n\u001b[1;32m   1817\u001b[0m                     init_score\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_score, predictor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predictor,\n\u001b[1;32m   1818\u001b[0m                     silent\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msilent, feature_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_name,\n\u001b[1;32m   1819\u001b[0m                     categorical_feature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategorical_feature, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m   1820\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfree_raw_data:\n\u001b[1;32m   1821\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1538\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__init_from_csc(data, params_str, ref_dataset)\n\u001b[1;32m   1537\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m-> 1538\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__init_from_np2d(data, params_str, ref_dataset)\n\u001b[1;32m   1539\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(x, np\u001b[39m.\u001b[39mndarray) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1659\u001b[0m, in \u001b[0;36mDataset.__init_from_np2d\u001b[0;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m   1656\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(mat\u001b[39m.\u001b[39mreshape(mat\u001b[39m.\u001b[39msize), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m   1658\u001b[0m ptr_data, type_ptr_data, _ \u001b[39m=\u001b[39m c_float_array(data)\n\u001b[0;32m-> 1659\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_DatasetCreateFromMat(\n\u001b[1;32m   1660\u001b[0m     ptr_data,\n\u001b[1;32m   1661\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(type_ptr_data),\n\u001b[1;32m   1662\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]),\n\u001b[1;32m   1663\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int32(mat\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]),\n\u001b[1;32m   1664\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(C_API_IS_ROW_MAJOR),\n\u001b[1;32m   1665\u001b[0m     c_str(params_str),\n\u001b[1;32m   1666\u001b[0m     ref_dataset,\n\u001b[1;32m   1667\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle)))\n\u001b[1;32m   1668\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    run_train()\n",
    "#inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
