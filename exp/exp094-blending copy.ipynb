{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp094"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp094のblending"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各level_groupに閉じた形 : 0.699077  \n",
    "level_group跨ぎ : 0.698151  \n",
    "questionをcategoryに : 0.698698  \n",
    "class_weight : 0.698593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp094-blending\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "    base_exp = None # 特徴量重要度を使う元のexp\n",
    "    n_features = 500 # 特徴量削減の数\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "    import cudf\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof1 = pd.read_csv(cfg.output_dir + \"exp094/oof.csv.gz\")\n",
    "oof2 = pd.read_csv(cfg.output_dir + \"exp094-2/oof.csv.gz\")\n",
    "oof3 = pd.read_csv(cfg.output_dir + \"exp094-3/oof.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(cfg.input_dir + \"train_labels.csv\")\n",
    "train = transform_labels_df_train(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0_4 = train[train[\"level_group\"]==\"0-4\"].reset_index(drop=True)\n",
    "train5_12 = train[train[\"level_group\"]==\"5-12\"].reset_index(drop=True)\n",
    "train13_22 = train[train[\"level_group\"]==\"13-22\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"exp094\"\n",
    "oof_pivot = pd.pivot_table(oof1, index=\"session_id\", columns=\"question\", values=\"pred\")\n",
    "new_cols = [f\"{exp_name}_pred_q\" + str(i) for i in oof_pivot.columns]\n",
    "oof_pivot.columns = new_cols\n",
    "train0_4 = train0_4.merge(oof_pivot[[f\"{exp_name}_pred_q\" + str(i) for i in [1,2,3]]], on=\"session_id\", how=\"left\")\n",
    "train5_12 = train5_12.merge(oof_pivot[[f\"{exp_name}_pred_q\" + str(i) for i in [1,2,3,4,5,6,7,8,9,10,11,12,13]]], on=\"session_id\", how=\"left\")\n",
    "train13_22 = train13_22.merge(oof_pivot[[f\"{exp_name}_pred_q\" + str(i) for i in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]]], on=\"session_id\", how=\"left\")\n",
    "\n",
    "exp_name = \"exp094-2\"\n",
    "oof_pivot = pd.pivot_table(oof2, index=\"session_id\", columns=\"question\", values=\"pred\")\n",
    "new_cols = [f\"{exp_name}_pred_q\" + str(i) for i in oof_pivot.columns]\n",
    "oof_pivot.columns = new_cols\n",
    "train0_4 = train0_4.merge(oof_pivot[[f\"{exp_name}_pred_q\" + str(i) for i in [1,2,3]]], on=\"session_id\", how=\"left\")\n",
    "train5_12 = train5_12.merge(oof_pivot[[f\"{exp_name}_pred_q\" + str(i) for i in [1,2,3,4,5,6,7,8,9,10,11,12,13]]], on=\"session_id\", how=\"left\")\n",
    "train13_22 = train13_22.merge(oof_pivot[[f\"{exp_name}_pred_q\" + str(i) for i in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]]], on=\"session_id\", how=\"left\")\n",
    "\n",
    "exp_name = \"exp094-3\"\n",
    "oof_pivot = pd.pivot_table(oof3, index=\"session_id\", columns=\"question\", values=\"pred\")\n",
    "new_cols = [f\"{exp_name}_pred_q\" + str(i) for i in oof_pivot.columns]\n",
    "oof_pivot.columns = new_cols\n",
    "train0_4 = train0_4.merge(oof_pivot[[f\"{exp_name}_pred_q\" + str(i) for i in [1,2,3]]], on=\"session_id\", how=\"left\")\n",
    "train5_12 = train5_12.merge(oof_pivot[[f\"{exp_name}_pred_q\" + str(i) for i in [1,2,3,4,5,6,7,8,9,10,11,12,13]]], on=\"session_id\", how=\"left\")\n",
    "train13_22 = train13_22.merge(oof_pivot[[f\"{exp_name}_pred_q\" + str(i) for i in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]]], on=\"session_id\", how=\"left\")\n",
    "\n",
    "train = pd.concat([train0_4, train5_12, train13_22], ignore_index=True)\n",
    "train[\"question\"] = train[\"question\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = len(train)/len(train[train[\"correct\"] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = len(train)/len(train[train[\"correct\"] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"w\"] = 0.0\n",
    "train.loc[train[\"correct\"]==0, \"w\"] = w0\n",
    "train.loc[train[\"correct\"]==1, \"w\"] = w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary', \n",
    "        'boosting': 'gbdt', \n",
    "        'learning_rate': 0.01, \n",
    "        'metric': 'binary_logloss', \n",
    "        'seed': cfg.seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"correct\"\n",
    "not_use_cols = [target, \"session_id\", \"level_group\", \"w\"]\n",
    "features = [c for c in train.columns if c not in not_use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 0\n",
      "[LightGBM] [Info] Number of positive: 239789, number of negative: 99493\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13789\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501349 -> initscore=0.005396\n",
      "[LightGBM] [Info] Start training from score 0.005396\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.573053\tvalid_1's binary_logloss: 0.573039\n",
      "[200]\ttraining's binary_logloss: 0.546361\tvalid_1's binary_logloss: 0.5469\n",
      "[300]\ttraining's binary_logloss: 0.539044\tvalid_1's binary_logloss: 0.540131\n",
      "[400]\ttraining's binary_logloss: 0.536213\tvalid_1's binary_logloss: 0.538117\n",
      "[500]\ttraining's binary_logloss: 0.534508\tvalid_1's binary_logloss: 0.537589\n",
      "[600]\ttraining's binary_logloss: 0.533153\tvalid_1's binary_logloss: 0.537383\n",
      "[700]\ttraining's binary_logloss: 0.531995\tvalid_1's binary_logloss: 0.537275\n",
      "[800]\ttraining's binary_logloss: 0.530889\tvalid_1's binary_logloss: 0.537258\n",
      "Early stopping, best iteration is:\n",
      "[792]\ttraining's binary_logloss: 0.530977\tvalid_1's binary_logloss: 0.537253\n",
      "fold : 1\n",
      "[LightGBM] [Info] Number of positive: 239066, number of negative: 100216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13789\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498784 -> initscore=-0.004865\n",
      "[LightGBM] [Info] Start training from score -0.004865\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572369\tvalid_1's binary_logloss: 0.575256\n",
      "[200]\ttraining's binary_logloss: 0.545553\tvalid_1's binary_logloss: 0.549529\n",
      "[300]\ttraining's binary_logloss: 0.538152\tvalid_1's binary_logloss: 0.542967\n",
      "[400]\ttraining's binary_logloss: 0.535365\tvalid_1's binary_logloss: 0.541178\n",
      "[500]\ttraining's binary_logloss: 0.533614\tvalid_1's binary_logloss: 0.540619\n",
      "[600]\ttraining's binary_logloss: 0.532281\tvalid_1's binary_logloss: 0.540441\n",
      "[700]\ttraining's binary_logloss: 0.531117\tvalid_1's binary_logloss: 0.540337\n",
      "[800]\ttraining's binary_logloss: 0.53006\tvalid_1's binary_logloss: 0.540321\n",
      "[900]\ttraining's binary_logloss: 0.529017\tvalid_1's binary_logloss: 0.540293\n",
      "[1000]\ttraining's binary_logloss: 0.527999\tvalid_1's binary_logloss: 0.540254\n",
      "[1100]\ttraining's binary_logloss: 0.526982\tvalid_1's binary_logloss: 0.54022\n",
      "[1200]\ttraining's binary_logloss: 0.52602\tvalid_1's binary_logloss: 0.540214\n",
      "Early stopping, best iteration is:\n",
      "[1172]\ttraining's binary_logloss: 0.526291\tvalid_1's binary_logloss: 0.5402\n",
      "fold : 2\n",
      "[LightGBM] [Info] Number of positive: 239010, number of negative: 100290\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13789\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498541 -> initscore=-0.005837\n",
      "[LightGBM] [Info] Start training from score -0.005837\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572579\tvalid_1's binary_logloss: 0.574517\n",
      "[200]\ttraining's binary_logloss: 0.545848\tvalid_1's binary_logloss: 0.549231\n",
      "[300]\ttraining's binary_logloss: 0.53834\tvalid_1's binary_logloss: 0.542813\n",
      "[400]\ttraining's binary_logloss: 0.53554\tvalid_1's binary_logloss: 0.541063\n",
      "[500]\ttraining's binary_logloss: 0.533831\tvalid_1's binary_logloss: 0.54046\n",
      "[600]\ttraining's binary_logloss: 0.532529\tvalid_1's binary_logloss: 0.540243\n",
      "[700]\ttraining's binary_logloss: 0.531408\tvalid_1's binary_logloss: 0.540159\n",
      "[800]\ttraining's binary_logloss: 0.530349\tvalid_1's binary_logloss: 0.540112\n",
      "[900]\ttraining's binary_logloss: 0.529285\tvalid_1's binary_logloss: 0.540081\n",
      "[1000]\ttraining's binary_logloss: 0.52824\tvalid_1's binary_logloss: 0.540083\n",
      "[1100]\ttraining's binary_logloss: 0.527238\tvalid_1's binary_logloss: 0.540076\n",
      "Early stopping, best iteration is:\n",
      "[1070]\ttraining's binary_logloss: 0.527519\tvalid_1's binary_logloss: 0.540071\n",
      "fold : 3\n",
      "[LightGBM] [Info] Number of positive: 239535, number of negative: 99765\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13789\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500401 -> initscore=0.001606\n",
      "[LightGBM] [Info] Start training from score 0.001606\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572816\tvalid_1's binary_logloss: 0.57363\n",
      "[200]\ttraining's binary_logloss: 0.546185\tvalid_1's binary_logloss: 0.547784\n",
      "[300]\ttraining's binary_logloss: 0.538724\tvalid_1's binary_logloss: 0.541232\n",
      "[400]\ttraining's binary_logloss: 0.53585\tvalid_1's binary_logloss: 0.539491\n",
      "[500]\ttraining's binary_logloss: 0.534093\tvalid_1's binary_logloss: 0.539023\n",
      "[600]\ttraining's binary_logloss: 0.532772\tvalid_1's binary_logloss: 0.538954\n",
      "[700]\ttraining's binary_logloss: 0.531686\tvalid_1's binary_logloss: 0.538951\n",
      "Early stopping, best iteration is:\n",
      "[630]\ttraining's binary_logloss: 0.532417\tvalid_1's binary_logloss: 0.538943\n",
      "fold : 4\n",
      "[LightGBM] [Info] Number of positive: 239684, number of negative: 99616\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13789\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500931 -> initscore=0.003722\n",
      "[LightGBM] [Info] Start training from score 0.003722\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572919\tvalid_1's binary_logloss: 0.573723\n",
      "[200]\ttraining's binary_logloss: 0.546339\tvalid_1's binary_logloss: 0.547476\n",
      "[300]\ttraining's binary_logloss: 0.539021\tvalid_1's binary_logloss: 0.540447\n",
      "[400]\ttraining's binary_logloss: 0.536246\tvalid_1's binary_logloss: 0.538358\n",
      "[500]\ttraining's binary_logloss: 0.534598\tvalid_1's binary_logloss: 0.537669\n",
      "[600]\ttraining's binary_logloss: 0.533322\tvalid_1's binary_logloss: 0.53744\n",
      "[700]\ttraining's binary_logloss: 0.532194\tvalid_1's binary_logloss: 0.537343\n",
      "[800]\ttraining's binary_logloss: 0.531067\tvalid_1's binary_logloss: 0.537314\n",
      "[900]\ttraining's binary_logloss: 0.529929\tvalid_1's binary_logloss: 0.537306\n",
      "[1000]\ttraining's binary_logloss: 0.528856\tvalid_1's binary_logloss: 0.537304\n",
      "[1100]\ttraining's binary_logloss: 0.527835\tvalid_1's binary_logloss: 0.537334\n",
      "Early stopping, best iteration is:\n",
      "[1025]\ttraining's binary_logloss: 0.528609\tvalid_1's binary_logloss: 0.537299\n",
      "logloss 0.540689\n",
      "best_score 0.698593\n",
      "best_threshold 0.410\n",
      "------------------------------\n",
      "Q1 : F1 = 0.451225\n",
      "Q2 : F1 = 0.592170\n",
      "Q3 : F1 = 0.572936\n",
      "Q4 : F1 = 0.477636\n",
      "Q5 : F1 = 0.314588\n",
      "Q6 : F1 = 0.431477\n",
      "Q7 : F1 = 0.352202\n",
      "Q8 : F1 = 0.279888\n",
      "Q9 : F1 = 0.341766\n",
      "Q10 : F1 = 0.330910\n",
      "Q11 : F1 = 0.269715\n",
      "Q12 : F1 = 0.427806\n",
      "Q13 : F1 = 0.420255\n",
      "Q14 : F1 = 0.273181\n",
      "Q15 : F1 = 0.343946\n",
      "Q16 : F1 = 0.223044\n",
      "Q17 : F1 = 0.242655\n",
      "Q18 : F1 = 0.570210\n"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "fis = []\n",
    "\n",
    "oofs = []\n",
    "for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "    model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{i}.lgb\"\n",
    "    \n",
    "    print(f\"fold : {i}\")\n",
    "    tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "    vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "    tr_data = lgb.Dataset(tr_x, label=tr_y, weight=train.iloc[tr_idx][\"w\"])\n",
    "    vl_data = lgb.Dataset(vl_x, label=vl_y, weight=train.iloc[vl_idx][\"w\"])\n",
    "\n",
    "\n",
    "    model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                    num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)\n",
    "    # モデル出力\n",
    "    model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{i}.lgb\")\n",
    "\n",
    "    # valid_pred\n",
    "    oof_fold = train.iloc[vl_idx].copy()\n",
    "    oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "    oofs.append(oof_fold)\n",
    "\n",
    "    # 特徴量重要度\n",
    "    fi_fold = pd.DataFrame()\n",
    "    fi_fold[\"feature\"] = model.feature_name()\n",
    "    fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "    fi_fold[\"fold\"] = i\n",
    "    fis.append(fi_fold)\n",
    "\n",
    "fi = pd.concat(fis)    \n",
    "fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi.csv\", index=False)\n",
    "\n",
    "\n",
    "# cv\n",
    "oof = pd.concat(oofs)\n",
    "best_threshold = calc_metrics(oof)\n",
    "cfg.best_threshold = best_threshold\n",
    "oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
