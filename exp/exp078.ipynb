{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp078"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "event_name+fqid追加  \n",
    "特徴量削減処理の高速化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import polars as pl\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp078\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "    base_exp = None # 特徴量重要度を使う元のexp\n",
    "    n_features = 500 # 特徴量削減の数\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.01, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    with open(cfg.prep_dir + 'cat_col_lists_v2.pkl', 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) \n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    with open(\"/kaggle/input/psp-cat-col-lists/cat_col_lists_v2.pkl\", 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTrain:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"session_id\", \"level_group\", \"index\"], ignore_index=True)\n",
    "        self.features = self.sessions_df[[\"session_id\", \"level_group\"]].drop_duplicates().copy()\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].shift(1)\n",
    "        self.sessions_df[\"time_diff\"] = np.where(self.sessions_df[\"time_diff\"]<0, 0, self.sessions_df[\"time_diff\"])\n",
    "        self.sessions_df[\"time_diff\"] = np.nan_to_num(self.sessions_df[\"time_diff\"], 0)\n",
    "        self.sessions_df[\"event_name+name\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"name\"]\n",
    "        self.sessions_df[\"event_name+room_fqid\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"room_fqid\"]\n",
    "        self.sessions_df[\"event_name+fqid\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"fqid\"]\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_record_cnt\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].agg([max,min]).reset_index()\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[\"max\"] - add_features[\"min\"]\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[f\"{self.group}_group_elapsed_time\"].astype(np.float32)\n",
    "        add_features = add_features[[\"session_id\", \"level_group\", f\"{self.group}_group_elapsed_time\"]].copy()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[\"index\"].count().reset_index().rename(columns={\"index\":\"cnt\"})\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            tmp = add_features[add_features[cat_col]==cat][[\"session_id\", \"level_group\", \"cnt\"]].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp = tmp.rename(columns={\"cnt\": feat_name})\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[feat_name] = self.features[feat_name].fillna(0).astype(int)\n",
    "            else:\n",
    "                self.features[feat_name] = int(0)\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.dropna(subset=[cat_col]).drop_duplicates([\"session_id\", \"level_group\", cat_col])\n",
    "        add_features = add_features.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_{cat_col}_nunique\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")        \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        new_cols = [f\"{self.group}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{self.group}_{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "    def _cat_change_cnt(self, cat_col):\n",
    "        \"\"\"cat_colの変化回数\n",
    "        \"\"\"\n",
    "        tmp = self.sessions_df[[\"session_id\", \"level_group\", cat_col]].copy()\n",
    "        tmp[cat_col] = tmp[cat_col].fillna(\"nan\")\n",
    "        tmp[f\"{self.group}_{cat_col}_change_cnt\"] = (tmp[cat_col] != tmp.groupby([\"session_id\", \"level_group\"])[cat_col].shift(1)).astype(int)\n",
    "        add_features = tmp.groupby([\"session_id\", \"level_group\"])[f\"{self.group}_{cat_col}_change_cnt\"].sum().reset_index()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "\n",
    "    def _add_minigame_features(self, start_fqid, end_fqid):\n",
    "        game_name = start_fqid\n",
    "        dfs = []\n",
    "        for session_id in tqdm(self.sessions_df[\"session_id\"].unique()):\n",
    "            tmp = self.sessions_df[self.sessions_df[\"session_id\"]==session_id].copy()\n",
    "            start_indexes = tmp[(tmp[\"event_name\"]==\"navigate_click\")&(tmp[\"fqid\"]==start_fqid)][\"index\"].values\n",
    "            end_indexes = tmp[(tmp[\"event_name\"]==\"object_click\")&(tmp[\"fqid\"]==end_fqid)][\"index\"].values\n",
    "            if len(start_indexes) > 0:\n",
    "                start_index = start_indexes[0]\n",
    "            else:\n",
    "                start_index = np.nan\n",
    "            if len(end_indexes) > 0:\n",
    "                end_index = end_indexes[0]\n",
    "            else:\n",
    "                end_index = np.nan\n",
    "\n",
    "            if start_index < end_index:\n",
    "                mini_game_sessions = tmp[(tmp[\"index\"]>start_index)&(tmp[\"index\"]<=end_index)].copy()\n",
    "                record_cnt = len(mini_game_sessions)\n",
    "                total_duration = mini_game_sessions[\"time_diff\"].sum()\n",
    "                total_hover_duration = mini_game_sessions[\"hover_duration\"].sum()\n",
    "\n",
    "                hover_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_hover\"].copy()\n",
    "                if len(hover_sessions) > 0:\n",
    "                    hover_cnt = len(hover_sessions)\n",
    "                else:\n",
    "                    hover_cnt = 0\n",
    "\n",
    "                click_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_click\"].copy()\n",
    "                if len(click_sessions) > 0:\n",
    "                    click_cnt = len(click_sessions)\n",
    "                else:\n",
    "                    click_cnt = 0\n",
    "\n",
    "                feature_tmp = pd.DataFrame([[session_id, record_cnt, total_duration, total_hover_duration, hover_cnt, click_cnt]],\n",
    "                                            columns=[\"session_id\", f\"{self.group}_minigame_{game_name}_record_cnt\", f\"{self.group}_minigame_{game_name}_total_duration\", f\"{self.group}_minigame_{game_name}_total_hover_duration\",\n",
    "                                                    f\"{self.group}_minigame_{game_name}_hover_cnt\", f\"{self.group}_minigame_{game_name}_click_cnt\"]\n",
    "                                        )\n",
    "            else:\n",
    "                feature_tmp = pd.DataFrame([[session_id, 0, 0, 0, 0, 0]],\n",
    "                                            columns=[\"session_id\", f\"{self.group}_minigame_{game_name}_record_cnt\", f\"{self.group}_minigame_{game_name}_total_duration\", f\"{self.group}_minigame_{game_name}_total_hover_duration\",\n",
    "                                                    f\"{self.group}_minigame_{game_name}_hover_cnt\", f\"{self.group}_minigame_{game_name}_click_cnt\"]\n",
    "                                        )\n",
    "            dfs.append(feature_tmp)\n",
    "        add_features = pd.concat(dfs, ignore_index=True)\n",
    "        self.features = self.features.merge(add_features, on=\"session_id\", how=\"left\")\n",
    "\n",
    "\n",
    "    def get_train(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_record_cnt(\"event_name+name\")\n",
    "        self._cat_record_cnt(\"event_name+room_fqid\")\n",
    "        self._cat_record_cnt(\"event_name+fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "        self._cat_col_nunique(\"event_name+name\")\n",
    "        self._cat_col_nunique(\"event_name+room_fqid\")\n",
    "        self._cat_col_nunique(\"event_name+fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "        \n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"hover_duration\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['cutscene_click', 'person_click', 'navigate_click',\n",
    "                                             'observation_click', 'notification_click', 'object_click',\n",
    "                                             'map_click', 'checkpoint', 'notebook_click'])\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+name\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+room_fqid\") \n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"hover_duration\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+fqid\")    \n",
    "\n",
    "        \n",
    "        self._cat_change_cnt(\"text_fqid\")\n",
    "        self._cat_change_cnt(\"room_fqid\")\n",
    "\n",
    "        if self.group == \"0-4\":\n",
    "            self._add_minigame_features(\"tunic\", \"tunic.hub.slip\")\n",
    "            self._add_minigame_features(\"plaque\", \"plaque.face.date\")\n",
    "        \n",
    "        elif self.group == \"5-12\":\n",
    "            self._add_minigame_features(\"businesscards\", \"businesscards.card_bingo.bingo\")\n",
    "            self._add_minigame_features(\"logbook\", \"logbook.page.bingo\")\n",
    "            self._add_minigame_features(\"reader\", \"reader.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals\", \"journals.pic_2.bingo\")\n",
    "        \n",
    "        elif self.group == \"13-22\":\n",
    "            self._add_minigame_features(\"tracks\", \"tracks.hub.deer\")\n",
    "            self._add_minigame_features(\"reader_flag\", \"reader_flag.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals_flag\", \"journals_flag.pic_0.bingo\")\n",
    "        \n",
    "        self.result = self.result.merge(self.features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesInf:\n",
    "    def __init__(self, sessions_df, labels, feature_select=False, need_create_features=[]):\n",
    "        self.sessions_df = sessions_df.sort_values([\"index\"], ignore_index=True)\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.use_cols = [\n",
    "            \"elapsed_time\", \"event_name\", \"name\", \"level\", \"page\", \"index\",\n",
    "            \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\",\n",
    "            \"hover_duration\", \"text\", \"fqid\", \"room_fqid\", \"text_fqid\", \"event_name+name\", \"event_name+room_fqid\", \"time_diff\", \"event_name+fqid\"\n",
    "        ]\n",
    "        self.feature_select = feature_select\n",
    "        self.need_create_features = need_create_features\n",
    "\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"event_name+name\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"name\"]\n",
    "        self.sessions_df[\"event_name+room_fqid\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"room_fqid\"]\n",
    "        self.sessions_df[\"event_name+fqid\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"fqid\"]\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df[\"elapsed_time\"].shift(1).values\n",
    "        self.sessions_df[\"time_diff\"] = np.where(self.sessions_df[\"time_diff\"]<0, 0, self.sessions_df[\"time_diff\"])\n",
    "        self.sessions_df[\"time_diff\"] = np.nan_to_num(self.sessions_df[\"time_diff\"], 0)\n",
    "        # dataframeの各列をnumpy arrayで保持\n",
    "        self.sessions = {}\n",
    "        for c in self.use_cols:\n",
    "            self.sessions[c] = self.sessions_df[c].values\n",
    "        \n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        feat_name = f\"{self.group}_record_cnt\"\n",
    "        if self.feature_select & (feat_name not in self.need_create_features):\n",
    "            pass\n",
    "        else:\n",
    "            add_feature = len(self.sessions[\"elapsed_time\"])\n",
    "            self.result[feat_name] = add_feature\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        feat_name = f\"{self.group}_group_elapsed_time\"\n",
    "        if self.feature_select & (feat_name not in self.need_create_features):\n",
    "            pass\n",
    "        else:\n",
    "            add_feature = np.max(self.sessions[\"elapsed_time\"]) - np.min(self.sessions[\"elapsed_time\"])\n",
    "            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                pass\n",
    "            else:\n",
    "                add_feature = (self.sessions[cat_col] == cat).astype(int).sum()\n",
    "                self.result[feat_name] = add_feature\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        feat_name = f\"{self.group}_{cat_col}_nunique\"\n",
    "        if self.feature_select & (feat_name not in self.need_create_features):\n",
    "            pass\n",
    "        else:\n",
    "            self.result[feat_name] = self.sessions_df[cat_col].dropna().nunique()     \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        for val_col, agg in itertools.product(val_cols, aggs):\n",
    "            feat_name = f\"{self.group}_{val_col}_{agg}\"\n",
    "            if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                pass\n",
    "            else:\n",
    "                if agg == \"mean\":\n",
    "                    add_feature = np.nanmean(self.sessions[val_col])\n",
    "                elif agg == \"max\":\n",
    "                    add_feature = np.nanmax(self.sessions[val_col])\n",
    "                elif agg == \"min\":\n",
    "                    add_feature = np.nanmin(self.sessions[val_col])\n",
    "                elif agg == \"std\":\n",
    "                    add_feature = np.nanstd(self.sessions[val_col], ddof=1)\n",
    "                elif agg == \"sum\":\n",
    "                    add_feature = np.nansum(self.sessions[val_col])\n",
    "                elif agg == \"median\":\n",
    "                    add_feature = np.nanmedian(self.sessions[val_col])\n",
    "                self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp = self.sessions[val_col][idx]\n",
    "                        if agg == \"mean\":\n",
    "                            add_feature = np.nanmean(tmp)\n",
    "                        elif agg == \"max\":\n",
    "                            add_feature = np.nanmax(tmp)\n",
    "                        elif agg == \"min\":\n",
    "                            add_feature = np.nanmin(tmp)\n",
    "                        elif agg == \"std\":\n",
    "                            add_feature = np.nanstd(tmp, ddof=1)\n",
    "                        elif agg == \"sum\":\n",
    "                            add_feature = np.nansum(tmp)\n",
    "                        elif agg == \"median\":\n",
    "                            add_feature = np.nanmedian(tmp)\n",
    "                        if np.isnan(add_feature):\n",
    "                            self.result[feat_name] = np.float32(-1)\n",
    "                        else:\n",
    "                            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_change_cnt(self, cat_col):\n",
    "        \"\"\"cat_colの変化回数\n",
    "        \"\"\"\n",
    "        feat_name = f\"{self.group}_{cat_col}_change_cnt\"\n",
    "        if self.feature_select & (feat_name not in self.need_create_features):\n",
    "            pass\n",
    "        else:\n",
    "            tmp = self.sessions_df[cat_col].copy()\n",
    "            tmp = tmp.fillna(\"nan\")\n",
    "            self.result[feat_name] = (tmp != tmp.shift(1)).sum()\n",
    "\n",
    "\n",
    "    def _add_minigame_features(self, start_fqid, end_fqid):\n",
    "        game_name = start_fqid\n",
    "        start_indexes = self.sessions_df[(self.sessions_df[\"event_name\"]==\"navigate_click\")&(self.sessions_df[\"fqid\"]==start_fqid)][\"index\"].values\n",
    "        end_indexes = self.sessions_df[(self.sessions_df[\"event_name\"]==\"object_click\")&(self.sessions_df[\"fqid\"]==end_fqid)][\"index\"].values\n",
    "        if len(start_indexes) > 0:\n",
    "            start_index = start_indexes[0]\n",
    "        else:\n",
    "            start_index = np.nan\n",
    "        if len(end_indexes) > 0:\n",
    "            end_index = end_indexes[0]\n",
    "        else:\n",
    "            end_index = np.nan\n",
    "\n",
    "        if start_index < end_index:\n",
    "            mini_game_sessions = self.sessions_df[(self.sessions_df[\"index\"]>start_index)&(self.sessions_df[\"index\"]<=end_index)].copy()\n",
    "            record_cnt = len(mini_game_sessions)\n",
    "            total_duration = mini_game_sessions[\"time_diff\"].sum()\n",
    "            total_hover_duration = mini_game_sessions[\"hover_duration\"].sum()\n",
    "\n",
    "            hover_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_hover\"].copy()\n",
    "            if len(hover_sessions) > 0:\n",
    "                hover_cnt = len(hover_sessions)\n",
    "            else:\n",
    "                hover_cnt = 0\n",
    "\n",
    "            click_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_click\"].copy()\n",
    "            if len(click_sessions) > 0:\n",
    "                click_cnt = len(click_sessions)\n",
    "            else:\n",
    "                click_cnt = 0\n",
    "                                    \n",
    "        else:\n",
    "            record_cnt = 0\n",
    "            total_duration = 0\n",
    "            total_hover_duration = 0\n",
    "            hover_cnt = 0\n",
    "            click_cnt = 0\n",
    "        \n",
    "        self.result[f\"{self.group}_minigame_{game_name}_record_cnt\"] = record_cnt\n",
    "        self.result[f\"{self.group}_minigame_{game_name}_total_duration\"] = total_duration\n",
    "        self.result[f\"{self.group}_minigame_{game_name}_total_hover_duration\"] = total_hover_duration\n",
    "        self.result[f\"{self.group}_minigame_{game_name}_hover_cnt\"] = hover_cnt\n",
    "        self.result[f\"{self.group}_minigame_{game_name}_click_cnt\"] = click_cnt\n",
    "            \n",
    "\n",
    "    def get_test(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_record_cnt(\"event_name+name\")\n",
    "        self._cat_record_cnt(\"event_name+room_fqid\")\n",
    "        self._cat_record_cnt(\"event_name+fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "        self._cat_col_nunique(\"event_name+name\")\n",
    "        self._cat_col_nunique(\"event_name+room_fqid\")\n",
    "        self._cat_col_nunique(\"event_name+fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "        \n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"hover_duration\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['cutscene_click', 'person_click', 'navigate_click',\n",
    "                                             'observation_click', 'notification_click', 'object_click',\n",
    "                                             'map_click', 'checkpoint', 'notebook_click'])\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+name\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+room_fqid\") \n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"hover_duration\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+fqid\")   \n",
    "\n",
    "        \n",
    "        self._cat_change_cnt(\"text_fqid\")\n",
    "        self._cat_change_cnt(\"room_fqid\")\n",
    "\n",
    "        if self.group == \"0-4\":\n",
    "            self._add_minigame_features(\"tunic\", \"tunic.hub.slip\")\n",
    "            self._add_minigame_features(\"plaque\", \"plaque.face.date\")\n",
    "        \n",
    "        elif self.group == \"5-12\":\n",
    "            self._add_minigame_features(\"businesscards\", \"businesscards.card_bingo.bingo\")\n",
    "            self._add_minigame_features(\"logbook\", \"logbook.page.bingo\")\n",
    "            self._add_minigame_features(\"reader\", \"reader.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals\", \"journals.pic_2.bingo\")\n",
    "        \n",
    "        elif self.group == \"13-22\":\n",
    "            self._add_minigame_features(\"tracks\", \"tracks.hub.deer\")\n",
    "            self._add_minigame_features(\"reader_flag\", \"reader_flag.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals_flag\", \"journals_flag.pic_0.bingo\")\n",
    "        \n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesTrain(sessions, labels)\n",
    "    train = feat.get_train()\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels, feature_select=False, need_create_features=[]):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesInf(sessions, labels, feature_select, need_create_features)\n",
    "    test = feat.get_test()\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesSelect:\n",
    "    def __init__(self, df, init_features, corr_th=0.99):\n",
    "        self.init_features = init_features\n",
    "        self.df = cudf.from_pandas(df)\n",
    "        self.corr_th = corr_th\n",
    "        self.drop_cols = []\n",
    "    \n",
    "    def _high_corr_features_drop(self):\n",
    "        num_cols = self.df[self.init_features].select_dtypes(include=\"number\").columns\n",
    "\n",
    "        # 特徴量間の相関行列を計算\n",
    "        corr_matrix = self.df[num_cols].fillna(-1).corr().abs().to_pandas()\n",
    "        # 相関行列の上三角行列を取得します。（相関行列が対称であるため、重複する相関を取り除くため）\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        drop_cols = []\n",
    "        for c in num_cols:\n",
    "            if any(upper[c] > self.corr_th):\n",
    "                drop_cols.append(c)\n",
    "                upper = upper.drop(index=c)\n",
    "        print(f\"特徴量間の相関性が高い特徴量を{str(len(drop_cols))}個抽出\")\n",
    "        self.df = self.df.drop(columns=drop_cols)\n",
    "        self.drop_cols = list(set(self.drop_cols + drop_cols))\n",
    "\n",
    "    def features_select(self):\n",
    "        self._high_corr_features_drop()\n",
    "        selected_features = list(set(self.init_features) - set(self.drop_cols))\n",
    "        print(f\"{str(len(self.init_features))} -> {str(len(selected_features))}\")\n",
    "\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}_cleaned.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "    \n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "        # 特徴量選択\n",
    "        if cfg.base_exp is None:\n",
    "            features = FeaturesSelect(train, features).features_select()\n",
    "        else:\n",
    "            # 使用する特徴量の抽出\n",
    "            features = pd.read_csv(cfg.output_dir + f\"{cfg.base_exp}/fi_{group}.csv\").head(cfg.n_features)[\"feature\"].tolist()\n",
    "\n",
    "        gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "        fis = []\n",
    "        \n",
    "        oof_groups = []\n",
    "        for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "            model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\"\n",
    "            \n",
    "            print(f\"fold : {i}\")\n",
    "            tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "            vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "            tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "            if os.path.exists(model_path):\n",
    "                print(f\"modelが既に存在するのでロード : {model_path}\")\n",
    "                model = lgb.Booster(model_file=model_path)\n",
    "            else:\n",
    "                model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                                num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)\n",
    "            # モデル出力\n",
    "            model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\")\n",
    "        \n",
    "            # valid_pred\n",
    "            oof_fold = train.iloc[vl_idx].copy()\n",
    "            oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "            oof_groups.append(oof_fold)\n",
    "\n",
    "            # 特徴量重要度\n",
    "            fi_fold = pd.DataFrame()\n",
    "            fi_fold[\"feature\"] = model.feature_name()\n",
    "            fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "            fi_fold[\"fold\"] = i\n",
    "            fis.append(fi_fold)\n",
    "\n",
    "        fi = pd.concat(fis)    \n",
    "        fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "        fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "        fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi_{group}.csv\", index=False)\n",
    "\n",
    "        oof_group = pd.concat(oof_groups)\n",
    "        oofs.append(oof_group)\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\", \"level_group\"])\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = oof_group.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{group}_pred_mean\", \"max\":f\"{group}_pred_max\", \"min\":f\"{group}_pred_min\", \"std\":f\"{group}_pred_std\"})\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"level_group2\"] = test[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"level_group2\"] = sub[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in test.groupby(\"level_group2\")]\n",
    "    subs = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in sub.groupby(\"level_group2\")]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    model_dict = {}\n",
    "    features_dict = {}\n",
    "    for g in level_group_list:\n",
    "        if mode == \"local_cv\":\n",
    "            model_paths = [cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_paths = [f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        model_dict[g] = [lgb.Booster(model_file=p) for p in model_paths]\n",
    "        features_dict[g] = model_dict[g][0].feature_name()\n",
    "    need_create_features = features_dict[\"0-4\"] + features_dict[\"5-12\"] + features_dict[\"13-22\"]\n",
    "    not_drop_cols = [\"0-4_elapsed_time_max\", \"0-4_index_max\", \"5-12_elapsed_time_max\", \"5-12_index_max\", \"13-22_elapsed_time_max\", \"13-22_index_max\",\n",
    "                     \"0-4_elapsed_time_min\", \"0-4_index_min\", \"5-12_elapsed_time_min\", \"5-12_index_min\", \"13-22_elapsed_time_min\", \"13-22_index_min\"]\n",
    "    need_create_features = need_create_features + not_drop_cols\n",
    "    need_create_features = list(set(need_create_features))\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission, feature_select=True, need_create_features=need_create_features)\n",
    "        features = features_dict[level_group]\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "\n",
    "        prev_features_df = test.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\"])\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = model_dict[level_group][i]\n",
    "            preds += model.predict(test[features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        test[\"pred\"] = preds\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = test.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{level_group}_pred_mean\", \"max\":f\"{level_group}_pred_max\", \"min\":f\"{level_group}_pred_min\", \"std\":f\"{level_group}_pred_std\"})\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_train_test_process_identity():\n",
    "    iter_train = get_mock_iter_train()\n",
    "    iter_test = get_mock_iter_test()\n",
    "\n",
    "    print(\"train_iter\")\n",
    "    train_df_dict = {}\n",
    "    train_features_dict = {}\n",
    "    prev_features_df = None\n",
    "    for (sessions, sub) in iter_train:\n",
    "        group = sessions[\"level_group\"].values[0]\n",
    "        print(group)\n",
    "        train = get_train_dataset(sessions, sub)\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "            # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "        train_df_dict[group] = train[[\"session_id\"]+features].sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "        prev_features_df = train[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        train_features_dict[group] = features\n",
    "\n",
    "\n",
    "    print(\"test_iter\")\n",
    "    test_dfs_0_4 = []\n",
    "    test_dfs_5_12 = []\n",
    "    test_dfs_13_22 = []\n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        session_id = test_sessions[\"session_id\"].values[0]\n",
    "        print(session_id, level_group)\n",
    "        features = train_features_dict[level_group]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in test.columns if c not in not_use_cols]\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        if level_group == \"0-4\":\n",
    "            test_dfs_0_4.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"5-12\":\n",
    "            test_dfs_5_12.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"13-22\":\n",
    "            test_dfs_13_22.append(test[[\"session_id\"]+features])\n",
    "        \n",
    "\n",
    "    test_dfs_0_4 = pd.concat(test_dfs_0_4, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_5_12 = pd.concat(test_dfs_5_12, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_13_22 = pd.concat(test_dfs_13_22, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "\n",
    "    assert train_df_dict[\"0-4\"][train_features_dict[\"0-4\"]].equals(test_dfs_0_4[train_features_dict[\"0-4\"]])\n",
    "    assert train_df_dict[\"5-12\"][train_features_dict[\"5-12\"]].equals(test_dfs_5_12[train_features_dict[\"5-12\"]])\n",
    "    assert train_df_dict[\"13-22\"][train_features_dict[\"13-22\"]].equals(test_dfs_13_22[train_features_dict[\"13-22\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iter\n",
      "0-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 343.43it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 418.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 390.01it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 381.38it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 366.18it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 323.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 399.52it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 371.14it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 382.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_iter\n",
      "20090109393214576 0-4\n",
      "20090109393214576 5-12\n",
      "20090109393214576 13-22\n",
      "20090312143683264 0-4\n",
      "20090312143683264 5-12\n",
      "20090312143683264 13-22\n",
      "20090312331414616 0-4\n",
      "20090312331414616 5-12\n",
      "20090312331414616 13-22\n",
      "0-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23562/23562 [01:39<00:00, 236.75it/s]\n",
      "100%|██████████| 23562/23562 [01:40<00:00, 234.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量間の相関性が高い特徴量を1144個抽出\n",
      "2377 -> 1233\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49821, number of negative: 6726\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.366818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173643\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 1220\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.881055 -> initscore=2.002456\n",
      "[LightGBM] [Info] Start training from score 2.002456\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.282733\tvalid_1's binary_logloss: 0.304935\n",
      "[200]\ttraining's binary_logloss: 0.250821\tvalid_1's binary_logloss: 0.283867\n",
      "[300]\ttraining's binary_logloss: 0.231837\tvalid_1's binary_logloss: 0.275252\n",
      "[400]\ttraining's binary_logloss: 0.218128\tvalid_1's binary_logloss: 0.271528\n",
      "[500]\ttraining's binary_logloss: 0.206512\tvalid_1's binary_logloss: 0.269792\n",
      "[600]\ttraining's binary_logloss: 0.196523\tvalid_1's binary_logloss: 0.269113\n",
      "[700]\ttraining's binary_logloss: 0.187752\tvalid_1's binary_logloss: 0.268748\n",
      "[800]\ttraining's binary_logloss: 0.179854\tvalid_1's binary_logloss: 0.26868\n",
      "Early stopping, best iteration is:\n",
      "[772]\ttraining's binary_logloss: 0.182103\tvalid_1's binary_logloss: 0.268631\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49692, number of negative: 6855\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.293319 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 173648\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 1219\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.878773 -> initscore=1.980866\n",
      "[LightGBM] [Info] Start training from score 1.980866\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.28558\tvalid_1's binary_logloss: 0.292995\n",
      "[200]\ttraining's binary_logloss: 0.25368\tvalid_1's binary_logloss: 0.274386\n",
      "[300]\ttraining's binary_logloss: 0.234532\tvalid_1's binary_logloss: 0.266776\n",
      "[400]\ttraining's binary_logloss: 0.220514\tvalid_1's binary_logloss: 0.263284\n",
      "[500]\ttraining's binary_logloss: 0.208937\tvalid_1's binary_logloss: 0.2618\n",
      "[600]\ttraining's binary_logloss: 0.198744\tvalid_1's binary_logloss: 0.261014\n",
      "[700]\ttraining's binary_logloss: 0.189739\tvalid_1's binary_logloss: 0.260754\n",
      "[800]\ttraining's binary_logloss: 0.181551\tvalid_1's binary_logloss: 0.260463\n",
      "[900]\ttraining's binary_logloss: 0.174157\tvalid_1's binary_logloss: 0.260426\n",
      "Early stopping, best iteration is:\n",
      "[852]\ttraining's binary_logloss: 0.177603\tvalid_1's binary_logloss: 0.260338\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49772, number of negative: 6778\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.955561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173751\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 1221\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880141 -> initscore=1.993771\n",
      "[LightGBM] [Info] Start training from score 1.993771\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.284611\tvalid_1's binary_logloss: 0.297021\n",
      "[200]\ttraining's binary_logloss: 0.25386\tvalid_1's binary_logloss: 0.276682\n",
      "[300]\ttraining's binary_logloss: 0.234255\tvalid_1's binary_logloss: 0.267333\n",
      "[400]\ttraining's binary_logloss: 0.219895\tvalid_1's binary_logloss: 0.262844\n",
      "[500]\ttraining's binary_logloss: 0.208453\tvalid_1's binary_logloss: 0.261081\n",
      "[600]\ttraining's binary_logloss: 0.198494\tvalid_1's binary_logloss: 0.260193\n",
      "[700]\ttraining's binary_logloss: 0.189554\tvalid_1's binary_logloss: 0.259645\n",
      "[800]\ttraining's binary_logloss: 0.181529\tvalid_1's binary_logloss: 0.259469\n",
      "Early stopping, best iteration is:\n",
      "[753]\ttraining's binary_logloss: 0.185184\tvalid_1's binary_logloss: 0.259343\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49755, number of negative: 6795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.543889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 173793\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 1221\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.879841 -> initscore=1.990924\n",
      "[LightGBM] [Info] Start training from score 1.990924\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.283624\tvalid_1's binary_logloss: 0.299567\n",
      "[200]\ttraining's binary_logloss: 0.252407\tvalid_1's binary_logloss: 0.28121\n",
      "[300]\ttraining's binary_logloss: 0.232775\tvalid_1's binary_logloss: 0.272947\n",
      "[400]\ttraining's binary_logloss: 0.21834\tvalid_1's binary_logloss: 0.269314\n",
      "[500]\ttraining's binary_logloss: 0.206771\tvalid_1's binary_logloss: 0.268172\n",
      "[600]\ttraining's binary_logloss: 0.196843\tvalid_1's binary_logloss: 0.267664\n",
      "[700]\ttraining's binary_logloss: 0.187778\tvalid_1's binary_logloss: 0.267498\n",
      "[800]\ttraining's binary_logloss: 0.179768\tvalid_1's binary_logloss: 0.267557\n",
      "Early stopping, best iteration is:\n",
      "[719]\ttraining's binary_logloss: 0.186216\tvalid_1's binary_logloss: 0.26747\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49804, number of negative: 6746\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.473410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 173671\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 1221\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880707 -> initscore=1.999146\n",
      "[LightGBM] [Info] Start training from score 1.999146\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.283678\tvalid_1's binary_logloss: 0.300867\n",
      "[200]\ttraining's binary_logloss: 0.252987\tvalid_1's binary_logloss: 0.279658\n",
      "[300]\ttraining's binary_logloss: 0.233564\tvalid_1's binary_logloss: 0.269977\n",
      "[400]\ttraining's binary_logloss: 0.21927\tvalid_1's binary_logloss: 0.265655\n",
      "[500]\ttraining's binary_logloss: 0.207819\tvalid_1's binary_logloss: 0.263687\n",
      "[600]\ttraining's binary_logloss: 0.197919\tvalid_1's binary_logloss: 0.262748\n",
      "[700]\ttraining's binary_logloss: 0.188978\tvalid_1's binary_logloss: 0.262246\n",
      "[800]\ttraining's binary_logloss: 0.18101\tvalid_1's binary_logloss: 0.262087\n",
      "[900]\ttraining's binary_logloss: 0.173666\tvalid_1's binary_logloss: 0.261912\n",
      "[1000]\ttraining's binary_logloss: 0.167026\tvalid_1's binary_logloss: 0.262028\n",
      "Early stopping, best iteration is:\n",
      "[904]\ttraining's binary_logloss: 0.17337\tvalid_1's binary_logloss: 0.261898\n",
      "5-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23562/23562 [02:50<00:00, 137.99it/s]\n",
      "100%|██████████| 23562/23562 [02:50<00:00, 137.85it/s]\n",
      "100%|██████████| 23562/23562 [02:51<00:00, 137.57it/s]\n",
      "100%|██████████| 23562/23562 [02:51<00:00, 137.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量間の相関性が高い特徴量を2820個抽出\n",
      "6421 -> 3601\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.588405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 542872\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 3543\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.571809\tvalid_1's binary_logloss: 0.579655\n",
      "[200]\ttraining's binary_logloss: 0.540091\tvalid_1's binary_logloss: 0.553779\n",
      "[300]\ttraining's binary_logloss: 0.523447\tvalid_1's binary_logloss: 0.543099\n",
      "[400]\ttraining's binary_logloss: 0.512437\tvalid_1's binary_logloss: 0.537968\n",
      "[500]\ttraining's binary_logloss: 0.504016\tvalid_1's binary_logloss: 0.535355\n",
      "[600]\ttraining's binary_logloss: 0.497001\tvalid_1's binary_logloss: 0.533857\n",
      "[700]\ttraining's binary_logloss: 0.490852\tvalid_1's binary_logloss: 0.532978\n",
      "[800]\ttraining's binary_logloss: 0.485277\tvalid_1's binary_logloss: 0.532347\n",
      "[900]\ttraining's binary_logloss: 0.480215\tvalid_1's binary_logloss: 0.531964\n",
      "[1000]\ttraining's binary_logloss: 0.475482\tvalid_1's binary_logloss: 0.531628\n",
      "[1100]\ttraining's binary_logloss: 0.471152\tvalid_1's binary_logloss: 0.531429\n",
      "[1200]\ttraining's binary_logloss: 0.467114\tvalid_1's binary_logloss: 0.53118\n",
      "[1300]\ttraining's binary_logloss: 0.46337\tvalid_1's binary_logloss: 0.531016\n",
      "[1400]\ttraining's binary_logloss: 0.459847\tvalid_1's binary_logloss: 0.530843\n",
      "[1500]\ttraining's binary_logloss: 0.45666\tvalid_1's binary_logloss: 0.530762\n",
      "[1600]\ttraining's binary_logloss: 0.453565\tvalid_1's binary_logloss: 0.530724\n",
      "[1700]\ttraining's binary_logloss: 0.450557\tvalid_1's binary_logloss: 0.530628\n",
      "[1800]\ttraining's binary_logloss: 0.447599\tvalid_1's binary_logloss: 0.530573\n",
      "[1900]\ttraining's binary_logloss: 0.444866\tvalid_1's binary_logloss: 0.530558\n",
      "Early stopping, best iteration is:\n",
      "[1836]\ttraining's binary_logloss: 0.4466\tvalid_1's binary_logloss: 0.530543\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122423, number of negative: 66067\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 5.153818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 542771\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 3543\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649493 -> initscore=0.616813\n",
      "[LightGBM] [Info] Start training from score 0.616813\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.571769\tvalid_1's binary_logloss: 0.57892\n",
      "[200]\ttraining's binary_logloss: 0.539814\tvalid_1's binary_logloss: 0.553868\n",
      "[300]\ttraining's binary_logloss: 0.523092\tvalid_1's binary_logloss: 0.543603\n",
      "[400]\ttraining's binary_logloss: 0.512022\tvalid_1's binary_logloss: 0.538926\n",
      "[500]\ttraining's binary_logloss: 0.503656\tvalid_1's binary_logloss: 0.536523\n",
      "[600]\ttraining's binary_logloss: 0.496627\tvalid_1's binary_logloss: 0.535253\n",
      "[700]\ttraining's binary_logloss: 0.490431\tvalid_1's binary_logloss: 0.534502\n",
      "[800]\ttraining's binary_logloss: 0.484835\tvalid_1's binary_logloss: 0.534093\n",
      "[900]\ttraining's binary_logloss: 0.479752\tvalid_1's binary_logloss: 0.53377\n",
      "[1000]\ttraining's binary_logloss: 0.475035\tvalid_1's binary_logloss: 0.533548\n",
      "[1100]\ttraining's binary_logloss: 0.470714\tvalid_1's binary_logloss: 0.533341\n",
      "[1200]\ttraining's binary_logloss: 0.466694\tvalid_1's binary_logloss: 0.533115\n",
      "[1300]\ttraining's binary_logloss: 0.462901\tvalid_1's binary_logloss: 0.533097\n",
      "[1400]\ttraining's binary_logloss: 0.459339\tvalid_1's binary_logloss: 0.532995\n",
      "[1500]\ttraining's binary_logloss: 0.456153\tvalid_1's binary_logloss: 0.532974\n",
      "Early stopping, best iteration is:\n",
      "[1474]\ttraining's binary_logloss: 0.456952\tvalid_1's binary_logloss: 0.532947\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122249, number of negative: 66251\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.532039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 543022\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 3545\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.648536 -> initscore=0.612609\n",
      "[LightGBM] [Info] Start training from score 0.612609\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.570615\tvalid_1's binary_logloss: 0.57485\n",
      "[200]\ttraining's binary_logloss: 0.539195\tvalid_1's binary_logloss: 0.550525\n",
      "[300]\ttraining's binary_logloss: 0.523322\tvalid_1's binary_logloss: 0.541459\n",
      "[400]\ttraining's binary_logloss: 0.512671\tvalid_1's binary_logloss: 0.53709\n",
      "[500]\ttraining's binary_logloss: 0.504173\tvalid_1's binary_logloss: 0.534548\n",
      "[600]\ttraining's binary_logloss: 0.497143\tvalid_1's binary_logloss: 0.533236\n",
      "[700]\ttraining's binary_logloss: 0.49102\tvalid_1's binary_logloss: 0.532451\n",
      "[800]\ttraining's binary_logloss: 0.485501\tvalid_1's binary_logloss: 0.531989\n",
      "[900]\ttraining's binary_logloss: 0.480432\tvalid_1's binary_logloss: 0.531619\n",
      "[1000]\ttraining's binary_logloss: 0.475745\tvalid_1's binary_logloss: 0.531339\n",
      "[1100]\ttraining's binary_logloss: 0.471438\tvalid_1's binary_logloss: 0.531065\n",
      "[1200]\ttraining's binary_logloss: 0.467504\tvalid_1's binary_logloss: 0.530985\n",
      "[1300]\ttraining's binary_logloss: 0.463804\tvalid_1's binary_logloss: 0.53089\n",
      "[1400]\ttraining's binary_logloss: 0.46033\tvalid_1's binary_logloss: 0.530816\n",
      "[1500]\ttraining's binary_logloss: 0.457058\tvalid_1's binary_logloss: 0.530704\n",
      "[1600]\ttraining's binary_logloss: 0.453981\tvalid_1's binary_logloss: 0.530623\n",
      "[1700]\ttraining's binary_logloss: 0.45109\tvalid_1's binary_logloss: 0.530609\n",
      "[1800]\ttraining's binary_logloss: 0.448328\tvalid_1's binary_logloss: 0.53052\n",
      "[1900]\ttraining's binary_logloss: 0.445602\tvalid_1's binary_logloss: 0.530528\n",
      "Early stopping, best iteration is:\n",
      "[1868]\ttraining's binary_logloss: 0.446485\tvalid_1's binary_logloss: 0.530492\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122566, number of negative: 65934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.426793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 543114\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 3544\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650218 -> initscore=0.619995\n",
      "[LightGBM] [Info] Start training from score 0.619995\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.568973\tvalid_1's binary_logloss: 0.576197\n",
      "[200]\ttraining's binary_logloss: 0.54031\tvalid_1's binary_logloss: 0.5536\n",
      "[300]\ttraining's binary_logloss: 0.523394\tvalid_1's binary_logloss: 0.542368\n",
      "[400]\ttraining's binary_logloss: 0.512686\tvalid_1's binary_logloss: 0.537402\n",
      "[500]\ttraining's binary_logloss: 0.504357\tvalid_1's binary_logloss: 0.534661\n",
      "[600]\ttraining's binary_logloss: 0.497358\tvalid_1's binary_logloss: 0.533037\n",
      "[700]\ttraining's binary_logloss: 0.491207\tvalid_1's binary_logloss: 0.532098\n",
      "[800]\ttraining's binary_logloss: 0.485637\tvalid_1's binary_logloss: 0.531434\n",
      "[900]\ttraining's binary_logloss: 0.480594\tvalid_1's binary_logloss: 0.531037\n",
      "[1000]\ttraining's binary_logloss: 0.475981\tvalid_1's binary_logloss: 0.530854\n",
      "[1100]\ttraining's binary_logloss: 0.471706\tvalid_1's binary_logloss: 0.530669\n",
      "[1200]\ttraining's binary_logloss: 0.46776\tvalid_1's binary_logloss: 0.530536\n",
      "[1300]\ttraining's binary_logloss: 0.464034\tvalid_1's binary_logloss: 0.530403\n",
      "[1400]\ttraining's binary_logloss: 0.460537\tvalid_1's binary_logloss: 0.530293\n",
      "[1500]\ttraining's binary_logloss: 0.457234\tvalid_1's binary_logloss: 0.530151\n",
      "[1600]\ttraining's binary_logloss: 0.454077\tvalid_1's binary_logloss: 0.530116\n",
      "[1700]\ttraining's binary_logloss: 0.45105\tvalid_1's binary_logloss: 0.529976\n",
      "Early stopping, best iteration is:\n",
      "[1694]\ttraining's binary_logloss: 0.451211\tvalid_1's binary_logloss: 0.529955\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122635, number of negative: 65865\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 6.093398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 542688\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 3545\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650584 -> initscore=0.621605\n",
      "[LightGBM] [Info] Start training from score 0.621605\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.56973\tvalid_1's binary_logloss: 0.578802\n",
      "[200]\ttraining's binary_logloss: 0.538612\tvalid_1's binary_logloss: 0.553772\n",
      "[300]\ttraining's binary_logloss: 0.522797\tvalid_1's binary_logloss: 0.544119\n",
      "[400]\ttraining's binary_logloss: 0.512182\tvalid_1's binary_logloss: 0.53937\n",
      "[500]\ttraining's binary_logloss: 0.503717\tvalid_1's binary_logloss: 0.536481\n",
      "[600]\ttraining's binary_logloss: 0.496663\tvalid_1's binary_logloss: 0.534882\n",
      "[700]\ttraining's binary_logloss: 0.490516\tvalid_1's binary_logloss: 0.533958\n",
      "[800]\ttraining's binary_logloss: 0.484983\tvalid_1's binary_logloss: 0.533389\n",
      "[900]\ttraining's binary_logloss: 0.479931\tvalid_1's binary_logloss: 0.533005\n",
      "[1000]\ttraining's binary_logloss: 0.475295\tvalid_1's binary_logloss: 0.532743\n",
      "[1100]\ttraining's binary_logloss: 0.470985\tvalid_1's binary_logloss: 0.532562\n",
      "[1200]\ttraining's binary_logloss: 0.467114\tvalid_1's binary_logloss: 0.532405\n",
      "[1300]\ttraining's binary_logloss: 0.463355\tvalid_1's binary_logloss: 0.532317\n",
      "[1400]\ttraining's binary_logloss: 0.459928\tvalid_1's binary_logloss: 0.53229\n",
      "[1500]\ttraining's binary_logloss: 0.456562\tvalid_1's binary_logloss: 0.532116\n",
      "[1600]\ttraining's binary_logloss: 0.453463\tvalid_1's binary_logloss: 0.532086\n",
      "[1700]\ttraining's binary_logloss: 0.450547\tvalid_1's binary_logloss: 0.532029\n",
      "[1800]\ttraining's binary_logloss: 0.447754\tvalid_1's binary_logloss: 0.531989\n",
      "[1900]\ttraining's binary_logloss: 0.445111\tvalid_1's binary_logloss: 0.531937\n",
      "[2000]\ttraining's binary_logloss: 0.442614\tvalid_1's binary_logloss: 0.53197\n",
      "[2100]\ttraining's binary_logloss: 0.440065\tvalid_1's binary_logloss: 0.531911\n",
      "[2200]\ttraining's binary_logloss: 0.437682\tvalid_1's binary_logloss: 0.531869\n",
      "Early stopping, best iteration is:\n",
      "[2163]\ttraining's binary_logloss: 0.438497\tvalid_1's binary_logloss: 0.531843\n",
      "13-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23562/23562 [04:33<00:00, 86.07it/s]\n",
      "100%|██████████| 23562/23562 [04:34<00:00, 85.78it/s]\n",
      "100%|██████████| 23562/23562 [04:32<00:00, 86.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量間の相関性が高い特徴量を4924個抽出\n",
      "11887 -> 6963\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67313, number of negative: 26932\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.447970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1048303\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 6847\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.714234 -> initscore=0.916038\n",
      "[LightGBM] [Info] Start training from score 0.916038\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.52583\tvalid_1's binary_logloss: 0.543301\n",
      "[200]\ttraining's binary_logloss: 0.494558\tvalid_1's binary_logloss: 0.521751\n",
      "[300]\ttraining's binary_logloss: 0.476329\tvalid_1's binary_logloss: 0.513233\n",
      "[400]\ttraining's binary_logloss: 0.463032\tvalid_1's binary_logloss: 0.509386\n",
      "[500]\ttraining's binary_logloss: 0.451605\tvalid_1's binary_logloss: 0.506894\n",
      "[600]\ttraining's binary_logloss: 0.441938\tvalid_1's binary_logloss: 0.505597\n",
      "[700]\ttraining's binary_logloss: 0.433103\tvalid_1's binary_logloss: 0.504838\n",
      "[800]\ttraining's binary_logloss: 0.424739\tvalid_1's binary_logloss: 0.504353\n",
      "[900]\ttraining's binary_logloss: 0.417137\tvalid_1's binary_logloss: 0.503947\n",
      "[1000]\ttraining's binary_logloss: 0.410438\tvalid_1's binary_logloss: 0.503747\n",
      "[1100]\ttraining's binary_logloss: 0.404532\tvalid_1's binary_logloss: 0.503724\n",
      "[1200]\ttraining's binary_logloss: 0.398907\tvalid_1's binary_logloss: 0.503661\n",
      "Early stopping, best iteration is:\n",
      "[1163]\ttraining's binary_logloss: 0.400831\tvalid_1's binary_logloss: 0.503562\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 66951, number of negative: 27294\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.463860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1048308\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 6846\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710393 -> initscore=0.897294\n",
      "[LightGBM] [Info] Start training from score 0.897294\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.53102\tvalid_1's binary_logloss: 0.534344\n",
      "[200]\ttraining's binary_logloss: 0.500782\tvalid_1's binary_logloss: 0.514807\n",
      "[300]\ttraining's binary_logloss: 0.480849\tvalid_1's binary_logloss: 0.505042\n",
      "[400]\ttraining's binary_logloss: 0.466279\tvalid_1's binary_logloss: 0.50012\n",
      "[500]\ttraining's binary_logloss: 0.454693\tvalid_1's binary_logloss: 0.497652\n",
      "[600]\ttraining's binary_logloss: 0.444551\tvalid_1's binary_logloss: 0.496217\n",
      "[700]\ttraining's binary_logloss: 0.435447\tvalid_1's binary_logloss: 0.495349\n",
      "[800]\ttraining's binary_logloss: 0.42748\tvalid_1's binary_logloss: 0.494857\n",
      "[900]\ttraining's binary_logloss: 0.42021\tvalid_1's binary_logloss: 0.494527\n",
      "[1000]\ttraining's binary_logloss: 0.413646\tvalid_1's binary_logloss: 0.494368\n",
      "[1100]\ttraining's binary_logloss: 0.407418\tvalid_1's binary_logloss: 0.494192\n",
      "[1200]\ttraining's binary_logloss: 0.401851\tvalid_1's binary_logloss: 0.494048\n",
      "[1300]\ttraining's binary_logloss: 0.396523\tvalid_1's binary_logloss: 0.494024\n",
      "[1400]\ttraining's binary_logloss: 0.391337\tvalid_1's binary_logloss: 0.494052\n",
      "Early stopping, best iteration is:\n",
      "[1328]\ttraining's binary_logloss: 0.394984\tvalid_1's binary_logloss: 0.493982\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 66989, number of negative: 27261\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.025964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1048970\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 6847\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710759 -> initscore=0.899071\n",
      "[LightGBM] [Info] Start training from score 0.899071\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.528698\tvalid_1's binary_logloss: 0.533717\n",
      "[200]\ttraining's binary_logloss: 0.497056\tvalid_1's binary_logloss: 0.512781\n",
      "[300]\ttraining's binary_logloss: 0.478642\tvalid_1's binary_logloss: 0.504614\n",
      "[400]\ttraining's binary_logloss: 0.465486\tvalid_1's binary_logloss: 0.50099\n",
      "[500]\ttraining's binary_logloss: 0.454048\tvalid_1's binary_logloss: 0.49877\n",
      "[600]\ttraining's binary_logloss: 0.444273\tvalid_1's binary_logloss: 0.497599\n",
      "[700]\ttraining's binary_logloss: 0.435299\tvalid_1's binary_logloss: 0.496751\n",
      "[800]\ttraining's binary_logloss: 0.42706\tvalid_1's binary_logloss: 0.49629\n",
      "[900]\ttraining's binary_logloss: 0.419487\tvalid_1's binary_logloss: 0.495899\n",
      "[1000]\ttraining's binary_logloss: 0.412935\tvalid_1's binary_logloss: 0.495763\n",
      "[1100]\ttraining's binary_logloss: 0.406804\tvalid_1's binary_logloss: 0.495543\n",
      "[1200]\ttraining's binary_logloss: 0.401027\tvalid_1's binary_logloss: 0.495543\n",
      "[1300]\ttraining's binary_logloss: 0.395776\tvalid_1's binary_logloss: 0.495517\n",
      "Early stopping, best iteration is:\n",
      "[1222]\ttraining's binary_logloss: 0.399798\tvalid_1's binary_logloss: 0.495479\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67214, number of negative: 27036\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.556267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1048110\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 6847\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.713146 -> initscore=0.910712\n",
      "[LightGBM] [Info] Start training from score 0.910712\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.526606\tvalid_1's binary_logloss: 0.541177\n",
      "[200]\ttraining's binary_logloss: 0.494935\tvalid_1's binary_logloss: 0.52014\n",
      "[300]\ttraining's binary_logloss: 0.476429\tvalid_1's binary_logloss: 0.511752\n",
      "[400]\ttraining's binary_logloss: 0.463342\tvalid_1's binary_logloss: 0.508284\n",
      "[500]\ttraining's binary_logloss: 0.45196\tvalid_1's binary_logloss: 0.50602\n",
      "[600]\ttraining's binary_logloss: 0.442187\tvalid_1's binary_logloss: 0.50485\n",
      "[700]\ttraining's binary_logloss: 0.433329\tvalid_1's binary_logloss: 0.504094\n",
      "[800]\ttraining's binary_logloss: 0.425067\tvalid_1's binary_logloss: 0.503702\n",
      "[900]\ttraining's binary_logloss: 0.41758\tvalid_1's binary_logloss: 0.503387\n",
      "[1000]\ttraining's binary_logloss: 0.411066\tvalid_1's binary_logloss: 0.503355\n",
      "[1100]\ttraining's binary_logloss: 0.405107\tvalid_1's binary_logloss: 0.503315\n",
      "Early stopping, best iteration is:\n",
      "[1051]\ttraining's binary_logloss: 0.408005\tvalid_1's binary_logloss: 0.503263\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67245, number of negative: 27005\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.930454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1048256\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 6847\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.713475 -> initscore=0.912321\n",
      "[LightGBM] [Info] Start training from score 0.912321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.526353\tvalid_1's binary_logloss: 0.542645\n",
      "[200]\ttraining's binary_logloss: 0.494854\tvalid_1's binary_logloss: 0.521433\n",
      "[300]\ttraining's binary_logloss: 0.476459\tvalid_1's binary_logloss: 0.512866\n",
      "[400]\ttraining's binary_logloss: 0.463369\tvalid_1's binary_logloss: 0.50928\n",
      "[500]\ttraining's binary_logloss: 0.451913\tvalid_1's binary_logloss: 0.506885\n",
      "[600]\ttraining's binary_logloss: 0.442159\tvalid_1's binary_logloss: 0.505608\n",
      "[700]\ttraining's binary_logloss: 0.43335\tvalid_1's binary_logloss: 0.504621\n",
      "[800]\ttraining's binary_logloss: 0.424988\tvalid_1's binary_logloss: 0.503984\n",
      "[900]\ttraining's binary_logloss: 0.417553\tvalid_1's binary_logloss: 0.503646\n",
      "[1000]\ttraining's binary_logloss: 0.410982\tvalid_1's binary_logloss: 0.503496\n",
      "[1100]\ttraining's binary_logloss: 0.405072\tvalid_1's binary_logloss: 0.503449\n",
      "[1200]\ttraining's binary_logloss: 0.399288\tvalid_1's binary_logloss: 0.503283\n",
      "[1300]\ttraining's binary_logloss: 0.393921\tvalid_1's binary_logloss: 0.503217\n",
      "[1400]\ttraining's binary_logloss: 0.388995\tvalid_1's binary_logloss: 0.503241\n",
      "Early stopping, best iteration is:\n",
      "[1334]\ttraining's binary_logloss: 0.392048\tvalid_1's binary_logloss: 0.503146\n",
      "logloss 0.477867\n",
      "best_score 0.698981\n",
      "best_threshold 0.630\n",
      "------------------------------\n",
      "Q1 : F1 = 0.631316\n",
      "Q2 : F1 = 0.550739\n",
      "Q3 : F1 = 0.590891\n",
      "Q4 : F1 = 0.663176\n",
      "Q5 : F1 = 0.404565\n",
      "Q6 : F1 = 0.625301\n",
      "Q7 : F1 = 0.583919\n",
      "Q8 : F1 = 0.353449\n",
      "Q9 : F1 = 0.585032\n",
      "Q10 : F1 = 0.358879\n",
      "Q11 : F1 = 0.409282\n",
      "Q12 : F1 = 0.592043\n",
      "Q13 : F1 = 0.423359\n",
      "Q14 : F1 = 0.518081\n",
      "Q15 : F1 = 0.373196\n",
      "Q16 : F1 = 0.428286\n",
      "Q17 : F1 = 0.378668\n",
      "Q18 : F1 = 0.555275\n",
      "[1 1 1]\n",
      "[1 0 1 1 0 1 0 1 1 0]\n",
      "[1 0 1 1 1]\n",
      "[0 1 1]\n",
      "[0 0 0 0 0 0 0 0 1 0]\n",
      "[0 0 0 0 1]\n",
      "[1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 0]\n",
      "[1 1 1 1 1]\n",
      "sample_inf処理時間 :  12.7 秒\n"
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    valid_train_test_process_identity()\n",
    "    run_train()\n",
    "inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
