{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp056"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text„ÅÆ„É¢„Éº„Éâ„Åî„Å®„ÅÆÂ∑ÆÁï∞„ÇíÂêçÂØÑ„Åõ  \n",
    "|  Âèñ„ÇäÁµÑ„Åø  |  cv  |  logloss |„ÄÄÊé°Âê¶ |\n",
    "| ---- | ---- | ---- | ----- |  \n",
    "| exp052 | 0.694623 | 0.483687 | - |  \n",
    "| textÂêçÂØÑ„Åõ | 0.694668 | 0.483620 | - |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp056\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cv„ÅÆÁµêÊûú„ÇíÂÖ•„Çå„Çã\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.1, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used_total = [\n",
    "    'record_cnt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col_lists = {'event_name': ['cutscene_click',\n",
    "  'person_click',\n",
    "  'navigate_click',\n",
    "  'observation_click',\n",
    "  'notification_click',\n",
    "  'object_click',\n",
    "  'object_hover',\n",
    "  'map_hover',\n",
    "  'map_click',\n",
    "  'checkpoint',\n",
    "  'notebook_click'],\n",
    " 'name': ['basic', 'undefined', 'close', 'open', 'prev', 'next'],\n",
    " 'level': [0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  5,\n",
    "  6,\n",
    "  7,\n",
    "  8,\n",
    "  9,\n",
    "  10,\n",
    "  11,\n",
    "  12,\n",
    "  13,\n",
    "  14,\n",
    "  15,\n",
    "  16,\n",
    "  17,\n",
    "  18,\n",
    "  19,\n",
    "  20,\n",
    "  21,\n",
    "  22],\n",
    " 'page': [0.0, 1.0, 3.0, 4.0, 5.0, 6.0, 2.0],\n",
    " 'text': ['undefined',\n",
    " 'Whatcha doing over there, Jo?',\n",
    " 'Just talking to Teddy.',\n",
    " 'I gotta run to my meeting!',\n",
    " 'Meetings are BORING!',\n",
    " 'Grab your notebook and come upstairs!',\n",
    " 'Hang tight, Teddy.',\n",
    " \"I'll hurry back and then we can go exploring!\",\n",
    " 'Now where did I put my notebook?',\n",
    " '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00b4',\n",
    " 'I love these photos of me and Teddy!',\n",
    " 'Found it!',\n",
    " 'Gramps is in trouble for losing papers?',\n",
    " \"This can't be right!\",\n",
    " 'Gramps is a great historian!',\n",
    " \"Hmm. Button's still not working.\",\n",
    " \"Let's get started. The Wisconsin Wonders exhibit opens tomorrow!\",\n",
    " 'Who wants to investigate the shirt artifact?',\n",
    " \"Not Leopold here. He's been losing papers lately.\",\n",
    " 'Hey!',\n",
    " \"It's true, they do keep going missing lately.\",\n",
    " 'See?',\n",
    " 'Besides, I already figured out the shirt.',\n",
    " \"It's a women's basketball jersey!\",\n",
    " 'That settles it.',\n",
    " 'Wells, finish up your report.',\n",
    " \"Leopold, why don't you help me set up in the Capitol?\",\n",
    " 'We need to talk about that missing paperwork.',\n",
    " 'Will do, Boss.',\n",
    " \"Hey Jo, let's take a look at the shirt!\",\n",
    " 'Your grampa is waiting for you in the collection room.',\n",
    " \"Why don't you go play with your grampa?\",\n",
    " 'What a fascinating artifact!',\n",
    " \"Wow, that's so cool, Gramps!\",\n",
    " 'Can I take a closer look?',\n",
    " \"Hmmm. Shouldn't you be doing your homework?\",\n",
    " \"It's already all done!\",\n",
    " 'Plus, my teacher said I could help you out for extra credit!',\n",
    " \"Well, that's good enough for me.\",\n",
    " 'Go ahead, take a peek at the shirt!',\n",
    " 'This looks like a clue!',\n",
    " \"I'll record this in my notebook.\",\n",
    " 'Find anything?',\n",
    " 'Just this old slip from 1916.',\n",
    " 'Hot Dog! I knew it!',\n",
    " \"I'm not so sure that this is a basketball jersey.\",\n",
    " 'Wait, you mean Wells is wrong?!',\n",
    " 'Could be. But we need evidence!',\n",
    " \"Why don't you head to the Basketball Center and rustle up some clues?\",\n",
    " 'Ooh, I like clues!',\n",
    " \"I'll be at the Capitol. Let me know if you find anything!\",\n",
    " 'Better check back later.',\n",
    " \"That's it!\",\n",
    " \"The slip is from 1916 but the team didn't start until 1974!\",\n",
    " 'Our shirt is too old to be a basketball jersey!',\n",
    " 'I need to get to the Capitol and tell Gramps!',\n",
    " 'Well? What are you still doing here?',\n",
    " 'Go find your grampa and get to work!',\n",
    " 'Oh no!',\n",
    " 'What happened here?!',\n",
    " \"I don't know!\",\n",
    " 'I got here and the whole place was a mess!',\n",
    " 'Can you help me tidy up?',\n",
    " \"Teddy's scarf! Somebody must've taken him!\",\n",
    " 'Try not to panic, Jo.',\n",
    " 'Maybe he just got scared and ran off.',\n",
    " 'But he never goes anywhere without his scarf!',\n",
    " \"I think he's in trouble!\",\n",
    " 'Is this your coffee, Gramps?',\n",
    " \"Nope, that's from Bean Town. I only drink Holdgers!\",\n",
    " \"Who could've done this?\",\n",
    " 'I have an idea.',\n",
    " 'He\\'s wrong about old shirts and his name rhymes with \\\\\"smells\\\\\"...',\n",
    " 'Hold your horses, Jo.',\n",
    " 'BUT WELLS STOLE TEDDY!',\n",
    " 'Could be. But we need evidence.',\n",
    " \"Fine. Let's investigate!\",\n",
    " \"I'm afraid my papers have gone missing in this mess.\",\n",
    " \"You'll have to get started without me.\",\n",
    " \"Don't worry, Gramps. I'll find Teddy!\",\n",
    " \"And I'll figure out the shirt, too.\",\n",
    " 'I knew I could count on you, Jo!',\n",
    " \"Why don't you go upstairs and see the archivist?\",\n",
    " \"He's our expert record keeper.\",\n",
    " 'I need your help!',\n",
    " 'And you are?',\n",
    " \"I'm Leopold's grandkid!\",\n",
    " \"I don't have time for kids.\",\n",
    " 'Now if only I could read this thing. Blasted tiny letters...',\n",
    " \"Can't believe I lost my reading glasses.\",\n",
    " 'I bet the archivist could use this!',\n",
    " \"Ah, that's better!\",\n",
    " 'Did you have a question?',\n",
    " 'Yes! I was wondering-',\n",
    " 'Wait a minute!',\n",
    " 'Where did you get that coffee?',\n",
    " \"Oh, that's from Bean Town.\",\n",
    " 'I ran into Wells there this morning.',\n",
    " 'Wells? I knew it!',\n",
    " 'Do you know anything about this slip?',\n",
    " 'I found it on an old shirt.',\n",
    " 'An old shirt? Try the university.',\n",
    " 'You can talk to a textile expert there.',\n",
    " \"What's a textile expert?\",\n",
    " 'They study clothes and fabric.',\n",
    " 'Great! Thanks for the help!',\n",
    " 'Run along to the university.',\n",
    " 'Hello there!',\n",
    " 'Wow! What is all this stuff?',\n",
    " \"It's our Norwegian Craft exhibit!\",\n",
    " 'Can I give you the tour?',\n",
    " \"Sorry, I'm in a hurry.\",\n",
    " 'Do you know what this slip is?',\n",
    " 'Looks like a dry cleaning receipt.',\n",
    " 'Ooh, thanks!',\n",
    " 'Yikes... this could take a while.',\n",
    " 'Maybe I can help!',\n",
    " \"I've got a stack of business cards from my favorite cleaners.\",\n",
    " \"Why don't you take a look?\",\n",
    " 'This place was around in 1916! I can start there!',\n",
    " \"You haven't seen any badgers around here, have you?\",\n",
    " 'Badgers? No.',\n",
    " 'Ugh. Fine.',\n",
    " 'Huh?',\n",
    " 'Can you help me? I need to find the owner of this slip.',\n",
    " \"Well, I can't show our log books to just anybody.\",\n",
    " 'Please?',\n",
    " \"It's for Grampa Leo. He's a historian!\",\n",
    " 'Leo... you mean Leopold?',\n",
    " 'Your gramps is awesome! Always full of stories.',\n",
    " \"Guess it couldn't hurt to let you take a look.\",\n",
    " \"Here's the log book.\",\n",
    " \"It's a match!\",\n",
    " 'Theodora Youmans must be the owner!',\n",
    " 'Do you know who Theodora Youmans is?',\n",
    " \"Hmmm... not sure. Why don't you try the library?\",\n",
    " 'Thanks for the help!',\n",
    " 'Oh, hello there!',\n",
    " \"You look like you're on a mission.\",\n",
    " 'Have you seen a badger around here?',\n",
    " \"I'm afraid not.\",\n",
    " \"Please let me know if you do. It's important!\",\n",
    " \"I'm also looking for Theodora Youmans. Have you heard of her?\",\n",
    " 'Theodora Youmans? Of course!',\n",
    " \"Check out our microfiche. It's right through that door.\",\n",
    " 'Youmans was a suffragist!',\n",
    " 'She helped get votes for women!',\n",
    " 'Wells! What was he doing here? I should ask the librarian.',\n",
    " 'What was Wells doing here?',\n",
    " 'He was looking for a taxidermist.',\n",
    " \"What's a taxidermist?\",\n",
    " 'Not sure. Here, let me look it up.',\n",
    " '\\\\Taxidermy: the art of preparing, stuffing, and mounting the skins of animals.\\\\',\n",
    " 'Oh no... Teddy!',\n",
    " 'I need to find Wells right away!! Do you know where he is?',\n",
    " 'You could ask the archivist. He knows everybody!',\n",
    " \"Jolie! I was hoping you'd stop by. Any news on the shirt artifact?\",\n",
    " \"I haven't quite figured it out just yet...\",\n",
    " \"Well, get on it. I'm counting on you and your gramps to figure this out!\",\n",
    " 'I need to find Wells!!!',\n",
    " \"Calm down, kid. I haven't seen him.\",\n",
    " \"I can't calm down. This is important!\",\n",
    " \"Sorry, can't help you.\",\n",
    " 'Do you have any info on Theodora Youmans?',\n",
    " 'Theodora Youmans? Is that who owned the shirt?',\n",
    " 'Yep.',\n",
    " \"Why didn't you say so?\",\n",
    " 'Youmans was a suffragist here in Wisconsin.',\n",
    " 'She led marches and helped women get the right to vote!',\n",
    " \"Wait a sec. Women couldn't vote?!\",\n",
    " 'Nope. But Youmans and other suffragists worked hard to change that.',\n",
    " 'Thanks to them, Wisconsin was the first state to approve votes for women!',\n",
    " 'Wow!',\n",
    " \"Here's a call number to find more info in the Stacks.\",\n",
    " 'Where are the Stacks?',\n",
    " 'Right outside the door.',\n",
    " 'Hey, this is Youmans!',\n",
    " \"And look! She's wearing the shirt!\",\n",
    " 'I should go to the Capitol and tell everyone!',\n",
    " 'Jo!',\n",
    " 'Check out the next artifact!',\n",
    " 'What is it?',\n",
    " \"I think it's a flag! Pretty interesting, huh?\",\n",
    " \"It's really cool, Gramps. But I'm worried about Teddy.\",\n",
    " \"He's still missing!\",\n",
    " \"We'll find him, Jo.\",\n",
    " 'Want to look for more clues?',\n",
    " \"We'll find Teddy.\",\n",
    " 'We just have to keep our eyes open!',\n",
    " 'Hey, look at those scratches!',\n",
    " 'The kidnapper probably took Teddy on the elevator!',\n",
    " \"You're right, Jo!\",\n",
    " \"Why isn't the button working?\",\n",
    " \"We'll need a key card.\",\n",
    " 'I had one, but Teddy chewed it up.',\n",
    " \"I've got Wells's ID!\",\n",
    " 'What should we do next?',\n",
    " \"I need to take the artifact upstairs. Why don't you investigate those scratch marks?\",\n",
    " \"Okay. I'll try.\",\n",
    " 'Teddy, here I come!',\n",
    " 'I wonder whose glasses these are.',\n",
    " 'Teddy!!!',\n",
    " \"Hang on. I'll get you out of there!\",\n",
    " 'Whoever lost these glasses probably took Teddy!',\n",
    " 'How can I find out whose glasses these are?',\n",
    " 'Oh! There was a staff directory in the entryway!',\n",
    " \"I'll go look at everyone's pictures!\",\n",
    " 'Those are the same glasses!',\n",
    " \"The archivist must've taken Teddy!\",\n",
    " \"Yes! It's the key for Teddy's cage!\",\n",
    " 'I found the key!',\n",
    " \"Come on, let's get out of here!\",\n",
    " \"Here's your scarf back!\",\n",
    " 'What are you doing down here?',\n",
    " 'And how did that badger get free?',\n",
    " \"I'm here to rescue my friend!\",\n",
    " \"What's going on here?\",\n",
    " 'Thanks for coming, Boss.',\n",
    " 'I told you!',\n",
    " 'I captured a badger in our museum!',\n",
    " \"He's been eating my lunch every day this week!\",\n",
    " 'He has??',\n",
    " \"I've seen him eating homework and important papers, too.\",\n",
    " \"Jolie- keep your badger under control, or he'll have to go.\",\n",
    " 'And you, Frank-',\n",
    " \"You can't just steal Jolie's pet.\",\n",
    " 'Alright, Jolie. Back to work.',\n",
    " \"Come on, Teddy. Let's go help Gramps!\",\n",
    " \"Let's go help Gramps!\",\n",
    " 'Gramps must be up in the collection room.',\n",
    " \"Let's go find him!\",\n",
    " \"Teddy! I'm sure glad to see you.\",\n",
    " 'The archivist had him locked up!',\n",
    " 'Gadzooks! Poor critter.',\n",
    " \"You're becoming quite the detective, Jo.\",\n",
    " 'Notice any clues about this flag?',\n",
    " 'Well... it looks hand-stitched.',\n",
    " 'Aha! Good catch, Jo.',\n",
    " 'Go on, tell the boss what you found!',\n",
    " \"I'm telling you, Boss. Taxidermy is the way to go!\",\n",
    " 'Nonsense. I want live animals at the exhibit, not stuffed ones.',\n",
    " \"Ah, Jolie! I'm glad you're here.\",\n",
    " \"I'm putting you in charge of the flag case.\",\n",
    " 'Make sure to get some old photos for the exhibit, like last time!',\n",
    " \"Wait! Can't I do it?\",\n",
    " 'The symbol on the flag looks sort of like a deer hoof.',\n",
    " 'It could be an early design for the Wisconsin state flag!',\n",
    " 'Wells, you already have a job to do.',\n",
    " 'What now, kid?',\n",
    " 'Do you really think that symbol is a deer hoof?',\n",
    " 'Not sure. Do I look like a deer expert to you?',\n",
    " 'Do you know where I can find a deer expert?',\n",
    " 'Hmm. You could try the Aldo Leopold Wildlife Center.',\n",
    " 'Ugh. I have to head over there and check out the animals.',\n",
    " \"I'll ride with you!\",\n",
    " \"Come on, kid. You're slowing me down.\",\n",
    " 'Head over to the Wildlife Center!',\n",
    " \"I'm sure they'll be able to help.\",\n",
    " 'People sure drink a lot of coffee around here.',\n",
    " \"Gah. I can't believe this.\",\n",
    " 'Ugh... I think that lynx is looking at me funny.',\n",
    " 'Oh no! What happened to that crane?',\n",
    " 'Her beak is stuck in a coffee cup.',\n",
    " \"It's lucky we found her.\",\n",
    " 'Ugh! Those cups are all over the place.',\n",
    " \"I need to get her free. She won't hold still!\",\n",
    " 'Can Teddy and I help?',\n",
    " 'Sure! Give it a try.',\n",
    " 'Careful. That beak is sharp!',\n",
    " 'We need to calm her down, Teddy.',\n",
    " 'Any ideas?',\n",
    " '\\\\u00f0\\\\u0178\\\\u00a6\\\\u2014',\n",
    " 'Oh yeah, cranes eat insects!',\n",
    " 'Luckily there are tons of insects around here...',\n",
    " 'Got one!',\n",
    " \"Maybe she'll let me take off the cup!\",\n",
    " \"It's OK, girl! Look, I found you a cricket!\",\n",
    " 'You did it! Thanks, kid.',\n",
    " 'Can I help you with anything?',\n",
    " \"I'm investigating this symbol.\",\n",
    " 'Does it look like a deer hoof?',\n",
    " \"There's a diagram of animal tracks over there.\",\n",
    " 'Go take a look!',\n",
    " \"That hoofprint doesn't match the flag!\",\n",
    " 'Thanks for your help, kid!',\n",
    " \"So? What'd you find out?\",\n",
    " \"Looks like it's not a deer hoof.\",\n",
    " 'Great. Just great. Could this day get any worse?!',\n",
    " 'Hey, Wells...',\n",
    " 'I think I can help with your animal problem.',\n",
    " \"Ha! I don't need your help.\",\n",
    " \"Fine. Then I guess you don't want a real, live badger for the exhibit.\",\n",
    " 'Wait! What?! Really?',\n",
    " 'Wells, meet Teddy.',\n",
    " '\\\\u00f0\\\\u0178\\\\u02dc\\\\u0160',\n",
    " \"He says he'd be willing to help out.\",\n",
    " 'A real, live ferret!',\n",
    " 'And we still need to figure out that flag!',\n",
    " \"Fine, fine. Let's see...\",\n",
    " 'Actually, I went to school with somebody who LOVES old flags.',\n",
    " \"Why don't you go talk to her? I'll let her know you're coming.\",\n",
    " 'Hey, nice dog! What breed is he?',\n",
    " \"Actually, he's a badger.\",\n",
    " \"Oh, cool! I've never seen a badger in real life.\",\n",
    " \"You've got a million flags here!\",\n",
    " \"Yep. I'm a vexillophile!\",\n",
    " 'A vexy-wha?',\n",
    " 'It just means flag expert. How can I help?',\n",
    " \"I'm investigating this flag.\",\n",
    " 'Can you take a look?',\n",
    " \"Hey, I've seen that symbol before! Check it out!\",\n",
    " '\\\\Ecology flag, by Ron Cobb.\\\\',\n",
    " \"It's an ecology flag!\",\n",
    " 'Do you know what this flag was used for?',\n",
    " \"I'm not sure.\",\n",
    " \"If I were you, I'd go to the library and do some digging.\",\n",
    " 'Good idea. Thanks!',\n",
    " 'Welcome back, Dear! How can I help you?',\n",
    " 'I need to learn more about this flag!',\n",
    " 'It has something to do with ecology.',\n",
    " 'Hmm... those stripes remind me of the American flag.',\n",
    " 'Your flag must have been part of a national movement!',\n",
    " \"Go check the microfiche. Maybe you'll find something!\",\n",
    " \"Hey! That's Governor Nelson in front of our flag!\",\n",
    " 'I found the flag! Governor Nelson used it on the first Earth Day!',\n",
    " 'Wow! You figured it out!',\n",
    " 'Now I just need some old photos, like last time.',\n",
    " 'The boss is gonna love it!!!',\n",
    " 'You could try the archives.',\n",
    " 'Though the archivist might be too busy to help...',\n",
    " \"Oh, trust me. He'll make time.\",\n",
    " 'üôÑ',\n",
    " \"Actually, we're just here for some photos.\",\n",
    " \"It's for the flag display!\",\n",
    " 'Wait a minute...',\n",
    " \"YOU'RE the new history detective everybody's talking about?\",\n",
    " \"Teddy's helping too.\",\n",
    " 'What kind of photos do you need?',\n",
    " 'Something to do with ecology and Wisconsin.',\n",
    " \"Here's a call number for the Stacks. Go find some photos.\",\n",
    " 'Look at all those activists!',\n",
    " 'This is perfect for the exhibit.',\n",
    " 'I should go to the Capitol and tell Mrs. M!',\n",
    " 'I should see what Grampa is up to!',\n",
    " 'What should I do first?',\n",
    " 'Head upstairs and talk to the archivist. He might be able to help!',\n",
    " \"It's locked!\",\n",
    " \"Jolie! I was hoping you'd stop by. Any news on the flag artifact?\",\n",
    " \"Well, get on it. I'm counting on you to figure this out!\",\n",
    " 'Nice seeing you, Jolie!',\n",
    " \"It's such a nice fall day.\",\n",
    " 'I love these photos of me and Teddy.',\n",
    " \"Why don't you go talk to the boss?\",\n",
    " \"She's right outside.\",\n",
    " 'My friend is a flag expert.',\n",
    " 'She should be able to help you out.',\n",
    " 'There are some old newspapers loaded up in the microfiche.',\n",
    " 'The Stacks are right outside the door. Go find some photos!',\n",
    " 'Well, Leopold here is always losing papers...',\n",
    " 'Ha. Told you so!',\n",
    " 'Can we hurry up, Gramps?',\n",
    " 'Teddy and I were gonna go climb that huge tree out back!',\n",
    " \"Hmmm. Don't forget about your homework.\",\n",
    " 'Your teacher said you missed 7 assignments in a row!',\n",
    " 'So? History is boring!',\n",
    " 'I suppose historians are boring, too?',\n",
    " \"No way, Gramps. You're the best!\",\n",
    " 'Then do it for me!',\n",
    " 'Your teacher said you could help me for extra credit.',\n",
    " 'A boring old shirt.',\n",
    " 'What the-',\n",
    " \"He's wrong about old shirts and his name rhymes with \\\\smells\\\\...\",\n",
    " \"I don't have time for this, Gramps.\",\n",
    " 'Teddy is still missing!',\n",
    " \"Let's follow those scratch marks!\",\n",
    " \"I can't go with you. I need to take the artifact upstairs.\",\n",
    " \"It's okay, Gramps. I'll go by myself.\",\n",
    " 'You stole Teddy! How could you?!',\n",
    " \"No he hasn't!\",\n",
    " \"Yes, he has. I've seen him eating homework and important papers, too.\",\n",
    " 'Come on, Teddy.',\n",
    " \"Let's go find Gramps!\",\n",
    " 'Um... what did you want me to do again?',\n",
    " 'Head over to the Basketball Center.',\n",
    " 'Hopefully you can rustle up some clues!',\n",
    " 'I should stay and look for clues!',\n",
    " 'Where should I go again?',\n",
    " 'You could try the archivist. Maybe he can help you find Wells!',\n",
    " 'Hi, Mrs. M.',\n",
    " 'Head back to the museum. Your gramps is waiting for you.',\n",
    " \"I don't need that right now.\",\n",
    " \"I feel like I'm forgetting something.\",\n",
    " 'Gramps is the best historian ever!',\n",
    " 'This button never works!',\n",
    " \"Look at that! It's the bee's knees!\",\n",
    " \"Well, I did SOME of those. I just couldn't find them!\",\n",
    " 'Did you do all of them?',\n",
    " 'No... because history is boring!',\n",
    " 'Hooray, a boring old shirt.',\n",
    " 'I got here and the whole place was ransacked!',\n",
    " '*grumble grumble*',\n",
    " 'Knew what?',\n",
    " 'Did you have a question or not?',\n",
    " 'Yes!',\n",
    " \"You're still here? I'm trying to work!\",\n",
    " 'Now I just need to find all the cleaners from wayyyy back in 1916.',\n",
    " 'Hi! *cough*',\n",
    " 'Can you help-',\n",
    " '*cough cough*',\n",
    " 'Can you help me-',\n",
    " '*COUGH COUGH COUGH*',\n",
    " 'Um, are you okay?',\n",
    " \"Oh, I'm fine! Just a little hoarse.\",\n",
    " 'Ha! What do you call a pony with a sore throat?',\n",
    " 'A little horse!',\n",
    " \"Ha! You're funny.\",\n",
    " 'I got that one from my Gramps!',\n",
    " \"Yup, that's him!\",\n",
    " \"Unless you're too busy horsing around.\",\n",
    " 'Ha! Good one.',\n",
    " 'Two missions, actually!',\n",
    " 'Oh my!',\n",
    " \"I think it's a flag! Pretty spiffy, eh?\",\n",
    " \"Great Scott, you're right!\",\n",
    " \"Jo! I can't go with you. I need to take the artifact upstairs.\",\n",
    " '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00ad',\n",
    " '\\\\u00e2\\\\u009d\\\\u00a4\\\\u00ef\\\\u00b8\\\\u008f',\n",
    " 'GRRRRRRR',\n",
    " 'GAH! And what is THAT doing out of its cage?!',\n",
    " '\\\\u00f0\\\\u0178\\\\u02dc\\\\u0090',\n",
    " 'Teddy! Did you really eat his lunch?',\n",
    " \"Did you steal Gramps's paperwork too?!\",\n",
    " 'And my homework?!?!',\n",
    " 'See?!',\n",
    " \"That thing's a monster!\",\n",
    " \"I don't have time for this.\",\n",
    " 'YEAH!',\n",
    " 'Wait- me?',\n",
    " \"You can't just steal Jolie's pet. Don't you know badgers are protected animals?\",\n",
    " 'Besides, he looks friendly to me.',\n",
    " 'Wha?!',\n",
    " '\\\\u00f0\\\\u0178\\\\u02dc\\\\u009d',\n",
    " 'FINE. That possum better not scratch my leather seats...',\n",
    " \"He's a badger!\",\n",
    " '\\\\u00f0\\\\u0178\\\\u00a7\\\\u02dc',\n",
    " 'Yoga does sound nice.',\n",
    " \"But cranes can't do yoga, Teddy!\",\n",
    " '\\\\u00f0\\\\u0178\\\\u008d\\\\u00a9',\n",
    " \"Cranes don't eat donuts!\",\n",
    " 'Besides, you just ate my last snack.',\n",
    " \"I'm a historian, not a zookeeper!\",\n",
    " 'And this place is dirty, and itchy, and-',\n",
    " 'I love it!',\n",
    " \"Of course you do. You've got a rodent following you around.\",\n",
    " \"Actually, badgers aren't rodents-\",\n",
    " 'Whatever.',\n",
    " \"Yes!!! I'm saved!\",\n",
    " \"He's. A. Badger.\",\n",
    " 'Ooh... \\\\Ecology flag, by Ron Cobb.\\\\',\n",
    " \"You again! Don't let him hurt me!\",\n",
    " '\\\\u00f0\\\\u0178\\\\u2122\\\\u201e',\n",
    " 'Guess so!',\n",
    " 'YOU?!',\n",
    " \"Just please, don't let your badger eat them!\",\n",
    " 'I should help Gramps clean.',\n",
    " \"Maybe there's a clue in this mess!\",\n",
    " \"Poor Gramps! I should make sure he's okay.\",\n",
    " 'The archivist said I should look in the stacks.',\n",
    " 'There should be some info about that symbol in my book.',\n",
    " 'I should go talk to Gramps!',\n",
    " 'Yeah. Thanks anyway.',\n",
    " 'What are you waiting for? The Stacks are right outside the door.',\n",
    " \"I'll be in the collection room. Come find me when you're ready to check out the artifact.\",\n",
    " 'Good luck!',\n",
    " 'Wha... but...',\n",
    " \"Weren't you going to check out our microfiche?\",\n",
    " \"I'm sure you'll find Theodora in there somewhere!\",\n",
    " \"But I hear the museum's got one on the loose!\",\n",
    " 'So much cleaning to do...',\n",
    " 'I should check out that pair of glasses.',\n",
    " 'I should ask the librarian where to go next.',\n",
    " \"Check out the archives. They've got tons of old photos!\",\n",
    " 'I used to have a magnifying glass around here\\\\u00e2\\\\u20ac\\\\u00a6',\n",
    " \"Did you drop something, Dear? There's a card on the floor.\",\n",
    " 'Take a look!',\n",
    " 'I should see what Gramps is up to!',\n",
    " 'I found it!',\n",
    " 'Theodora wearing the shirt!',\n",
    " 'You better get to the capitol!',\n",
    " 'Nice decorations.',\n",
    " 'Did you drop something, Dear?',\n",
    " 'Gramps said to look for clues. Better look around.',\n",
    " 'I should find out if she can help me!',\n",
    " 'Ooh, nice decorations!',\n",
    " 'The libarian said I could find some information on Youmans in here...',\n",
    " 'Have a look at the artifact!',\n",
    " 'What is it, Teddy?',\n",
    " 'Oh no... they got sick from polluted water?',\n",
    " 'Poor foxes!',\n",
    " 'I should ask the librarian why Wells was here.',\n",
    " \"I wonder if there's a clue in those business cards...\",\n",
    " 'Thanks. Did you figure out the shirt?',\n",
    " 'Jolie! Where have you been?',\n",
    " 'The exhibit opens tomorrow.',\n",
    " 'Welcome back, Jolie. Did you figure out the shirt?',\n",
    " 'Wells got in trouble for littering at the Wildlife Center.',\n",
    " 'I should check that logbook to see who owned this slip...',\n",
    " 'AND I know who took Teddy!',\n",
    " 'Who is Teddy?',\n",
    " \"And where's your grampa?\",\n",
    " 'Sorry for the delay, Boss.',\n",
    " 'I had some cleaning up to do in my office.',\n",
    " 'Mrs. M, I think Wells kidnapped Teddy.',\n",
    " \"And he messed up Gramps's office, too!\",\n",
    " 'One step at a time, Jo.',\n",
    " 'Did you figure out the shirt?',\n",
    " 'I knew you could do it, Jo!',\n",
    " 'Now can I tell you what happened to Teddy?',\n",
    " 'He needs our help!',\n",
    " \"Sorry I'm late.\",\n",
    " \"Wells! Where's Teddy? Is he okay?\",\n",
    " 'I figured out that you kidnapped him!',\n",
    " 'Easy, Jo.',\n",
    " \"Why don't you prove your case?\",\n",
    " \"It'll be okay, Jo. We'll find Teddy!\",\n",
    " 'Nice work on the shirt, Jolie!',\n",
    " 'Leopold, can you run back to the museum?',\n",
    " 'Sounds good, Boss.',\n",
    " 'Jo, meet me back at my office.',\n",
    " 'I hope you find your badger, kid.',\n",
    " 'Thanks!',\n",
    " \"Are you going home now? Tomorrow's the big day!\",\n",
    " 'He got a park named after him? Cool!',\n",
    " 'Come on, Jo!',\n",
    " \"Meet me back in my office and we'll get started!\",\n",
    " 'Here I am!',\n",
    " 'Wells sabotaged Gramps!',\n",
    " 'AND he stole Teddy!'],\n",
    " 'fqid': ['intro',\n",
    "  'gramps',\n",
    "  'teddy',\n",
    "  'photo',\n",
    "  'notebook',\n",
    "  'retirement_letter',\n",
    "  'tobasement',\n",
    "  'janitor',\n",
    "  'toentry',\n",
    "  'groupconvo',\n",
    "  'report',\n",
    "  'boss',\n",
    "  'wells',\n",
    "  'directory',\n",
    "  'tocollection',\n",
    "  'cs',\n",
    "  'tunic',\n",
    "  'tunic.hub.slip',\n",
    "  'tostacks',\n",
    "  'outtolunch',\n",
    "  'tocloset',\n",
    "  'tomap',\n",
    "  'tunic.historicalsociety',\n",
    "  'tunic.kohlcenter',\n",
    "  'plaque',\n",
    "  'plaque.face.date',\n",
    "  'togrampa',\n",
    "  'tunic.capitol_0',\n",
    "  'chap1_finale',\n",
    "  'chap1_finale_c',\n",
    "  'tocloset_dirty',\n",
    "  'what_happened',\n",
    "  'trigger_scarf',\n",
    "  'trigger_coffee',\n",
    "  'tunic.capitol_1',\n",
    "  'tofrontdesk',\n",
    "  'archivist',\n",
    "  'magnify',\n",
    "  'tunic.humanecology',\n",
    "  'worker',\n",
    "  'businesscards',\n",
    "  'businesscards.card_0.next',\n",
    "  'businesscards.card_1.next',\n",
    "  'businesscards.card_bingo.next',\n",
    "  'businesscards.card_bingo.bingo',\n",
    "  'tohallway',\n",
    "  'tunic.drycleaner',\n",
    "  'logbook',\n",
    "  'logbook.page.bingo',\n",
    "  'tunic.library',\n",
    "  'tomicrofiche',\n",
    "  'reader',\n",
    "  'reader.paper0.next',\n",
    "  'reader.paper1.next',\n",
    "  'reader.paper2.bingo',\n",
    "  'wellsbadge',\n",
    "  'journals',\n",
    "  'journals.hub.topics',\n",
    "  'journals.pic_0.next',\n",
    "  'journals.pic_1.next',\n",
    "  'journals.pic_2.bingo',\n",
    "  'chap2_finale_c',\n",
    "  'ch3start',\n",
    "  'seescratches',\n",
    "  'tocage',\n",
    "  'glasses',\n",
    "  'directory.closeup.archivist',\n",
    "  'key',\n",
    "  'unlockdoor',\n",
    "  'confrontation',\n",
    "  'savedteddy',\n",
    "  'tocollectionflag',\n",
    "  'groupconvo_flag',\n",
    "  'tunic.capitol_2',\n",
    "  'tunic.wildlife',\n",
    "  'coffee',\n",
    "  'crane_ranger',\n",
    "  'remove_cup',\n",
    "  'expert',\n",
    "  'tracks',\n",
    "  'tracks.hub.deer',\n",
    "  'tunic.flaghouse',\n",
    "  'flag_girl',\n",
    "  'colorbook',\n",
    "  'reader_flag',\n",
    "  'reader_flag.paper0.next',\n",
    "  'reader_flag.paper1.next',\n",
    "  'reader_flag.paper2.bingo',\n",
    "  'archivist_glasses',\n",
    "  'journals_flag',\n",
    "  'journals_flag.hub.topics_old',\n",
    "  'journals_flag.hub.topics',\n",
    "  'journals_flag.pic_0.bingo',\n",
    "  'journals_flag.pic_0.next',\n",
    "  'chap4_finale_c',\n",
    "  'block_tocollection',\n",
    "  'reader.paper2.next',\n",
    "  'journals.pic_2.next',\n",
    "  'lockeddoor',\n",
    "  'reader.paper2.prev',\n",
    "  'reader.paper0.prev',\n",
    "  'reader_flag.paper1.prev',\n",
    "  'journals_flag.pic_0_old.next',\n",
    "  'journals_flag.pic_1_old.next',\n",
    "  'door_block_clean',\n",
    "  'door_block_talk',\n",
    "  'block',\n",
    "  'reader_flag.paper2.next',\n",
    "  'journals_flag.pic_1.bingo',\n",
    "  'journals_flag.pic_1.next',\n",
    "  'journals_flag.pic_2.bingo',\n",
    "  'journals_flag.pic_2.next',\n",
    "  'reader_flag.paper0.prev',\n",
    "  'reader.paper1.prev',\n",
    "  'block_magnify',\n",
    "  'journals_flag.pic_2_old.next',\n",
    "  'block_0',\n",
    "  'doorblock',\n",
    "  'block_tomap1',\n",
    "  'block_tomap2',\n",
    "  'reader_flag.paper2.prev',\n",
    "  'need_glasses',\n",
    "  'block_badge',\n",
    "  'block_nelson',\n",
    "  'block_badge_2',\n",
    "  'block_1',\n",
    "  'fox'],\n",
    " 'room_fqid': ['tunic.historicalsociety.closet',\n",
    "  'tunic.historicalsociety.basement',\n",
    "  'tunic.historicalsociety.entry',\n",
    "  'tunic.historicalsociety.collection',\n",
    "  'tunic.historicalsociety.stacks',\n",
    "  'tunic.kohlcenter.halloffame',\n",
    "  'tunic.capitol_0.hall',\n",
    "  'tunic.historicalsociety.closet_dirty',\n",
    "  'tunic.historicalsociety.frontdesk',\n",
    "  'tunic.humanecology.frontdesk',\n",
    "  'tunic.drycleaner.frontdesk',\n",
    "  'tunic.library.frontdesk',\n",
    "  'tunic.library.microfiche',\n",
    "  'tunic.capitol_1.hall',\n",
    "  'tunic.historicalsociety.cage',\n",
    "  'tunic.historicalsociety.collection_flag',\n",
    "  'tunic.wildlife.center',\n",
    "  'tunic.flaghouse.entry',\n",
    "  'tunic.capitol_2.hall'],\n",
    " 'text_fqid': ['tunic.historicalsociety.closet.intro',\n",
    "  'tunic.historicalsociety.closet.gramps.intro_0_cs_0',\n",
    "  'tunic.historicalsociety.closet.teddy.intro_0_cs_0',\n",
    "  'tunic.historicalsociety.closet.teddy.intro_0_cs_5',\n",
    "  'tunic.historicalsociety.closet.photo',\n",
    "  'tunic.historicalsociety.closet.notebook',\n",
    "  'tunic.historicalsociety.closet.retirement_letter.hub',\n",
    "  'tunic.historicalsociety.basement.janitor',\n",
    "  'tunic.historicalsociety.entry.groupconvo',\n",
    "  'tunic.historicalsociety.entry.boss.talktogramps',\n",
    "  'tunic.historicalsociety.entry.wells.talktogramps',\n",
    "  'tunic.historicalsociety.collection.cs',\n",
    "  'tunic.historicalsociety.collection.tunic.slip',\n",
    "  'tunic.historicalsociety.collection.gramps.found',\n",
    "  'tunic.historicalsociety.stacks.outtolunch',\n",
    "  'tunic.kohlcenter.halloffame.plaque.face.date',\n",
    "  'tunic.kohlcenter.halloffame.togrampa',\n",
    "  'tunic.capitol_0.hall.boss.talktogramps',\n",
    "  'tunic.historicalsociety.closet_dirty.what_happened',\n",
    "  'tunic.historicalsociety.closet_dirty.gramps.helpclean',\n",
    "  'tunic.historicalsociety.closet_dirty.trigger_scarf',\n",
    "  'tunic.historicalsociety.closet_dirty.trigger_coffee',\n",
    "  'tunic.historicalsociety.closet_dirty.gramps.news',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.hello',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.need_glass_0',\n",
    "  'tunic.historicalsociety.frontdesk.magnify',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.have_glass',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.have_glass_recap',\n",
    "  'tunic.humanecology.frontdesk.worker.intro',\n",
    "  'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo',\n",
    "  'tunic.humanecology.frontdesk.worker.badger',\n",
    "  'tunic.drycleaner.frontdesk.worker.hub',\n",
    "  'tunic.drycleaner.frontdesk.logbook.page.bingo',\n",
    "  'tunic.drycleaner.frontdesk.worker.done',\n",
    "  'tunic.library.frontdesk.worker.hello',\n",
    "  'tunic.library.microfiche.reader.paper2.bingo',\n",
    "  'tunic.library.frontdesk.wellsbadge.hub',\n",
    "  'tunic.library.frontdesk.worker.wells',\n",
    "  'tunic.capitol_1.hall.boss.haveyougotit',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.newspaper',\n",
    "  'tunic.historicalsociety.stacks.journals.pic_2.bingo',\n",
    "  'tunic.historicalsociety.basement.ch3start',\n",
    "  'tunic.historicalsociety.basement.gramps.whatdo',\n",
    "  'tunic.historicalsociety.basement.seescratches',\n",
    "  'tunic.historicalsociety.cage.glasses.beforeteddy',\n",
    "  'tunic.historicalsociety.cage.teddy.trapped',\n",
    "  'tunic.historicalsociety.cage.glasses.afterteddy',\n",
    "  'tunic.historicalsociety.entry.directory.closeup.archivist',\n",
    "  'tunic.historicalsociety.frontdesk.key',\n",
    "  'tunic.historicalsociety.cage.unlockdoor',\n",
    "  'tunic.historicalsociety.cage.confrontation',\n",
    "  'tunic.historicalsociety.basement.savedteddy',\n",
    "  'tunic.historicalsociety.collection_flag.gramps.flag',\n",
    "  'tunic.historicalsociety.entry.groupconvo_flag',\n",
    "  'tunic.historicalsociety.entry.wells.flag',\n",
    "  'tunic.historicalsociety.entry.boss.flag',\n",
    "  'tunic.historicalsociety.entry.wells.flag_recap',\n",
    "  'tunic.historicalsociety.entry.boss.flag_recap',\n",
    "  'tunic.wildlife.center.coffee',\n",
    "  'tunic.wildlife.center.wells.animals',\n",
    "  'tunic.wildlife.center.wells.animals2',\n",
    "  'tunic.wildlife.center.crane_ranger.crane',\n",
    "  'tunic.wildlife.center.remove_cup',\n",
    "  'tunic.wildlife.center.expert.removed_cup',\n",
    "  'tunic.wildlife.center.tracks.hub.deer',\n",
    "  'tunic.wildlife.center.expert.recap',\n",
    "  'tunic.wildlife.center.wells.nodeer',\n",
    "  'tunic.flaghouse.entry.flag_girl.hello',\n",
    "  'tunic.flaghouse.entry.colorbook',\n",
    "  'tunic.flaghouse.entry.flag_girl.symbol',\n",
    "  'tunic.flaghouse.entry.flag_girl.symbol_recap',\n",
    "  'tunic.library.frontdesk.worker.flag',\n",
    "  'tunic.library.microfiche.reader_flag.paper2.bingo',\n",
    "  'tunic.library.frontdesk.worker.nelson',\n",
    "  'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation',\n",
    "  'tunic.historicalsociety.stacks.journals_flag.pic_0.bingo',\n",
    "  'tunic.historicalsociety.entry.block_tocollection',\n",
    "  'tunic.historicalsociety.closet_dirty.gramps.archivist',\n",
    "  'tunic.historicalsociety.cage.lockeddoor',\n",
    "  'tunic.capitol_2.hall.boss.haveyougotit',\n",
    "  'tunic.drycleaner.frontdesk.worker.done2',\n",
    "  'tunic.library.frontdesk.worker.preflag',\n",
    "  'tunic.historicalsociety.closet_dirty.photo',\n",
    "  'tunic.historicalsociety.collection_flag.gramps.recap',\n",
    "  'tunic.wildlife.center.wells.nodeer_recap',\n",
    "  'tunic.library.frontdesk.worker.flag_recap',\n",
    "  'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap',\n",
    "  'tunic.historicalsociety.closet_dirty.door_block_clean',\n",
    "  'tunic.historicalsociety.closet_dirty.door_block_talk',\n",
    "  'tunic.historicalsociety.stacks.block',\n",
    "  'tunic.flaghouse.entry.flag_girl.hello_recap',\n",
    "  'tunic.historicalsociety.stacks.journals_flag.pic_1.bingo',\n",
    "  'tunic.historicalsociety.stacks.journals_flag.pic_2.bingo',\n",
    "  'tunic.historicalsociety.collection.tunic',\n",
    "  'tunic.library.frontdesk.worker.wells_recap',\n",
    "  'tunic.historicalsociety.frontdesk.block_magnify',\n",
    "  'tunic.humanecology.frontdesk.block_0',\n",
    "  'tunic.historicalsociety.basement.gramps.seeyalater',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.need_glass_1',\n",
    "  'tunic.historicalsociety.closet.doorblock',\n",
    "  'tunic.library.frontdesk.worker.droppedbadge',\n",
    "  'tunic.library.frontdesk.worker.hello_short',\n",
    "  'tunic.capitol_1.hall.boss.writeitup',\n",
    "  'tunic.historicalsociety.entry.block_tomap1',\n",
    "  'tunic.historicalsociety.entry.block_tomap2',\n",
    "  'tunic.capitol_0.hall.chap1_finale_c',\n",
    "  'tunic.historicalsociety.collection.gramps.lost',\n",
    "  'tunic.historicalsociety.closet_dirty.gramps.nothing',\n",
    "  'tunic.drycleaner.frontdesk.worker.takealook',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.foundtheodora',\n",
    "  'tunic.historicalsociety.cage.need_glasses',\n",
    "  'tunic.library.frontdesk.worker.nelson_recap',\n",
    "  'tunic.library.frontdesk.block_badge',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.newspaper_recap',\n",
    "  'tunic.kohlcenter.halloffame.block_0',\n",
    "  'tunic.library.frontdesk.block_nelson',\n",
    "  'tunic.capitol_1.hall.chap2_finale_c',\n",
    "  'tunic.library.microfiche.block_0',\n",
    "  'tunic.library.frontdesk.block_badge_2',\n",
    "  'tunic.capitol_2.hall.chap4_finale_c',\n",
    "  'tunic.historicalsociety.collection.gramps.look_0',\n",
    "  'tunic.humanecology.frontdesk.block_1',\n",
    "  'tunic.drycleaner.frontdesk.block_0',\n",
    "  'tunic.wildlife.center.fox.concern',\n",
    "  'tunic.historicalsociety.entry.gramps.hub',\n",
    "  'tunic.drycleaner.frontdesk.block_1'],\n",
    "  'event_name+fqid':[\n",
    "  'cutscene_click_intro', \n",
    "  'person_click_gramps',\n",
    "  'person_click_teddy',\n",
    "  'navigate_click_teddy',\n",
    "  'navigate_click_photo', \n",
    "  'observation_click_photo',\n",
    "  'navigate_click_notebook', \n",
    "  'object_click_notebook',\n",
    "  'navigate_click_retirement_letter',\n",
    "  'object_click_retirement_letter',\n",
    "  'navigate_click_tobasement',\n",
    "  'navigate_click_janitor',\n",
    "  'observation_click_janitor',\n",
    "  'navigate_click_toentry', \n",
    "  'navigate_click_groupconvo',\n",
    "  'cutscene_click_groupconvo', \n",
    "  'object_hover_groupconvo',\n",
    "  'object_click_report', \n",
    "  'navigate_click_boss', \n",
    "  'person_click_boss',\n",
    "  'navigate_click_wells', \n",
    "  'person_click_wells',\n",
    "  'navigate_click_directory', \n",
    "  'object_click_directory',\n",
    "  'navigate_click_tocollection', \n",
    "  'cutscene_click_cs',\n",
    "  'navigate_click_tunic', \n",
    "  'object_hover_tunic', \n",
    "  'object_click_tunic',\n",
    "  'object_click_tunic.hub.slip', \n",
    "  'object_hover_tunic.hub.slip',\n",
    "  'navigate_click_gramps', \n",
    "  'navigate_click_tostacks',\n",
    "  'navigate_click_outtolunch', \n",
    "  'observation_click_outtolunch',\n",
    "  'navigate_click_tocloset', \n",
    "  'navigate_click_tomap',\n",
    "  'map_hover_tunic.historicalsociety', \n",
    "  'map_hover_tunic.kohlcenter',\n",
    "  'map_click_tunic.kohlcenter', \n",
    "  'navigate_click_plaque',\n",
    "  'object_click_plaque.face.date', \n",
    "  'object_click_plaque',\n",
    "  'object_hover_plaque.face.date', \n",
    "  'cutscene_click_togrampa',\n",
    "  'map_hover_toentry', \n",
    "  'map_click_tunic.capitol_0',\n",
    "  'navigate_click_chap1_finale', \n",
    "  'checkpoint_chap1_finale_c',\n",
    "  'map_click_tunic.historicalsociety',\n",
    "  'navigate_click_tocloset_dirty', \n",
    "  'cutscene_click_what_happened',\n",
    "  'navigate_click_trigger_scarf', \n",
    "  'cutscene_click_trigger_scarf',\n",
    "  'navigate_click_trigger_coffee', \n",
    "  'cutscene_click_trigger_coffee',\n",
    "  'map_hover_tomap', \n",
    "  'map_hover_tunic.capitol_1',\n",
    "  'navigate_click_tofrontdesk', \n",
    "  'navigate_click_archivist',\n",
    "  'person_click_archivist', \n",
    "  'navigate_click_magnify',\n",
    "  'observation_click_magnify', \n",
    "  'map_click_tunic.humanecology',\n",
    "  'navigate_click_worker', \n",
    "  'person_click_worker',\n",
    "  'navigate_click_businesscards',\n",
    "  'object_hover_businesscards.card_0.next',\n",
    "  'object_click_businesscards.card_0.next',\n",
    "  'object_click_businesscards.card_1.next',\n",
    "  'object_hover_businesscards.card_1.next',\n",
    "  'object_click_businesscards',\n",
    "  'object_hover_businesscards.card_bingo.next',\n",
    "  'object_click_businesscards.card_bingo.bingo',\n",
    "  'object_hover_businesscards.card_bingo.bingo',\n",
    "  'navigate_click_tohallway', \n",
    "  'map_click_tunic.drycleaner',\n",
    "  'navigate_click_logbook', \n",
    "  'object_hover_logbook.page.bingo',\n",
    "  'object_click_logbook', \n",
    "  'object_click_logbook.page.bingo',\n",
    "  'map_click_tunic.library', \n",
    "  'navigate_click_tomicrofiche',\n",
    "  'navigate_click_reader', \n",
    "  'object_hover_reader',\n",
    "  'object_hover_reader.paper0.next',\n",
    "  'object_click_reader.paper0.next',\n",
    "  'object_click_reader.paper1.next',\n",
    "  'object_click_reader.paper2.bingo',\n",
    "  'object_hover_reader.paper2.bingo', \n",
    "  'object_click_reader',\n",
    "  'navigate_click_wellsbadge', \n",
    "  'object_click_wellsbadge',\n",
    "  'map_hover_tunic.library', \n",
    "  'map_click_tunic.capitol_1',\n",
    "  'map_hover_tunic.drycleaner', \n",
    "  'navigate_click_journals',\n",
    "  'object_hover_journals', \n",
    "  'object_hover_journals.hub.topics',\n",
    "  'object_click_journals', \n",
    "  'object_click_journals.hub.topics',\n",
    "  'object_hover_journals.pic_0.next',\n",
    "  'object_click_journals.pic_0.next',\n",
    "  'object_click_journals.pic_1.next',\n",
    "  'object_click_journals.pic_2.bingo',\n",
    "  'object_hover_journals.pic_2.bingo',\n",
    "  'navigate_click_chap2_finale_c', \n",
    "  'checkpoint_chap2_finale_c',\n",
    "  'cutscene_click_ch3start', \n",
    "  'navigate_click_seescratches',\n",
    "  'cutscene_click_seescratches', \n",
    "  'navigate_click_tocage',\n",
    "  'navigate_click_glasses', \n",
    "  'person_click_glasses',\n",
    "  'object_hover_directory',\n",
    "  'object_hover_directory.closeup.archivist',\n",
    "  'object_click_directory.closeup.archivist', \n",
    "  'navigate_click_key',\n",
    "  'observation_click_key', \n",
    "  'navigate_click_unlockdoor',\n",
    "  'cutscene_click_unlockdoor', \n",
    "  'navigate_click_confrontation',\n",
    "  'cutscene_click_confrontation', \n",
    "  'cutscene_click_savedteddy',\n",
    "  'navigate_click_tocollectionflag',\n",
    "  'navigate_click_groupconvo_flag', \n",
    "  'cutscene_click_groupconvo_flag',\n",
    "  'map_hover_tunic.capitol_2', \n",
    "  'map_click_tunic.wildlife',\n",
    "  'navigate_click_coffee', \n",
    "  'observation_click_coffee',\n",
    "  'navigate_click_crane_ranger', \n",
    "  'person_click_crane_ranger',\n",
    "  'navigate_click_remove_cup', \n",
    "  'observation_click_remove_cup',\n",
    "  'navigate_click_expert', \n",
    "  'person_click_expert',\n",
    "  'navigate_click_tracks', \n",
    "  'object_hover_tracks',\n",
    "  'object_hover_tracks.hub.deer', \n",
    "  'object_click_tracks.hub.deer',\n",
    "  'object_click_tracks', \n",
    "  'map_hover_tunic.flaghouse',\n",
    "  'map_click_tunic.flaghouse', \n",
    "  'navigate_click_flag_girl',\n",
    "  'person_click_flag_girl', \n",
    "  'navigate_click_colorbook',\n",
    "  'object_click_colorbook', \n",
    "  'navigate_click_reader_flag',\n",
    "  'object_click_reader_flag', \n",
    "  'object_click_reader_flag.paper0.next',\n",
    "  'object_click_reader_flag.paper1.next',\n",
    "  'object_hover_reader_flag.paper0.next',\n",
    "  'object_click_reader_flag.paper2.bingo',\n",
    "  'object_hover_reader_flag.paper2.bingo',\n",
    "  'navigate_click_archivist_glasses',\n",
    "  'person_click_archivist_glasses', \n",
    "  'navigate_click_journals_flag',\n",
    "  'object_hover_journals_flag.hub.topics_old',\n",
    "  'object_click_journals_flag.hub.topics',\n",
    "  'object_hover_journals_flag.hub.topics',\n",
    "  'object_hover_journals_flag.pic_0.bingo',\n",
    "  'object_hover_journals_flag.pic_0.next',\n",
    "  'object_click_journals_flag.pic_0.bingo',\n",
    "  'object_click_journals_flag', \n",
    "  'map_click_tunic.capitol_2',\n",
    "  'navigate_click_chap4_finale_c', \n",
    "  'checkpoint_chap4_finale_c',\n",
    "  'navigate_click_block_tocollection',\n",
    "  'observation_click_block_tocollection',\n",
    "  'object_click_businesscards.card_bingo.next',\n",
    "  'map_hover_tohallway', \n",
    "  'object_hover_logbook',\n",
    "  'object_hover_reader.paper1.next',\n",
    "  'object_hover_reader.paper2.next', \n",
    "  'map_hover_tunic.humanecology',\n",
    "  'object_hover_journals.pic_1.next',\n",
    "  'object_click_journals.pic_2.next',\n",
    "  'object_hover_journals.pic_2.next', \n",
    "  'map_hover_tobasement',\n",
    "  'navigate_click_lockeddoor', \n",
    "  'observation_click_lockeddoor',\n",
    "  'object_hover_plaque', \n",
    "  'object_hover_reader.paper2.prev',\n",
    "  'object_click_reader.paper2.next',\n",
    "  'object_hover_reader.paper0.prev', \n",
    "  'map_hover_tunic.wildlife',\n",
    "  'object_hover_reader_flag', \n",
    "  'object_hover_reader_flag.paper1.prev',\n",
    "  'object_hover_reader_flag.paper1.next',\n",
    "  'object_click_journals_flag.hub.topics_old',\n",
    "  'object_click_journals_flag.pic_0_old.next',\n",
    "  'object_click_journals_flag.pic_1_old.next',\n",
    "  'object_hover_journals_flag.pic_0_old.next',\n",
    "  'object_hover_notebook', \n",
    "  'navigate_click_door_block_clean',\n",
    "  'cutscene_click_door_block_clean',\n",
    "  'navigate_click_door_block_talk', \n",
    "  'cutscene_click_door_block_talk',\n",
    "  'navigate_click_block', \n",
    "  'observation_click_block',\n",
    "  'object_hover_reader_flag.paper2.next',\n",
    "  'object_click_journals_flag.pic_0.next',\n",
    "  'object_click_journals_flag.pic_1.bingo',\n",
    "  'object_hover_journals_flag.pic_1.bingo',\n",
    "  'object_click_journals_flag.pic_1.next',\n",
    "  'object_hover_journals_flag.pic_1.next',\n",
    "  'object_click_journals_flag.pic_2.bingo',\n",
    "  'object_hover_journals_flag.pic_2.bingo',\n",
    "  'object_click_journals_flag.pic_2.next',\n",
    "  'object_hover_journals_flag.pic_2.next', \n",
    "  'object_hover_gramps',\n",
    "  'map_hover_tunic.capitol_0', \n",
    "  'object_hover_businesscards',\n",
    "  'object_hover_wellsbadge', \n",
    "  'object_hover_colorbook',\n",
    "  'object_click_reader_flag.paper2.next',\n",
    "  'object_click_reader_flag.paper0.prev',\n",
    "  'object_hover_reader_flag.paper0.prev',\n",
    "  'object_hover_journals_flag', \n",
    "  'object_hover_reader.paper1.prev',\n",
    "  'object_hover_retirement_letter',\n",
    "  'object_click_reader.paper1.prev', \n",
    "  'object_hover_boss',\n",
    "  'navigate_click_block_magnify', \n",
    "  'observation_click_block_magnify',\n",
    "  'object_hover_journals_flag.pic_1_old.next',\n",
    "  'object_click_journals_flag.pic_2_old.next',\n",
    "  'object_hover_journals_flag.pic_2_old.next',\n",
    "  'navigate_click_block_0', \n",
    "  'observation_click_block_0',\n",
    "  'navigate_click_doorblock', \n",
    "  'observation_click_doorblock',\n",
    "  'object_hover_doorblock', \n",
    "  'object_click_reader.paper0.prev',\n",
    "  'navigate_click_block_tomap1', \n",
    "  'observation_click_block_tomap1',\n",
    "  'navigate_click_block_tomap2', \n",
    "  'observation_click_block_tomap2',\n",
    "  'cutscene_click_chap1_finale_c', \n",
    "  'map_hover_boss',\n",
    "  'object_hover_reader_flag.paper2.prev',\n",
    "  'object_click_reader_flag.paper2.prev', \n",
    "  'object_hover_wells',\n",
    "  'object_click_reader.paper2.prev', \n",
    "  'navigate_click_need_glasses',\n",
    "  'observation_click_need_glasses',\n",
    "  'object_click_reader_flag.paper1.prev', \n",
    "  'object_hover_tostacks',\n",
    "  'navigate_click_block_badge', \n",
    "  'cutscene_click_block_badge',\n",
    "  'object_hover_worker', \n",
    "  'navigate_click_block_nelson',\n",
    "  'observation_click_block_nelson', \n",
    "  'cutscene_click_chap2_finale_c',\n",
    "  'map_hover_flag_girl', \n",
    "  'object_hover_tobasement',\n",
    "  'navigate_click_block_badge_2', \n",
    "  'observation_click_block_badge_2',\n",
    "  'cutscene_click_chap4_finale_c', \n",
    "  'map_hover_tostacks',\n",
    "  'object_hover_block_0', \n",
    "  'object_hover_toentry',\n",
    "  'object_hover_tofrontdesk', \n",
    "  'navigate_click_block_1',\n",
    "  'observation_click_block_1', \n",
    "  'map_hover_worker',\n",
    "  'map_hover_logbook', \n",
    "  'object_hover_block',\n",
    "  'object_hover_tocollection', \n",
    "  'navigate_click_fox',\n",
    "  'person_click_fox', \n",
    "  'object_hover_tomap', \n",
    "  'object_hover_flag_girl',\n",
    "  'object_hover_expert', \n",
    "  'object_hover_crane_ranger',\n",
    "  'map_hover_wells', \n",
    "  'object_hover_block_tomap1', \n",
    "  'map_hover_coffee',\n",
    "  'map_hover_expert', \n",
    "  'map_hover_directory',\n",
    "  'object_hover_tocollectionflag', \n",
    "  'map_hover_colorbook',\n",
    "  'object_hover_tomicrofiche', \n",
    "  'object_hover_block_1']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_translation_map():\n",
    "    df = pd.read_csv(cfg.prep_dir + \"game_type_text_translation.csv\")\n",
    "    text_translation_map = {}\n",
    "    text_translation_map.update({a:b for a, b in zip(df[\"dry\"].values, df[\"normal\"].values)})\n",
    "    text_translation_map.update({a:b for a, b in zip(df[\"nohumor\"].values, df[\"normal\"].values)})\n",
    "    text_translation_map.update({a:b for a, b in zip(df[\"nosnark\"].values, df[\"normal\"].values)})\n",
    "    return text_translation_map\n",
    "\n",
    "text_translation_map = get_text_translation_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labels„Éá„Éº„Çø„ÇíÊï¥ÂΩ¢„Åô„Çã\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # train„ÅÆÁâπÂæ¥Èáè„Å®ÁµêÂêà„Åô„Çã„Åü„ÇÅ„Å´question„Å´ÂØæÂøú„Åô„Çãlabel_group„ÇíÂàó„Å®„Åó„Å¶Ë®≠„Åë„Å¶„Åä„Åè\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labels„Éá„Éº„Çø„ÇíÊï¥ÂΩ¢„Åô„Çã\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTrain:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"session_id\", \"level_group\", \"elapsed_time\"], ignore_index=True)\n",
    "        self.features = self.sessions_df[[\"session_id\", \"level_group\"]].drop_duplicates().copy()\n",
    "        self.result = labels\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].shift(1)\n",
    "        self.sessions_df[\"text\"] = self.sessions_df[\"text\"].replace(text_translation_map)\n",
    "        self.sessions_df['event_name+fqid'] = self.sessions_df['event_name']+'_'+self.sessions_df['fqid']\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_group„Åî„Å®„ÅÆ„É¨„Ç≥„Éº„ÉâÊï∞\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":\"record_cnt\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_group„Åî„Å®„ÄÅepapsed_time„ÅÆmax - minÔºàÁµåÈÅéÊôÇÈñìÔºâ\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].agg([max,min]).reset_index()\n",
    "        add_features[\"group_elapsed_time\"] = add_features[\"max\"] - add_features[\"min\"]\n",
    "        add_features[\"group_elapsed_time\"] = add_features[\"group_elapsed_time\"].astype(np.float32)\n",
    "        add_features = add_features[[\"session_id\", \"level_group\", \"group_elapsed_time\"]].copy()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_group„Åî„Å®„ÄÅÂêÑ{cat}„ÅÆ„É¨„Ç≥„Éº„ÉâÊï∞\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[cat_col]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[\"index\"].count().reset_index().rename(columns={\"index\":\"cnt\"})\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{cat_col}_{str(cat)}_record_cnt\"\n",
    "            tmp = add_features[add_features[cat_col]==cat][[\"session_id\", \"level_group\", \"cnt\"]].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp = tmp.rename(columns={\"cnt\": feat_name})\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[feat_name] = self.features[feat_name].fillna(0)\n",
    "            else:\n",
    "                self.features[feat_name] = 0\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_group„Åî„Å®„ÄÅ[col]„ÅÆ„É¶„Éã„Éº„ÇØÊï∞\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.dropna(subset=[cat_col]).drop_duplicates([\"session_id\", \"level_group\", cat_col])\n",
    "        add_features = add_features.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{cat_col}_nunique\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")        \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        new_cols = [f\"{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "    def _cat_first_click_point(self, cat_col):\n",
    "        \"\"\"\n",
    "        {cat_col}„Åî„Å®„ÅÆÂàùÂõû„ÅÆ„ÇØ„É™„ÉÉ„ÇØÂ∫ßÊ®ô\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[cat_col]\n",
    "        first_record = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col]).tail(1).reset_index(drop=True)\n",
    "        first_record = first_record[[\"session_id\", \"level_group\", cat_col, \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"]].copy()\n",
    "        \n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{cat_col}_{cat}_first_click_{val_name}\" for val_name in [\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"]]\n",
    "            add_features = first_record[first_record[cat_col]==cat].drop(columns=[cat_col])\n",
    "            if len(add_features) == 0:\n",
    "                self.features[new_cols] = np.nan\n",
    "            else:\n",
    "                add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "                self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def get_train(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\", \"elapsed_time\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\", \"elapsed_time\"],\n",
    "                            aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                            cat_col=\"event_name+fqid\")\n",
    "\n",
    "        self.result = self.result.merge(self.features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "  \n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesInf:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"elapsed_time\"], ignore_index=True)\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.use_cols = [\n",
    "            \"elapsed_time\", \"event_name\", \"name\", \"level\", \"page\",\n",
    "            \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\",\n",
    "            \"hover_duration\", \"text\", \"fqid\", \"room_fqid\", \"text_fqid\", \"event_name+fqid\"\n",
    "        ]\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df['event_name+fqid'] = self.sessions_df['event_name']+'_'+self.sessions_df['fqid']\n",
    "        # dataframe„ÅÆÂêÑÂàó„Çínumpy array„Åß‰øùÊåÅ\n",
    "        self.sessions = {}\n",
    "        for c in self.use_cols:\n",
    "            self.sessions[c] = self.sessions_df[c].values\n",
    "        self.sessions[\"time_diff\"] = self.sessions[\"elapsed_time\"] - self.sessions_df[\"elapsed_time\"].shift(1).values\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_group„Åî„Å®„ÅÆ„É¨„Ç≥„Éº„ÉâÊï∞\n",
    "        \"\"\"\n",
    "        add_feature = len(self.sessions[\"elapsed_time\"])\n",
    "        self.result[\"record_cnt\"] = add_feature\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_group„Åî„Å®„ÄÅepapsed_time„ÅÆmax - minÔºàÁµåÈÅéÊôÇÈñìÔºâ\n",
    "        \"\"\"\n",
    "        add_feature = np.max(self.sessions[\"elapsed_time\"]) - np.min(self.sessions[\"elapsed_time\"])\n",
    "        self.result[\"group_elapsed_time\"] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_group„Åî„Å®„ÄÅÂêÑ{cat}„ÅÆ„É¨„Ç≥„Éº„ÉâÊï∞\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[cat_col]\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{cat_col}_{str(cat)}_record_cnt\"\n",
    "            add_feature = (self.sessions[cat_col] == cat).astype(int).sum()\n",
    "            self.result[feat_name] = add_feature\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_group„Åî„Å®„ÄÅ[col]„ÅÆ„É¶„Éã„Éº„ÇØÊï∞\n",
    "        \"\"\"\n",
    "        self.result[f\"{cat_col}_nunique\"] = self.sessions_df[cat_col].dropna().nunique()       \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        for val_col, agg in itertools.product(val_cols, aggs):\n",
    "            feat_name = f\"{val_col}_{agg}\"\n",
    "            if agg == \"mean\":\n",
    "                add_feature = np.nanmean(self.sessions[val_col])\n",
    "            elif agg == \"max\":\n",
    "                add_feature = np.nanmax(self.sessions[val_col])\n",
    "            elif agg == \"min\":\n",
    "                add_feature = np.nanmin(self.sessions[val_col])\n",
    "            elif agg == \"std\":\n",
    "                add_feature = np.nanstd(self.sessions[val_col], ddof=1)\n",
    "            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    tmp = self.sessions[val_col][idx]\n",
    "                    if agg == \"mean\":\n",
    "                        add_feature = np.nanmean(tmp)\n",
    "                    elif agg == \"max\":\n",
    "                        add_feature = np.nanmax(tmp)\n",
    "                    elif agg == \"min\":\n",
    "                        add_feature = np.nanmin(tmp)\n",
    "                    elif agg == \"std\":\n",
    "                        add_feature = np.nanstd(tmp, ddof=1)\n",
    "                    if np.isnan(add_feature):\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def get_test(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\", \"elapsed_time\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")      \n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\", \"elapsed_time\"],\n",
    "                            aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                            cat_col=\"event_name+fqid\")\n",
    "  \n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self):\n",
    "        self.store = {}\n",
    "        for c in features_used_total:\n",
    "            self.store[c] = defaultdict(int)\n",
    "\n",
    "    def record(self, train):\n",
    "        df = train.drop_duplicates(\"session_id\").set_index(\"session_id\")[features_used_total]\n",
    "        for session in df.index:\n",
    "            for c in features_used_total:\n",
    "                self.store[c][session] += df.at[session, c]\n",
    "\n",
    "    def add_total_features(self, train):\n",
    "        for c in features_used_total:\n",
    "            train[f\"total_{c}\"] = train[\"session_id\"].map(self.store[c])\n",
    "        return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels, hist):\n",
    "    # label„Éá„Éº„Çø„ÅÆÊï¥ÂΩ¢\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # ÁâπÂæ¥ÈáèÁîüÊàê\n",
    "    feat = FeaturesTrain(sessions, labels)\n",
    "    train = feat.get_train()\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    # level_group„ÅÆÁâπÂæ¥ÈáèË®òÈå≤ÔºÜÈÅéÂéª„ÅÆgroupÂê´„ÇÅ„ÅütotalÂÄ§„ÅÆÁâπÂæ¥ÈáèÂèñÂæó\n",
    "    hist.record(train)\n",
    "    train = hist.add_total_features(train)\n",
    "    return train, hist\n",
    "\n",
    "def get_test_dataset(sessions, labels, hist):\n",
    "    # label„Éá„Éº„Çø„ÅÆÊï¥ÂΩ¢\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # ÁâπÂæ¥ÈáèÁîüÊàê\n",
    "    feat = FeaturesInf(sessions, labels)\n",
    "    test = feat.get_test()\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    # level_group„ÅÆÁâπÂæ¥ÈáèË®òÈå≤ÔºÜÈÅéÂéª„ÅÆgroupÂê´„ÇÅ„ÅütotalÂÄ§„ÅÆÁâπÂæ¥ÈáèÂèñÂæó\n",
    "    hist.record(test)\n",
    "    test = hist.add_total_features(test)\n",
    "    return test, hist   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # QÂà•„Çπ„Ç≥„Ç¢\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    with open(cfg.prep_dir + 'iter_train.pkl', 'rb') as f:\n",
    "        iter_train = pickle.load(f) \n",
    "    \n",
    "    dfs = []\n",
    "    hist = History()\n",
    "    for group in level_group_list:\n",
    "        # „Éá„Éº„ÇøË™≠„ÅøËæº„Åø\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train_group, hist = get_train_dataset(train_sessions, labels, hist)\n",
    "        dfs.append(train_group)\n",
    "    train = pd.concat(dfs, ignore_index=True)\n",
    "    # concat„Åô„Çã„Å®categoryÂûã„Åå„É™„Çª„ÉÉ„Éà„Åï„Çå„Å¶„Åó„Åæ„ÅÜ„ÅÆ„ÅßÂÜçÂ∫¶cast\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    target = \"correct\"\n",
    "    not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "    gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "    fis = []\n",
    "    oofs = []\n",
    "    for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "        print(f\"fold : {i}\")\n",
    "        tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "        vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=2000000, early_stopping_rounds=100, verbose_eval=100)\n",
    "        # „É¢„Éá„É´Âá∫Âäõ\n",
    "        model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model{i}.lgb\")\n",
    "        \n",
    "        # valid_pred\n",
    "        oof_fold = train.iloc[vl_idx].copy()\n",
    "        oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oofs.append(oof_fold)\n",
    "\n",
    "        # ÁâπÂæ¥ÈáèÈáçË¶ÅÂ∫¶\n",
    "        fi_fold = pd.DataFrame()\n",
    "        fi_fold[\"feature\"] = model.feature_name()\n",
    "        fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "        fi_fold[\"fold\"] = i\n",
    "        fis.append(fi_fold)\n",
    "\n",
    "    fi = pd.concat(fis)    \n",
    "    fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "    fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi.csv\", index=False)\n",
    "    fi_n = fi['feature'].nunique()\n",
    "    order = list(fi.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\n",
    "    #plt.figure(figsize=(10, fi_n*0.2))\n",
    "    #sns.barplot(x=\"importance\", y=\"feature\", data=fi, order=order)\n",
    "    #plt.title(f\"LGBM importance\")\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(cfg.output_dir + f'{cfg.exp_name}/lgbm_importance.png')\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"train„Éá„Éº„Çø„ÅÆiterÂàÜÂâ≤„ÇíÈÅ©Áî®„Åó„Åütest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"session\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby([\"session_id\", \"level_group\"])]\n",
    "    subs = [df[1].drop(columns=[\"session\", \"session_level\"]).reset_index(drop=True) for df in sub.groupby([\"session\", \"level_group\"])]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"test„Éá„Éº„Çø„ÅÆiterÂàÜÂâ≤„ÇíÈÅ©Áî®„Åó„Åütest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_train_test_process_identity():\n",
    "    \"\"\"train„Å®test„ÅÆ„Éá„Éº„ÇøÂä†Â∑•„ÅÆÁµêÊûú„Åå‰∏ÄËá¥„Åô„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç„Åô„Çã\n",
    "    \"\"\"\n",
    "    iter_train = get_mock_iter_train()\n",
    "    iter_test = get_mock_iter_test()\n",
    "\n",
    "    dfs = []\n",
    "    hist = History()\n",
    "    for (sessions, sub) in iter_train:\n",
    "        df_iter, hist = get_train_dataset(sessions, sub, hist)\n",
    "        dfs.append(df_iter)\n",
    "    train_process_df = pd.concat(dfs, ignore_index=True)\n",
    "    # concat„Åô„Çã„Å®categoryÂûã„Åå„É™„Çª„ÉÉ„Éà„Åï„Çå„Å¶„Åó„Åæ„ÅÜ„ÅÆ„ÅßÂÜçÂ∫¶cast\n",
    "    train_process_df[\"question\"] = train_process_df[\"question\"].astype(\"category\")\n",
    "    train_process_df = train_process_df.drop(columns=\"level_group\")\n",
    "\n",
    "    dfs = []\n",
    "    hist = History()\n",
    "    for (sessions, sub) in iter_test:\n",
    "        df_iter, hist = get_test_dataset(sessions, sub, hist)\n",
    "        dfs.append(df_iter)\n",
    "    test_process_df = pd.concat(dfs, ignore_index=True)\n",
    "    # concat„Åô„Çã„Å®categoryÂûã„Åå„É™„Çª„ÉÉ„Éà„Åï„Çå„Å¶„Åó„Åæ„ÅÜ„ÅÆ„ÅßÂÜçÂ∫¶cast\n",
    "    test_process_df[\"question\"] = train_process_df[\"question\"].astype(\"category\")\n",
    "\n",
    "    train_process_df = train_process_df.sort_values([\"session_id\", \"question\"]).reset_index(drop=True)\n",
    "    test_process_df = test_process_df.sort_values([\"session_id\", \"question\"]).reset_index(drop=True)\n",
    "    train_process_df = train_process_df[sorted(train_process_df.columns.tolist())]\n",
    "    test_process_df = test_process_df[sorted(test_process_df.columns.tolist())]\n",
    "\n",
    "    assert train_process_df.equals(test_process_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series api„ÇíÊ®°„Åó„Åüiter„Çí„É¢„ÉÉ„ÇØ„Å®„Åó„Å¶Áî®ÊÑè„Åô„Çã\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    models = []\n",
    "    for i in range(cfg.n_splits):\n",
    "        if mode == \"local_cv\":\n",
    "            model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model{i}.lgb\"\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_path = f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model{i}.lgb\"\n",
    "        models.append(lgb.Booster(model_file=model_path))\n",
    "    features = models[0].feature_name()\n",
    "    \n",
    "    hist = History()\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        test, hist = get_test_dataset(test_sessions, sample_submission, hist)\n",
    "\n",
    "        preds = np.zeros(len(test))\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = models[i]\n",
    "            preds += model.predict(test[features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_infÂá¶ÁêÜÊôÇÈñì : \", process_time, \"Áßí\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239789, number of negative: 99493\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 5.120784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945892\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.706754 -> initscore=0.879672\n",
      "[LightGBM] [Info] Start training from score 0.879672\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.45176\tvalid_1's binary_logloss: 0.486141\n",
      "[200]\ttraining's binary_logloss: 0.427372\tvalid_1's binary_logloss: 0.485816\n",
      "Early stopping, best iteration is:\n",
      "[197]\ttraining's binary_logloss: 0.427928\tvalid_1's binary_logloss: 0.485736\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239066, number of negative: 100216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.994769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945596\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.704623 -> initscore=0.869412\n",
      "[LightGBM] [Info] Start training from score 0.869412\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.452812\tvalid_1's binary_logloss: 0.482787\n",
      "[200]\ttraining's binary_logloss: 0.428553\tvalid_1's binary_logloss: 0.482588\n",
      "Early stopping, best iteration is:\n",
      "[134]\ttraining's binary_logloss: 0.443144\tvalid_1's binary_logloss: 0.482116\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239010, number of negative: 100290\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.676755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945548\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.704421 -> initscore=0.868439\n",
      "[LightGBM] [Info] Start training from score 0.868439\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.453207\tvalid_1's binary_logloss: 0.482232\n",
      "[200]\ttraining's binary_logloss: 0.428834\tvalid_1's binary_logloss: 0.481827\n",
      "Early stopping, best iteration is:\n",
      "[163]\ttraining's binary_logloss: 0.436861\tvalid_1's binary_logloss: 0.481639\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239535, number of negative: 99765\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.681859 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945358\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.705968 -> initscore=0.875882\n",
      "[LightGBM] [Info] Start training from score 0.875882\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.452116\tvalid_1's binary_logloss: 0.485631\n",
      "[200]\ttraining's binary_logloss: 0.427988\tvalid_1's binary_logloss: 0.485253\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttraining's binary_logloss: 0.444545\tvalid_1's binary_logloss: 0.485046\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239684, number of negative: 99616\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.324192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945586\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.706407 -> initscore=0.877999\n",
      "[LightGBM] [Info] Start training from score 0.877999\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.452274\tvalid_1's binary_logloss: 0.48457\n",
      "[200]\ttraining's binary_logloss: 0.428112\tvalid_1's binary_logloss: 0.483736\n",
      "Early stopping, best iteration is:\n",
      "[160]\ttraining's binary_logloss: 0.436469\tvalid_1's binary_logloss: 0.483564\n",
      "logloss 0.483620\n",
      "best_score 0.694668\n",
      "best_threshold 0.620\n",
      "------------------------------\n",
      "Q1 : F1 = 0.609847\n",
      "Q2 : F1 = 0.526448\n",
      "Q3 : F1 = 0.583366\n",
      "Q4 : F1 = 0.644666\n",
      "Q5 : F1 = 0.383305\n",
      "Q6 : F1 = 0.613726\n",
      "Q7 : F1 = 0.564724\n",
      "Q8 : F1 = 0.331920\n",
      "Q9 : F1 = 0.563067\n",
      "Q10 : F1 = 0.348833\n",
      "Q11 : F1 = 0.384744\n",
      "Q12 : F1 = 0.588062\n",
      "Q13 : F1 = 0.420419\n",
      "Q14 : F1 = 0.499788\n",
      "Q15 : F1 = 0.358736\n",
      "Q16 : F1 = 0.411673\n",
      "Q17 : F1 = 0.354374\n",
      "Q18 : F1 = 0.545729\n"
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    #valid_train_test_process_identity()\n",
    "    run_train()\n",
    "#inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
