{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp101"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shift + , shift - 両方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp101-dart\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "    base_exp = \"exp101\" # 特徴量重要度を使う元のexp\n",
    "    n_features = 9999 # 特徴量削減の数\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "    import cudf\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'dart', \n",
    "    'learning_rate': 0.01, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_seqs = {}\n",
    "ideal_seqs[\"0-4\"] = \"a b c d c f g\"\n",
    "ideal_seqs[\"5-12\"] = \"g c b h b c e i e c j k l m l c e i e c n\"\n",
    "ideal_seqs[\"13-22\"] = \"n c b o b c e i e c b o b c p c q r l m l c e i e c s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = {}\n",
    "n_features[\"0-4\"] = 600\n",
    "n_features[\"5-12\"] = 1000\n",
    "n_features[\"13-22\"] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    with open(cfg.prep_dir + 'cat_col_lists_v3.pkl', 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) \n",
    "    with open(cfg.prep_dir + 'room_fqid_encoder.pkl', 'rb') as f:\n",
    "        room_fqid_encoder = pickle.load(f) \n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    with open(\"/kaggle/input/psp-cat-col-lists/cat_col_lists_v3.pkl\", 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) \n",
    "    with open(\"/kaggle/input/room-fqid-encoder/room_fqid_encoder.pkl\", 'rb') as f:\n",
    "        room_fqid_encoder = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_maxmin(s):\n",
    "    try:\n",
    "        return s.max() - s.min()\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features:\n",
    "    def __init__(self, sessions_df, need_create_features=None):\n",
    "        self.sessions_df = pl.from_pandas(sessions_df).sort([\"session_id\", \"index\"])\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.need_create_features = need_create_features\n",
    "\n",
    "    def prep(self):\n",
    "        self.sessions_df = self.sessions_df.with_columns(\n",
    "            [(pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(1)).clip(0, 1e9).fill_null(0).over([\"session_id\"]).alias(\"time_diff\"),\n",
    "             (pl.col(\"elapsed_time\").shift(-1)-pl.col(\"elapsed_time\")).clip(0, 1e9).fill_null(0).over([\"session_id\"]).alias(\"time_diff2\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"name\")).alias(\"event_name+name\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"room_fqid\")).alias(\"event_name+room_fqid\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"fqid\")).alias(\"event_name+fqid\"),\n",
    "             (pl.col(\"level\").cast(pl.Utf8) + \"_\" + pl.col(\"room_fqid\")).alias(\"level+room_fqid\"),\n",
    "             (pl.col(\"room_fqid\").map_dict(room_fqid_encoder).alias(\"room_fqid_encode\"))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "    def get_aggs(self):\n",
    "        g = self.group\n",
    "        cats = cat_col_lists[g]\n",
    "        aggs = []\n",
    "\n",
    "        # トータルレコード数\n",
    "        aggs += [pl.col(\"index\").count().alias(f\"{g}_record_cnt\")]\n",
    "\n",
    "        # グループ全体の経過時間\n",
    "        aggs += [pl.col(\"elapsed_time\").apply(lambda s:s.max() - s.min()).alias(f\"{g}_elapsed_time\")]\n",
    "\n",
    "        # 各categoryごとのレコード数\n",
    "        for c in [\"event_name\", \"name\", \"page\", \"level\", \"room_fqid\", \"fqid\", \"event_name+name\", \"event_name+room_fqid\", \"event_name+fqid\", \"level+room_fqid\"]:\n",
    "            aggs += [pl.col(\"index\").filter(pl.col(c)==v).count().fill_null(0).alias(f\"{g}_{c}_{str(v)}_record_cnt\") for v in cats[c]]\n",
    "        \n",
    "        # 各categoryごとのユニーク数\n",
    "        for c in [\"event_name\", \"name\", \"page\", \"level\", \"room_fqid\", \"fqid\", \"event_name+name\", \"event_name+room_fqid\", \"event_name+fqid\", \"level+room_fqid\"]:\n",
    "            aggs += [pl.col(c).drop_nulls().n_unique().fill_null(0).alias(f\"{g}_{c}_nunique\")]\n",
    "\n",
    "        # 各level - categoryごとのユニーク数\n",
    "        for c in [\"event_name\", \"name\", \"page\", \"level\", \"room_fqid\", \"fqid\", \"event_name+name\", \"event_name+room_fqid\", \"event_name+fqid\", \"level+room_fqid\"]:\n",
    "            aggs += [pl.col(c).filter(pl.col(\"level\")==l).drop_nulls().n_unique().fill_null(0).alias(f\"{g}_leve{l}_{c}_nunique\") for l in cats[\"level\"]]\n",
    "\n",
    "        # 集計量\n",
    "        for v in [\"elapsed_time\", \"index\"]:\n",
    "            aggs += [pl.col(v).max().fill_null(-1).alias(f\"{g}_{v}_max\").cast(pl.Float32), \n",
    "                     pl.col(v).max().fill_null(-1).alias(f\"{g}_{v}_min\").cast(pl.Float32)]\n",
    "\n",
    "        for v in [\"time_diff\", \"time_diff2\", \"hover_duration\"]:\n",
    "            aggs += [pl.col(v).max().fill_null(-1).alias(f\"{g}_{v}_max\").cast(pl.Float32), \n",
    "                     pl.col(v).min().fill_null(-1).alias(f\"{g}_{v}_min\").cast(pl.Float32), \n",
    "                     pl.col(v).std().fill_null(-1).alias(f\"{g}_{v}_std\").cast(pl.Float32),\n",
    "                     pl.col(v).mean().fill_null(-1).alias(f\"{g}_{v}_mean\").cast(pl.Float32), \n",
    "                     pl.col(v).sum().fill_null(-1).alias(f\"{g}_{v}_sum\").cast(pl.Float32), \n",
    "                     pl.col(v).median().fill_null(-1).alias(f\"{g}_{v}_median\").cast(pl.Float32)]\n",
    "            \n",
    "            aggs += [pl.col(v).quantile(0.25, \"nearest\").fill_null(-1).alias(f\"{g}_{v}_quantile025\"),\n",
    "                     pl.col(v).quantile(0.75, \"nearest\").fill_null(-1).alias(f\"{g}_{v}_quantile075\")\n",
    "            ]\n",
    "            \n",
    "        # カテゴリ×集計量\n",
    "        cs = [\"event_name\", \"room_fqid\", \"fqid\", \"text_fqid\", \"level\", \"name\", \"event_name+name\", \"event_name+room_fqid\", \"level+room_fqid\"]\n",
    "        vs = [\"time_diff\", \"time_diff2\"]\n",
    "        for c, v in itertools.product(cs, vs):\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).max().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_max\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).min().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_min\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).std().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_std\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).mean().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_mean\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).median().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_median\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).sum().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_sum\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).quantile(0.25, \"nearest\").fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_quantile025\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).quantile(0.75, \"nearest\").fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_quantile075\").cast(pl.Float32) for cat in cats[c]]\n",
    "\n",
    "        cs = [\"event_name\", \"room_fqid\", \"fqid\", \"text_fqid\", \"level\", \"name\", \"event_name+name\", \"event_name+room_fqid\", \"level+room_fqid\"]\n",
    "        vs = [\"elapsed_time\", \"index\"]\n",
    "        for c, v in itertools.product(cs, vs):\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).max().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_max\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).min().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_min\").cast(pl.Float32) for cat in cats[c]]\n",
    "\n",
    "        # カテゴリの変化回数\n",
    "        for c in [\"room_fqid\", \"text_fqid\"]:\n",
    "            aggs += [(pl.col(c) != pl.col(c).shift(1)).sum().alias(f\"{g}_{c}_change_cnt\")]\n",
    "\n",
    "        # levelごとのカテゴリ変化回数\n",
    "        for c in [\"room_fqid\", \"text_fqid\"]:\n",
    "            aggs += [pl.col(c).filter((pl.col(\"level\")==l)&(pl.col(c) != pl.col(c).shift(1))).count().alias(f\"{g}_level{l}_{c}_change_cnt\") for l in cats[\"level\"]]\n",
    "\n",
    "        # object_hoverのduration関連の特徴量（各fqidごと）\n",
    "        fqids = [c.removeprefix(\"object_hover_\") for c in cats[\"event_name+fqid\"] if \"object_hover\" in c]\n",
    "        for fqid in fqids:\n",
    "            aggs += [pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).max().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_max\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).min().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_min\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_std\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_mean\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).median().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_median\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).sum().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_sum\")\n",
    "                    ]\n",
    "\n",
    "        # miniゲームの所要時間\n",
    "        if g == \"0-4\":\n",
    "            for fqid_start, fqid_end in zip([\"tunic\", \"plaque\"],[\"tunic.hub.slip\", \"plaque.face.date\"]):\n",
    "                aggs += [\n",
    "                    pl.col(\"index\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_index_cnt\"),\n",
    "                    pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_duration\")\n",
    "                ]\n",
    "        elif g == \"5-12\":\n",
    "            for fqid_start, fqid_end in zip([\"businesscards\", \"logbook\", \"reader\", \"journals\"],[\"businesscards.card_bingo.bingo\", \"logbook.page.bingo\", \"reader.paper2.bingo\", \"journals.pic_2.bingo\"]):\n",
    "                aggs += [\n",
    "                    pl.col(\"index\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_index_cnt\"),\n",
    "                    pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_duration\")\n",
    "                ]\n",
    "        elif g == \"13-22\":\n",
    "            for fqid_start, fqid_end in zip([\"tracks\", \"reader_flag\", \"journals_flag\", \"directory\"],[\"tracks.hub.deer\", \"reader_flag.paper2.bingo\", \"journals_flag.pic_0.bingo\", \"directory.closeup.archivist\"]):\n",
    "                aggs += [\n",
    "                    pl.col(\"index\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_index_cnt\"),\n",
    "                    pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_duration\")\n",
    "                ]\n",
    "\n",
    "        # miniゲーム中のクリック座標\n",
    "        if g == \"0-4\":\n",
    "            fqids = [\"tunic\", \"plaque\"]\n",
    "        elif g == \"5-12\":\n",
    "            fqids = [\"businesscards\", \"logbook\", \"reader\", \"journals\"]\n",
    "        elif g == \"13-22\":\n",
    "            fqids = [\"tracks\", \"reader_flag\", \"journals_flag\", \"directory\"]\n",
    "\n",
    "        for fqid in fqids:\n",
    "            aggs += [\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).first().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_first_click_room_coor_x\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).first().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_first_click_room_coor_y\"),\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).last().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_last_click_room_coor_x\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).last().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_last_click_room_coor_y\"),\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_x_mean\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_y_mean\"),\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_x_std\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_y_std\"),\n",
    "            ]\n",
    "        \n",
    "        # 理想系列からのLevenshtein距離\n",
    "        aggs += [pl.col(\"room_fqid_encode\").filter(pl.col(\"room_fqid_encode\")!=pl.col(\"room_fqid_encode\").shift(1)).str.concat(\" \").apply(lambda x: Levenshtein.distance(x, ideal_seqs[g])).alias(f\"{g}_room_fqid_leven_dist\")]\n",
    "\n",
    "        # 生成する特徴量を限定\n",
    "        if self.need_create_features is not None:\n",
    "            feats = [re.findall(r'alias\\(\"(.*)\"\\)', str(a))[0] for a in aggs]\n",
    "            aggs = [aggs[i] for i, f in enumerate(feats) if f in self.need_create_features]\n",
    "\n",
    "        return aggs\n",
    "\n",
    "    def get_features(self):\n",
    "        self.prep()\n",
    "        aggs = self.get_aggs()\n",
    "        features = self.sessions_df.groupby([\"session_id\"], maintain_order=True).agg(aggs)\n",
    "        return features.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    features = Features(sessions).get_features()\n",
    "    train = labels.merge(features, on=[\"session_id\"], how=\"left\")\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels, need_create_features=None):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    features = Features(sessions, need_create_features).get_features()\n",
    "    test = labels.merge(features, on=[\"session_id\"], how=\"left\")\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesSelect:\n",
    "    def __init__(self, df, init_features, corr_th=0.99):\n",
    "        self.init_features = init_features\n",
    "        self.df = cudf.from_pandas(df)\n",
    "        self.corr_th = corr_th\n",
    "        self.drop_cols = []\n",
    "    \n",
    "    def _high_corr_features_drop(self):\n",
    "        num_cols = self.df[self.init_features].select_dtypes(include=\"number\").columns\n",
    "\n",
    "        # 特徴量間の相関行列を計算\n",
    "        corr_matrix = self.df[num_cols].fillna(-1).corr().abs().to_pandas()\n",
    "        # 相関行列の上三角行列を取得します。（相関行列が対称であるため、重複する相関を取り除くため）\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        drop_cols = []\n",
    "        for c in num_cols:\n",
    "            if any(upper[c] > self.corr_th):\n",
    "                drop_cols.append(c)\n",
    "                upper = upper.drop(index=c)\n",
    "        print(f\"特徴量間の相関性が高い特徴量を{str(len(drop_cols))}個抽出\")\n",
    "        self.df = self.df.drop(columns=drop_cols)\n",
    "        self.drop_cols = list(set(self.drop_cols + drop_cols))\n",
    "\n",
    "    def features_select(self):\n",
    "        self._high_corr_features_drop()\n",
    "        selected_features = list(set(self.init_features) - set(self.drop_cols))\n",
    "        print(f\"{str(len(self.init_features))} -> {str(len(selected_features))}\")\n",
    "\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_importance_feature_select(df, features, n):\n",
    "    null_imp_params = {\n",
    "        'objective': 'binary', \n",
    "        'boosting': 'gbdt', \n",
    "        'learning_rate': 0.1, \n",
    "        'metric': 'binary_logloss', \n",
    "        'seed': cfg.seed, \n",
    "        'feature_pre_filter': False, \n",
    "        'lambda_l1': 4.134488140102331, \n",
    "        'lambda_l2': 0.007775200046481757, \n",
    "        'num_leaves': 75, \n",
    "        'feature_fraction': 0.5, \n",
    "        'bagging_fraction': 0.7036110805680353, \n",
    "        'bagging_freq': 3, \n",
    "        'min_data_in_leaf': 50, \n",
    "        'min_child_samples': 100\n",
    "        }\n",
    "    \n",
    "    model = lgb.train(null_imp_params, lgb.Dataset(df[features], label=df[\"correct\"]), num_boost_round=100, verbose_eval=0)\n",
    "    fi_org = pd.DataFrame()\n",
    "    fi_org[\"feature\"] = model.feature_name()\n",
    "    fi_org[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "    fi_org = fi_org.sort_values(\"feature\", ignore_index=True)\n",
    "\n",
    "    null_imps = []\n",
    "    for i in range(10):\n",
    "        model = lgb.train(null_imp_params, lgb.Dataset(df[features], label=df[\"correct\"].sample(frac=1, random_state=i).values), num_boost_round=100, verbose_eval=0)\n",
    "        fi_tmp = pd.DataFrame()\n",
    "        fi_tmp[\"feature\"] = model.feature_name()\n",
    "        fi_tmp[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "        null_imps.append(fi_tmp)\n",
    "    null_imp = pd.concat(null_imps)\n",
    "    null_imp_mean = null_imp.groupby(\"feature\")[\"importance\"].quantile(0.8).reset_index().sort_values(\"feature\", ignore_index=True)\n",
    "    fi_org[\"null_imp\"] = null_imp_mean[\"importance\"]\n",
    "    fi_org[\"gain\"] = np.log(1e-10 + fi_org[\"importance\"] / (1 + fi_org[\"null_imp\"]))\n",
    "    selected_features = fi_org.sort_values(\"gain\", ascending=False).head(n)[\"feature\"].tolist()\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}_cleaned.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "\n",
    "            train[\"5-12_0-4_pred_max_diff\"] = train[\"5-12_pred_max\"] - train[\"0-4_pred_max\"]\n",
    "            train[\"5-12_0-4_pred_min_diff\"] = train[\"5-12_pred_min\"] - train[\"0-4_pred_min\"]\n",
    "            train[\"5-12_0-4_pred_mean_diff\"] = train[\"5-12_pred_mean\"] - train[\"0-4_pred_mean\"]\n",
    "            train[\"5-12_0-4_pred_std_diff\"] = train[\"5-12_pred_std\"] - train[\"0-4_pred_std\"]\n",
    "            train[\"5-12_0-4_pred_maxmin_diff\"] = train[\"5-12_pred_maxmin\"] - train[\"0-4_pred_maxmin\"]\n",
    "\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "        # 特徴量選択\n",
    "        if cfg.base_exp is None:\n",
    "            # features = FeaturesSelect(train, features).features_select()\n",
    "            features = null_importance_feature_select(train, features, n=n_features[group])\n",
    "        else:\n",
    "            # 使用する特徴量の抽出\n",
    "            features = pd.read_csv(cfg.output_dir + f\"{cfg.base_exp}/fi_{group}.csv\").head(cfg.n_features)[\"feature\"].tolist()\n",
    "\n",
    "        gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "        fis = []\n",
    "\n",
    "        if group == \"0-4\":\n",
    "            n_rounds = 20000\n",
    "        elif group == \"5-12\":\n",
    "            n_rounds = 20000\n",
    "        elif group == \"13-22\":\n",
    "            n_rounds = 20000\n",
    "        \n",
    "        oof_groups = []\n",
    "        for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "            model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\"\n",
    "            \n",
    "            print(f\"fold : {i}\")\n",
    "            tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "            vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "            tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "            if os.path.exists(model_path):\n",
    "                print(f\"modelが既に存在するのでロード : {model_path}\")\n",
    "                model = lgb.Booster(model_file=model_path)\n",
    "            else:\n",
    "                model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                                num_boost_round=n_rounds, verbose_eval=100)\n",
    "            # モデル出力\n",
    "            model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\")\n",
    "        \n",
    "            # valid_pred\n",
    "            oof_fold = train.iloc[vl_idx].copy()\n",
    "            oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "            oof_groups.append(oof_fold)\n",
    "\n",
    "            # 特徴量重要度\n",
    "            fi_fold = pd.DataFrame()\n",
    "            fi_fold[\"feature\"] = model.feature_name()\n",
    "            fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "            fi_fold[\"fold\"] = i\n",
    "            fis.append(fi_fold)\n",
    "\n",
    "        fi = pd.concat(fis)    \n",
    "        fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "        fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "        fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi_{group}.csv\", index=False)\n",
    "\n",
    "        oof_group = pd.concat(oof_groups)\n",
    "        oofs.append(oof_group)\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\", \"level_group\"])\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = oof_group.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{group}_pred_mean\", \"max\":f\"{group}_pred_max\", \"min\":f\"{group}_pred_min\", \"std\":f\"{group}_pred_std\"})\n",
    "        meta_df[f\"{group}_pred_maxmin\"] = meta_df[f\"{group}_pred_max\"] - meta_df[f\"{group}_pred_min\"]\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"level_group2\"] = test[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"level_group2\"] = sub[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in test.groupby(\"level_group2\")]\n",
    "    subs = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in sub.groupby(\"level_group2\")]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    model_dict = {}\n",
    "    features_dict = {}\n",
    "    for g in level_group_list:\n",
    "        if mode == \"local_cv\":\n",
    "            model_paths = [cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_paths = [f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        model_dict[g] = [lgb.Booster(model_file=p) for p in model_paths]\n",
    "        features_dict[g] = model_dict[g][0].feature_name()\n",
    "    need_create_features = features_dict[\"0-4\"] + features_dict[\"5-12\"] + features_dict[\"13-22\"]\n",
    "    not_drop_cols = [\"0-4_elapsed_time_max\", \"0-4_index_max\", \"5-12_elapsed_time_max\", \"5-12_index_max\", \"13-22_elapsed_time_max\", \"13-22_index_max\",\n",
    "                     \"0-4_elapsed_time_min\", \"0-4_index_min\", \"5-12_elapsed_time_min\", \"5-12_index_min\", \"13-22_elapsed_time_min\", \"13-22_index_min\"]\n",
    "    need_create_features = need_create_features + not_drop_cols\n",
    "    need_create_features = list(set(need_create_features))\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission, need_create_features=need_create_features)\n",
    "        features = features_dict[level_group]\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "\n",
    "            test[\"5-12_0-4_pred_max_diff\"] = test[\"5-12_pred_max\"] - test[\"0-4_pred_max\"]\n",
    "            test[\"5-12_0-4_pred_min_diff\"] = test[\"5-12_pred_min\"] - test[\"0-4_pred_min\"]\n",
    "            test[\"5-12_0-4_pred_mean_diff\"] = test[\"5-12_pred_mean\"] - test[\"0-4_pred_mean\"]\n",
    "            test[\"5-12_0-4_pred_std_diff\"] = test[\"5-12_pred_std\"] - test[\"0-4_pred_std\"]\n",
    "            test[\"5-12_0-4_pred_maxmin_diff\"] = test[\"5-12_pred_maxmin\"] - test[\"0-4_pred_maxmin\"]\n",
    "\n",
    "        prev_features_df = test.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\"])\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = model_dict[level_group][i]\n",
    "            preds += model.predict(test[features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        test[\"pred\"] = preds\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = test.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{level_group}_pred_mean\", \"max\":f\"{level_group}_pred_max\", \"min\":f\"{level_group}_pred_min\", \"std\":f\"{level_group}_pred_std\"})\n",
    "        meta_df[f\"{level_group}_pred_maxmin\"] = meta_df[f\"{level_group}_pred_max\"] - meta_df[f\"{level_group}_pred_min\"]\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_train_test_process_identity():\n",
    "    iter_train = get_mock_iter_train()\n",
    "    iter_test = get_mock_iter_test()\n",
    "\n",
    "    print(\"train_iter\")\n",
    "    train_df_dict = {}\n",
    "    train_features_dict = {}\n",
    "    prev_features_df = None\n",
    "    for (sessions, sub) in iter_train:\n",
    "        group = sessions[\"level_group\"].values[0]\n",
    "        print(group)\n",
    "        train = get_train_dataset(sessions, sub)\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "            # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "        train_df_dict[group] = train[[\"session_id\"]+features].sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "        prev_features_df = train[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        train_features_dict[group] = features\n",
    "\n",
    "\n",
    "    print(\"test_iter\")\n",
    "    test_dfs_0_4 = []\n",
    "    test_dfs_5_12 = []\n",
    "    test_dfs_13_22 = []\n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        session_id = test_sessions[\"session_id\"].values[0]\n",
    "        print(session_id, level_group)\n",
    "        features = train_features_dict[level_group]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in test.columns if c not in not_use_cols]\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        if level_group == \"0-4\":\n",
    "            test_dfs_0_4.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"5-12\":\n",
    "            test_dfs_5_12.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"13-22\":\n",
    "            test_dfs_13_22.append(test[[\"session_id\"]+features])\n",
    "        \n",
    "\n",
    "    test_dfs_0_4 = pd.concat(test_dfs_0_4, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_5_12 = pd.concat(test_dfs_5_12, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_13_22 = pd.concat(test_dfs_13_22, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "\n",
    "    assert train_df_dict[\"0-4\"][train_features_dict[\"0-4\"]].equals(test_dfs_0_4[train_features_dict[\"0-4\"]])\n",
    "    assert train_df_dict[\"5-12\"][train_features_dict[\"5-12\"]].equals(test_dfs_5_12[train_features_dict[\"5-12\"]])\n",
    "    assert train_df_dict[\"13-22\"][train_features_dict[\"13-22\"]].equals(test_dfs_13_22[train_features_dict[\"13-22\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iter\n",
      "0-4\n",
      "5-12\n",
      "13-22\n",
      "test_iter\n",
      "20090109393214576 0-4\n",
      "20090109393214576 5-12\n",
      "20090109393214576 13-22\n",
      "20090312143683264 0-4\n",
      "20090312143683264 5-12\n",
      "20090312143683264 13-22\n",
      "20090312331414616 0-4\n",
      "20090312331414616 5-12\n",
      "20090312331414616 13-22\n",
      "0-4\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49821, number of negative: 6726\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 143883\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 600\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.881055 -> initscore=2.002456\n",
      "[LightGBM] [Info] Start training from score 2.002456\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\ttraining's binary_logloss: 0.398547\tvalid_1's binary_logloss: 0.407024\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[200]\ttraining's binary_logloss: 0.414204\tvalid_1's binary_logloss: 0.422742\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[300]\ttraining's binary_logloss: 0.377413\tvalid_1's binary_logloss: 0.388522\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[400]\ttraining's binary_logloss: 0.361386\tvalid_1's binary_logloss: 0.374094\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[500]\ttraining's binary_logloss: 0.339162\tvalid_1's binary_logloss: 0.354171\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[600]\ttraining's binary_logloss: 0.325736\tvalid_1's binary_logloss: 0.342381\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[700]\ttraining's binary_logloss: 0.301398\tvalid_1's binary_logloss: 0.321556\n",
      "[800]\ttraining's binary_logloss: 0.283678\tvalid_1's binary_logloss: 0.307438\n",
      "[900]\ttraining's binary_logloss: 0.26945\tvalid_1's binary_logloss: 0.296838\n",
      "[1000]\ttraining's binary_logloss: 0.26197\tvalid_1's binary_logloss: 0.29159\n",
      "[1100]\ttraining's binary_logloss: 0.251666\tvalid_1's binary_logloss: 0.285155\n",
      "[1200]\ttraining's binary_logloss: 0.247829\tvalid_1's binary_logloss: 0.283264\n",
      "[1300]\ttraining's binary_logloss: 0.243162\tvalid_1's binary_logloss: 0.280985\n",
      "[1400]\ttraining's binary_logloss: 0.237182\tvalid_1's binary_logloss: 0.278047\n",
      "[1500]\ttraining's binary_logloss: 0.23439\tvalid_1's binary_logloss: 0.277148\n",
      "[1600]\ttraining's binary_logloss: 0.226805\tvalid_1's binary_logloss: 0.273875\n",
      "[1700]\ttraining's binary_logloss: 0.222828\tvalid_1's binary_logloss: 0.272625\n",
      "[1800]\ttraining's binary_logloss: 0.220519\tvalid_1's binary_logloss: 0.272224\n",
      "[1900]\ttraining's binary_logloss: 0.216607\tvalid_1's binary_logloss: 0.271052\n",
      "[2000]\ttraining's binary_logloss: 0.211979\tvalid_1's binary_logloss: 0.269741\n",
      "[2100]\ttraining's binary_logloss: 0.209203\tvalid_1's binary_logloss: 0.269202\n",
      "[2200]\ttraining's binary_logloss: 0.206937\tvalid_1's binary_logloss: 0.26892\n",
      "[2300]\ttraining's binary_logloss: 0.203447\tvalid_1's binary_logloss: 0.268259\n",
      "[2400]\ttraining's binary_logloss: 0.201395\tvalid_1's binary_logloss: 0.268077\n",
      "[2500]\ttraining's binary_logloss: 0.199122\tvalid_1's binary_logloss: 0.267818\n",
      "[2600]\ttraining's binary_logloss: 0.196367\tvalid_1's binary_logloss: 0.267311\n",
      "[2700]\ttraining's binary_logloss: 0.193359\tvalid_1's binary_logloss: 0.266942\n",
      "[2800]\ttraining's binary_logloss: 0.19165\tvalid_1's binary_logloss: 0.266774\n",
      "[2900]\ttraining's binary_logloss: 0.18851\tvalid_1's binary_logloss: 0.266442\n",
      "[3000]\ttraining's binary_logloss: 0.185555\tvalid_1's binary_logloss: 0.266247\n",
      "[3100]\ttraining's binary_logloss: 0.183157\tvalid_1's binary_logloss: 0.266103\n",
      "[3200]\ttraining's binary_logloss: 0.180585\tvalid_1's binary_logloss: 0.265847\n",
      "[3300]\ttraining's binary_logloss: 0.178222\tvalid_1's binary_logloss: 0.265745\n",
      "[3400]\ttraining's binary_logloss: 0.175114\tvalid_1's binary_logloss: 0.265584\n",
      "[3500]\ttraining's binary_logloss: 0.172848\tvalid_1's binary_logloss: 0.26556\n",
      "[3600]\ttraining's binary_logloss: 0.170641\tvalid_1's binary_logloss: 0.265482\n",
      "[3700]\ttraining's binary_logloss: 0.167655\tvalid_1's binary_logloss: 0.265513\n",
      "[3800]\ttraining's binary_logloss: 0.165722\tvalid_1's binary_logloss: 0.265552\n",
      "[3900]\ttraining's binary_logloss: 0.164058\tvalid_1's binary_logloss: 0.265564\n",
      "[4000]\ttraining's binary_logloss: 0.162414\tvalid_1's binary_logloss: 0.265527\n",
      "[4100]\ttraining's binary_logloss: 0.160402\tvalid_1's binary_logloss: 0.265439\n",
      "[4200]\ttraining's binary_logloss: 0.158071\tvalid_1's binary_logloss: 0.265496\n",
      "[4300]\ttraining's binary_logloss: 0.155682\tvalid_1's binary_logloss: 0.265582\n",
      "[4400]\ttraining's binary_logloss: 0.154549\tvalid_1's binary_logloss: 0.265536\n",
      "[4500]\ttraining's binary_logloss: 0.152776\tvalid_1's binary_logloss: 0.265585\n",
      "[4600]\ttraining's binary_logloss: 0.150153\tvalid_1's binary_logloss: 0.265681\n",
      "[4700]\ttraining's binary_logloss: 0.148819\tvalid_1's binary_logloss: 0.265658\n",
      "[4800]\ttraining's binary_logloss: 0.147043\tvalid_1's binary_logloss: 0.26571\n",
      "[4900]\ttraining's binary_logloss: 0.144987\tvalid_1's binary_logloss: 0.265784\n",
      "[5000]\ttraining's binary_logloss: 0.14355\tvalid_1's binary_logloss: 0.265798\n",
      "[5100]\ttraining's binary_logloss: 0.142209\tvalid_1's binary_logloss: 0.265821\n",
      "[5200]\ttraining's binary_logloss: 0.140306\tvalid_1's binary_logloss: 0.266044\n",
      "[5300]\ttraining's binary_logloss: 0.139289\tvalid_1's binary_logloss: 0.266121\n",
      "[5400]\ttraining's binary_logloss: 0.137676\tvalid_1's binary_logloss: 0.266174\n",
      "[5500]\ttraining's binary_logloss: 0.135939\tvalid_1's binary_logloss: 0.266255\n",
      "[5600]\ttraining's binary_logloss: 0.134728\tvalid_1's binary_logloss: 0.266374\n",
      "[5700]\ttraining's binary_logloss: 0.133177\tvalid_1's binary_logloss: 0.266559\n",
      "[5800]\ttraining's binary_logloss: 0.13213\tvalid_1's binary_logloss: 0.266621\n",
      "[5900]\ttraining's binary_logloss: 0.130662\tvalid_1's binary_logloss: 0.266829\n",
      "[6000]\ttraining's binary_logloss: 0.129744\tvalid_1's binary_logloss: 0.266854\n",
      "[6100]\ttraining's binary_logloss: 0.128686\tvalid_1's binary_logloss: 0.266896\n",
      "[6200]\ttraining's binary_logloss: 0.127578\tvalid_1's binary_logloss: 0.267017\n",
      "[6300]\ttraining's binary_logloss: 0.126248\tvalid_1's binary_logloss: 0.267146\n",
      "[6400]\ttraining's binary_logloss: 0.125183\tvalid_1's binary_logloss: 0.267271\n",
      "[6500]\ttraining's binary_logloss: 0.123838\tvalid_1's binary_logloss: 0.267414\n",
      "[6600]\ttraining's binary_logloss: 0.122259\tvalid_1's binary_logloss: 0.26773\n",
      "[6700]\ttraining's binary_logloss: 0.120882\tvalid_1's binary_logloss: 0.267908\n",
      "[6800]\ttraining's binary_logloss: 0.119449\tvalid_1's binary_logloss: 0.267991\n",
      "[6900]\ttraining's binary_logloss: 0.118275\tvalid_1's binary_logloss: 0.268103\n",
      "[7000]\ttraining's binary_logloss: 0.117067\tvalid_1's binary_logloss: 0.268309\n",
      "[7100]\ttraining's binary_logloss: 0.115664\tvalid_1's binary_logloss: 0.268598\n",
      "[7200]\ttraining's binary_logloss: 0.114395\tvalid_1's binary_logloss: 0.268842\n",
      "[7300]\ttraining's binary_logloss: 0.113262\tvalid_1's binary_logloss: 0.268974\n",
      "[7400]\ttraining's binary_logloss: 0.112193\tvalid_1's binary_logloss: 0.269149\n",
      "[7500]\ttraining's binary_logloss: 0.110984\tvalid_1's binary_logloss: 0.269321\n",
      "[7600]\ttraining's binary_logloss: 0.110048\tvalid_1's binary_logloss: 0.269415\n",
      "[7700]\ttraining's binary_logloss: 0.109261\tvalid_1's binary_logloss: 0.269561\n",
      "[7800]\ttraining's binary_logloss: 0.108277\tvalid_1's binary_logloss: 0.269702\n",
      "[7900]\ttraining's binary_logloss: 0.10723\tvalid_1's binary_logloss: 0.269917\n",
      "[8000]\ttraining's binary_logloss: 0.105981\tvalid_1's binary_logloss: 0.270142\n",
      "[8100]\ttraining's binary_logloss: 0.105114\tvalid_1's binary_logloss: 0.270272\n",
      "[8200]\ttraining's binary_logloss: 0.104079\tvalid_1's binary_logloss: 0.270639\n",
      "[8300]\ttraining's binary_logloss: 0.103324\tvalid_1's binary_logloss: 0.270778\n",
      "[8400]\ttraining's binary_logloss: 0.102708\tvalid_1's binary_logloss: 0.270745\n",
      "[8500]\ttraining's binary_logloss: 0.102055\tvalid_1's binary_logloss: 0.270864\n",
      "[8600]\ttraining's binary_logloss: 0.101269\tvalid_1's binary_logloss: 0.271024\n",
      "[8700]\ttraining's binary_logloss: 0.100018\tvalid_1's binary_logloss: 0.271472\n",
      "[8800]\ttraining's binary_logloss: 0.0992679\tvalid_1's binary_logloss: 0.271595\n",
      "[8900]\ttraining's binary_logloss: 0.0985083\tvalid_1's binary_logloss: 0.271768\n",
      "[9000]\ttraining's binary_logloss: 0.0976599\tvalid_1's binary_logloss: 0.27192\n",
      "[9100]\ttraining's binary_logloss: 0.0971275\tvalid_1's binary_logloss: 0.271941\n",
      "[9200]\ttraining's binary_logloss: 0.0961339\tvalid_1's binary_logloss: 0.272257\n",
      "[9300]\ttraining's binary_logloss: 0.0954735\tvalid_1's binary_logloss: 0.272388\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlocal_cv\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     valid_train_test_process_identity()\n\u001b[0;32m----> 3\u001b[0m     run_train()\n\u001b[1;32m      4\u001b[0m inference(cfg\u001b[39m.\u001b[39mmode)\n",
      "Cell \u001b[0;32mIn[15], line 67\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mBooster(model_file\u001b[39m=\u001b[39mmodel_path)\n\u001b[1;32m     66\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(params, tr_data, valid_sets\u001b[39m=\u001b[39;49m[tr_data, vl_data],\n\u001b[1;32m     68\u001b[0m                     num_boost_round\u001b[39m=\u001b[39;49mn_rounds, verbose_eval\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m     69\u001b[0m \u001b[39m# モデル出力\u001b[39;00m\n\u001b[1;32m     70\u001b[0m model\u001b[39m.\u001b[39msave_model(cfg\u001b[39m.\u001b[39moutput_dir \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mexp_name\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mcfg\u001b[39m.\u001b[39mexp_name\u001b[39m}\u001b[39;00m\u001b[39m_model_\u001b[39m\u001b[39m{\u001b[39;00mgroup\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.lgb\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    valid_train_test_process_identity()\n",
    "    run_train()\n",
    "inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
