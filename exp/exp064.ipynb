{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp064"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各level_groupの始点・終点とlevel_group跨ぎの差分特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp064\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.01, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.prep_dir + 'cat_col_lists.pkl', 'rb') as f:\n",
    "    cat_col_lists = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTrain:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"session_id\", \"level_group\", \"elapsed_time\"], ignore_index=True)\n",
    "        self.features = self.sessions_df[[\"session_id\", \"level_group\"]].drop_duplicates().copy()\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].shift(1)\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_record_cnt\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].agg([max,min]).reset_index()\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[\"max\"] - add_features[\"min\"]\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[f\"{self.group}_group_elapsed_time\"].astype(np.float32)\n",
    "        add_features = add_features[[\"session_id\", \"level_group\", f\"{self.group}_group_elapsed_time\"]].copy()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[\"index\"].count().reset_index().rename(columns={\"index\":\"cnt\"})\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            tmp = add_features[add_features[cat_col]==cat][[\"session_id\", \"level_group\", \"cnt\"]].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp = tmp.rename(columns={\"cnt\": feat_name})\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[feat_name] = self.features[feat_name].fillna(0)\n",
    "            else:\n",
    "                self.features[feat_name] = 0\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.dropna(subset=[cat_col]).drop_duplicates([\"session_id\", \"level_group\", cat_col])\n",
    "        add_features = add_features.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_{cat_col}_nunique\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")        \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        new_cols = [f\"{self.group}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{self.group}_{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "    def get_train(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "        \n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        \n",
    "        self.result = self.result.merge(self.features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesInf:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"elapsed_time\"], ignore_index=True)\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.use_cols = [\n",
    "            \"elapsed_time\", \"event_name\", \"name\", \"level\", \"page\", \"index\",\n",
    "            \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\",\n",
    "            \"hover_duration\", \"text\", \"fqid\", \"room_fqid\", \"text_fqid\"\n",
    "        ]\n",
    "\n",
    "    def _prep(self):\n",
    "        # dataframeの各列をnumpy arrayで保持\n",
    "        self.sessions = {}\n",
    "        for c in self.use_cols:\n",
    "            self.sessions[c] = self.sessions_df[c].values\n",
    "        self.sessions[\"time_diff\"] = self.sessions[\"elapsed_time\"] - self.sessions_df[\"elapsed_time\"].shift(1).values\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_feature = len(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_record_cnt\"] = add_feature\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_feature = np.max(self.sessions[\"elapsed_time\"]) - np.min(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_group_elapsed_time\"] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            add_feature = (self.sessions[cat_col] == cat).astype(int).sum()\n",
    "            self.result[feat_name] = add_feature\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        self.result[f\"{self.group}_{cat_col}_nunique\"] = self.sessions_df[cat_col].dropna().nunique()       \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        for val_col, agg in itertools.product(val_cols, aggs):\n",
    "            feat_name = f\"{self.group}_{val_col}_{agg}\"\n",
    "            if agg == \"mean\":\n",
    "                add_feature = np.nanmean(self.sessions[val_col])\n",
    "            elif agg == \"max\":\n",
    "                add_feature = np.nanmax(self.sessions[val_col])\n",
    "            elif agg == \"min\":\n",
    "                add_feature = np.nanmin(self.sessions[val_col])\n",
    "            elif agg == \"std\":\n",
    "                add_feature = np.nanstd(self.sessions[val_col], ddof=1)\n",
    "            elif agg == \"sum\":\n",
    "                add_feature = np.nansum(self.sessions[val_col])\n",
    "            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    tmp = self.sessions[val_col][idx]\n",
    "                    if agg == \"mean\":\n",
    "                        add_feature = np.nanmean(tmp)\n",
    "                    elif agg == \"max\":\n",
    "                        add_feature = np.nanmax(tmp)\n",
    "                    elif agg == \"min\":\n",
    "                        add_feature = np.nanmin(tmp)\n",
    "                    elif agg == \"std\":\n",
    "                        add_feature = np.nanstd(tmp, ddof=1)\n",
    "                    elif agg == \"sum\":\n",
    "                        add_feature = np.nansum(tmp)\n",
    "                    if np.isnan(add_feature):\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def get_test(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "\n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesTrain(sessions, labels)\n",
    "    train = feat.get_train()\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesInf(sessions, labels)\n",
    "    test = feat.get_test()\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "    \n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "        gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "        fis = []\n",
    "        \n",
    "        for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "            oof_groups = []\n",
    "            print(f\"fold : {i}\")\n",
    "            tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "            vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "            tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "            model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                            num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)\n",
    "            # モデル出力\n",
    "            model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\")\n",
    "        \n",
    "            # valid_pred\n",
    "            oof_fold = train.iloc[vl_idx].copy()\n",
    "            oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "            oof_groups.append(oof_fold)\n",
    "\n",
    "            # 特徴量重要度\n",
    "            fi_fold = pd.DataFrame()\n",
    "            fi_fold[\"feature\"] = model.feature_name()\n",
    "            fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "            fi_fold[\"fold\"] = i\n",
    "            fis.append(fi_fold)\n",
    "\n",
    "        fi = pd.concat(fis)    \n",
    "        fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "        fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "        fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi_{group}.csv\", index=False)\n",
    "\n",
    "        oof_group = pd.concat(oof_groups)\n",
    "        oofs.append(oof_group)\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = oof_group.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{group}_pred_mean\", \"max\":f\"{group}_pred_max\", \"min\":f\"{group}_pred_min\", \"std\":f\"{group}_pred_std\"})\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"level_group2\"] = test[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"level_group2\"] = sub[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in test.groupby(\"level_group2\")]\n",
    "    subs = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in sub.groupby(\"level_group2\")]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iter\n",
      "0-4\n",
      "5-12\n",
      "13-22\n",
      "test_iter\n",
      "20090109393214576 0-4\n",
      "20090109393214576 5-12\n",
      "20090109393214576 13-22\n",
      "20090312143683264 0-4\n",
      "20090312143683264 5-12\n",
      "20090312143683264 13-22\n",
      "20090312331414616 0-4\n",
      "20090312331414616 5-12\n",
      "20090312331414616 13-22\n"
     ]
    }
   ],
   "source": [
    "iter_train = get_mock_iter_train()\n",
    "iter_test = get_mock_iter_test()\n",
    "\n",
    "print(\"train_iter\")\n",
    "train_dfs = []\n",
    "prev_features_df = None\n",
    "for (sessions, sub) in iter_train:\n",
    "    group = sessions[\"level_group\"].values[0]\n",
    "    print(group)\n",
    "    train = get_train_dataset(sessions, sub)\n",
    "    if prev_features_df is not None:\n",
    "        train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "    else:\n",
    "        pass\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "    if group == \"5-12\":\n",
    "        train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "        train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "    elif group == \"13-22\":\n",
    "        train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "        train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "    target = \"correct\"\n",
    "    not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "    train_dfs.append(train[[\"session_id\"]+features])\n",
    "    prev_features_df = train[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "\n",
    "\n",
    "print(\"test_iter\")\n",
    "model_dict = {}\n",
    "features_dict = {}\n",
    "for g in level_group_list:\n",
    "    model_paths = [cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "    model_dict[g] = [lgb.Booster(model_file=p) for p in model_paths]\n",
    "    features_dict[g] = model_dict[g][0].feature_name()\n",
    "\n",
    "test_dfs_0_4 = []\n",
    "test_dfs_5_12 = []\n",
    "test_dfs_13_22 = []\n",
    "prev_features_df = None\n",
    "for (test_sessions, sample_submission) in iter_test:\n",
    "    level_group = test_sessions[\"level_group\"].values[0]\n",
    "    session_id = test_sessions[\"session_id\"].values[0]\n",
    "    print(session_id, level_group)\n",
    "    features = features_dict[level_group]\n",
    "    test = get_test_dataset(test_sessions, sample_submission)\n",
    "\n",
    "    if level_group == \"0-4\":\n",
    "        pass\n",
    "    else:\n",
    "        test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "    # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "    if level_group == \"5-12\":\n",
    "        test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "        test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "    elif level_group == \"13-22\":\n",
    "        test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "        test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "    target = \"correct\"\n",
    "    not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "    features = [c for c in test.columns if c not in not_use_cols]\n",
    "    prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "    if level_group == \"0-4\":\n",
    "        test_dfs_0_4.append(test[[\"session_id\"]+features])\n",
    "    elif level_group == \"5-12\":\n",
    "        test_dfs_5_12.append(test[[\"session_id\"]+features])\n",
    "    elif level_group == \"13-22\":\n",
    "        test_dfs_13_22.append(test[[\"session_id\"]+features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    33850.0\n",
       "3    33850.0\n",
       "6    33850.0\n",
       "1    31816.0\n",
       "4    31816.0\n",
       "7    31816.0\n",
       "2    21043.0\n",
       "5    21043.0\n",
       "8    21043.0\n",
       "Name: 0-4_event_name_person_click_time_diff_sum, dtype: float32"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"0-4_event_name_person_click_time_diff_sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    33850.0\n",
       "1    33850.0\n",
       "2    33850.0\n",
       "0    31816.0\n",
       "1    31816.0\n",
       "2    31816.0\n",
       "0    21043.0\n",
       "1    21043.0\n",
       "2    21043.0\n",
       "Name: 0-4_event_name_person_click_time_diff_sum, dtype: float32"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"0-4_event_name_person_click_time_diff_sum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_dfs[2].sort_values([\"session_id\", \"question\"])\n",
    "test = pd.concat(test_dfs_13_22).sort_values([\"session_id\", \"question\"])\n",
    "for c in train.columns:\n",
    "    if not np.all(train[c].values == test[c].values):\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>question</th>\n",
       "      <th>0-4_record_cnt</th>\n",
       "      <th>0-4_group_elapsed_time</th>\n",
       "      <th>0-4_event_name_cutscene_click_record_cnt</th>\n",
       "      <th>0-4_event_name_person_click_record_cnt</th>\n",
       "      <th>0-4_event_name_navigate_click_record_cnt</th>\n",
       "      <th>0-4_event_name_observation_click_record_cnt</th>\n",
       "      <th>0-4_event_name_notification_click_record_cnt</th>\n",
       "      <th>0-4_event_name_object_click_record_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>0-4_name_open_screen_coor_x_mean</th>\n",
       "      <th>0-4_name_open_screen_coor_y_mean</th>\n",
       "      <th>0-4_name_prev_room_coor_x_mean</th>\n",
       "      <th>0-4_name_prev_room_coor_y_mean</th>\n",
       "      <th>0-4_name_prev_screen_coor_x_mean</th>\n",
       "      <th>0-4_name_prev_screen_coor_y_mean</th>\n",
       "      <th>0-4_name_next_room_coor_x_mean</th>\n",
       "      <th>0-4_name_next_room_coor_y_mean</th>\n",
       "      <th>0-4_name_next_screen_coor_x_mean</th>\n",
       "      <th>0-4_name_next_screen_coor_y_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090109393214576</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>267350.0</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>621.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20090109393214576</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>267350.0</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>621.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20090109393214576</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>267350.0</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>621.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312143683264</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>323646.0</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20090312143683264</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>323646.0</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20090312143683264</td>\n",
       "      <td>3</td>\n",
       "      <td>163</td>\n",
       "      <td>323646.0</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312331414616</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>256276.0</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>54.5</td>\n",
       "      <td>611.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20090312331414616</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>256276.0</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>54.5</td>\n",
       "      <td>611.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20090312331414616</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>256276.0</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>54.5</td>\n",
       "      <td>611.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 2789 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id question  0-4_record_cnt  0-4_group_elapsed_time  \\\n",
       "0  20090109393214576        1             140                267350.0   \n",
       "3  20090109393214576        2             140                267350.0   \n",
       "6  20090109393214576        3             140                267350.0   \n",
       "1  20090312143683264        1             163                323646.0   \n",
       "4  20090312143683264        2             163                323646.0   \n",
       "7  20090312143683264        3             163                323646.0   \n",
       "2  20090312331414616        1             130                256276.0   \n",
       "5  20090312331414616        2             130                256276.0   \n",
       "8  20090312331414616        3             130                256276.0   \n",
       "\n",
       "   0-4_event_name_cutscene_click_record_cnt  \\\n",
       "0                                        27   \n",
       "3                                        27   \n",
       "6                                        27   \n",
       "1                                        33   \n",
       "4                                        33   \n",
       "7                                        33   \n",
       "2                                        30   \n",
       "5                                        30   \n",
       "8                                        30   \n",
       "\n",
       "   0-4_event_name_person_click_record_cnt  \\\n",
       "0                                      21   \n",
       "3                                      21   \n",
       "6                                      21   \n",
       "1                                      18   \n",
       "4                                      18   \n",
       "7                                      18   \n",
       "2                                      19   \n",
       "5                                      19   \n",
       "8                                      19   \n",
       "\n",
       "   0-4_event_name_navigate_click_record_cnt  \\\n",
       "0                                        62   \n",
       "3                                        62   \n",
       "6                                        62   \n",
       "1                                        62   \n",
       "4                                        62   \n",
       "7                                        62   \n",
       "2                                        41   \n",
       "5                                        41   \n",
       "8                                        41   \n",
       "\n",
       "   0-4_event_name_observation_click_record_cnt  \\\n",
       "0                                            3   \n",
       "3                                            3   \n",
       "6                                            3   \n",
       "1                                            4   \n",
       "4                                            4   \n",
       "7                                            4   \n",
       "2                                            1   \n",
       "5                                            1   \n",
       "8                                            1   \n",
       "\n",
       "   0-4_event_name_notification_click_record_cnt  \\\n",
       "0                                             5   \n",
       "3                                             5   \n",
       "6                                             5   \n",
       "1                                             9   \n",
       "4                                             9   \n",
       "7                                             9   \n",
       "2                                             6   \n",
       "5                                             6   \n",
       "8                                             6   \n",
       "\n",
       "   0-4_event_name_object_click_record_cnt  ...  \\\n",
       "0                                       9  ...   \n",
       "3                                       9  ...   \n",
       "6                                       9  ...   \n",
       "1                                      22  ...   \n",
       "4                                      22  ...   \n",
       "7                                      22  ...   \n",
       "2                                      10  ...   \n",
       "5                                      10  ...   \n",
       "8                                      10  ...   \n",
       "\n",
       "   0-4_name_open_screen_coor_x_mean  0-4_name_open_screen_coor_y_mean  \\\n",
       "0                              54.0                             621.5   \n",
       "3                              54.0                             621.5   \n",
       "6                              54.0                             621.5   \n",
       "1                              44.0                             627.0   \n",
       "4                              44.0                             627.0   \n",
       "7                              44.0                             627.0   \n",
       "2                              54.5                             611.0   \n",
       "5                              54.5                             611.0   \n",
       "8                              54.5                             611.0   \n",
       "\n",
       "   0-4_name_prev_room_coor_x_mean  0-4_name_prev_room_coor_y_mean  \\\n",
       "0                            -1.0                            -1.0   \n",
       "3                            -1.0                            -1.0   \n",
       "6                            -1.0                            -1.0   \n",
       "1                            -1.0                            -1.0   \n",
       "4                            -1.0                            -1.0   \n",
       "7                            -1.0                            -1.0   \n",
       "2                            -1.0                            -1.0   \n",
       "5                            -1.0                            -1.0   \n",
       "8                            -1.0                            -1.0   \n",
       "\n",
       "   0-4_name_prev_screen_coor_x_mean  0-4_name_prev_screen_coor_y_mean  \\\n",
       "0                              -1.0                              -1.0   \n",
       "3                              -1.0                              -1.0   \n",
       "6                              -1.0                              -1.0   \n",
       "1                              -1.0                              -1.0   \n",
       "4                              -1.0                              -1.0   \n",
       "7                              -1.0                              -1.0   \n",
       "2                              -1.0                              -1.0   \n",
       "5                              -1.0                              -1.0   \n",
       "8                              -1.0                              -1.0   \n",
       "\n",
       "   0-4_name_next_room_coor_x_mean  0-4_name_next_room_coor_y_mean  \\\n",
       "0                            -1.0                            -1.0   \n",
       "3                            -1.0                            -1.0   \n",
       "6                            -1.0                            -1.0   \n",
       "1                            -1.0                            -1.0   \n",
       "4                            -1.0                            -1.0   \n",
       "7                            -1.0                            -1.0   \n",
       "2                            -1.0                            -1.0   \n",
       "5                            -1.0                            -1.0   \n",
       "8                            -1.0                            -1.0   \n",
       "\n",
       "   0-4_name_next_screen_coor_x_mean  0-4_name_next_screen_coor_y_mean  \n",
       "0                              -1.0                              -1.0  \n",
       "3                              -1.0                              -1.0  \n",
       "6                              -1.0                              -1.0  \n",
       "1                              -1.0                              -1.0  \n",
       "4                              -1.0                              -1.0  \n",
       "7                              -1.0                              -1.0  \n",
       "2                              -1.0                              -1.0  \n",
       "5                              -1.0                              -1.0  \n",
       "8                              -1.0                              -1.0  \n",
       "\n",
       "[9 rows x 2789 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>question</th>\n",
       "      <th>0-4_record_cnt</th>\n",
       "      <th>0-4_group_elapsed_time</th>\n",
       "      <th>0-4_event_name_cutscene_click_record_cnt</th>\n",
       "      <th>0-4_event_name_person_click_record_cnt</th>\n",
       "      <th>0-4_event_name_navigate_click_record_cnt</th>\n",
       "      <th>0-4_event_name_observation_click_record_cnt</th>\n",
       "      <th>0-4_event_name_notification_click_record_cnt</th>\n",
       "      <th>0-4_event_name_object_click_record_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>0-4_name_open_screen_coor_x_mean</th>\n",
       "      <th>0-4_name_open_screen_coor_y_mean</th>\n",
       "      <th>0-4_name_prev_room_coor_x_mean</th>\n",
       "      <th>0-4_name_prev_room_coor_y_mean</th>\n",
       "      <th>0-4_name_prev_screen_coor_x_mean</th>\n",
       "      <th>0-4_name_prev_screen_coor_y_mean</th>\n",
       "      <th>0-4_name_next_room_coor_x_mean</th>\n",
       "      <th>0-4_name_next_room_coor_y_mean</th>\n",
       "      <th>0-4_name_next_screen_coor_x_mean</th>\n",
       "      <th>0-4_name_next_screen_coor_y_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090109393214576</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>267350.0</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>621.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090109393214576</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>267350.0</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>621.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090109393214576</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>267350.0</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>621.5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312143683264</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>323646.0</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312143683264</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>323646.0</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312143683264</td>\n",
       "      <td>3</td>\n",
       "      <td>163</td>\n",
       "      <td>323646.0</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312331414616</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>256276.0</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>54.5</td>\n",
       "      <td>611.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312331414616</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>256276.0</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>54.5</td>\n",
       "      <td>611.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312331414616</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>256276.0</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>54.5</td>\n",
       "      <td>611.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 2789 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id question  0-4_record_cnt  0-4_group_elapsed_time  \\\n",
       "0  20090109393214576        1             140                267350.0   \n",
       "1  20090109393214576        2             140                267350.0   \n",
       "2  20090109393214576        3             140                267350.0   \n",
       "0  20090312143683264        1             163                323646.0   \n",
       "1  20090312143683264        2             163                323646.0   \n",
       "2  20090312143683264        3             163                323646.0   \n",
       "0  20090312331414616        1             130                256276.0   \n",
       "1  20090312331414616        2             130                256276.0   \n",
       "2  20090312331414616        3             130                256276.0   \n",
       "\n",
       "   0-4_event_name_cutscene_click_record_cnt  \\\n",
       "0                                        27   \n",
       "1                                        27   \n",
       "2                                        27   \n",
       "0                                        33   \n",
       "1                                        33   \n",
       "2                                        33   \n",
       "0                                        30   \n",
       "1                                        30   \n",
       "2                                        30   \n",
       "\n",
       "   0-4_event_name_person_click_record_cnt  \\\n",
       "0                                      21   \n",
       "1                                      21   \n",
       "2                                      21   \n",
       "0                                      18   \n",
       "1                                      18   \n",
       "2                                      18   \n",
       "0                                      19   \n",
       "1                                      19   \n",
       "2                                      19   \n",
       "\n",
       "   0-4_event_name_navigate_click_record_cnt  \\\n",
       "0                                        62   \n",
       "1                                        62   \n",
       "2                                        62   \n",
       "0                                        62   \n",
       "1                                        62   \n",
       "2                                        62   \n",
       "0                                        41   \n",
       "1                                        41   \n",
       "2                                        41   \n",
       "\n",
       "   0-4_event_name_observation_click_record_cnt  \\\n",
       "0                                            3   \n",
       "1                                            3   \n",
       "2                                            3   \n",
       "0                                            4   \n",
       "1                                            4   \n",
       "2                                            4   \n",
       "0                                            1   \n",
       "1                                            1   \n",
       "2                                            1   \n",
       "\n",
       "   0-4_event_name_notification_click_record_cnt  \\\n",
       "0                                             5   \n",
       "1                                             5   \n",
       "2                                             5   \n",
       "0                                             9   \n",
       "1                                             9   \n",
       "2                                             9   \n",
       "0                                             6   \n",
       "1                                             6   \n",
       "2                                             6   \n",
       "\n",
       "   0-4_event_name_object_click_record_cnt  ...  \\\n",
       "0                                       9  ...   \n",
       "1                                       9  ...   \n",
       "2                                       9  ...   \n",
       "0                                      22  ...   \n",
       "1                                      22  ...   \n",
       "2                                      22  ...   \n",
       "0                                      10  ...   \n",
       "1                                      10  ...   \n",
       "2                                      10  ...   \n",
       "\n",
       "   0-4_name_open_screen_coor_x_mean  0-4_name_open_screen_coor_y_mean  \\\n",
       "0                              54.0                             621.5   \n",
       "1                              54.0                             621.5   \n",
       "2                              54.0                             621.5   \n",
       "0                              44.0                             627.0   \n",
       "1                              44.0                             627.0   \n",
       "2                              44.0                             627.0   \n",
       "0                              54.5                             611.0   \n",
       "1                              54.5                             611.0   \n",
       "2                              54.5                             611.0   \n",
       "\n",
       "   0-4_name_prev_room_coor_x_mean  0-4_name_prev_room_coor_y_mean  \\\n",
       "0                            -1.0                            -1.0   \n",
       "1                            -1.0                            -1.0   \n",
       "2                            -1.0                            -1.0   \n",
       "0                            -1.0                            -1.0   \n",
       "1                            -1.0                            -1.0   \n",
       "2                            -1.0                            -1.0   \n",
       "0                            -1.0                            -1.0   \n",
       "1                            -1.0                            -1.0   \n",
       "2                            -1.0                            -1.0   \n",
       "\n",
       "   0-4_name_prev_screen_coor_x_mean  0-4_name_prev_screen_coor_y_mean  \\\n",
       "0                              -1.0                              -1.0   \n",
       "1                              -1.0                              -1.0   \n",
       "2                              -1.0                              -1.0   \n",
       "0                              -1.0                              -1.0   \n",
       "1                              -1.0                              -1.0   \n",
       "2                              -1.0                              -1.0   \n",
       "0                              -1.0                              -1.0   \n",
       "1                              -1.0                              -1.0   \n",
       "2                              -1.0                              -1.0   \n",
       "\n",
       "   0-4_name_next_room_coor_x_mean  0-4_name_next_room_coor_y_mean  \\\n",
       "0                            -1.0                            -1.0   \n",
       "1                            -1.0                            -1.0   \n",
       "2                            -1.0                            -1.0   \n",
       "0                            -1.0                            -1.0   \n",
       "1                            -1.0                            -1.0   \n",
       "2                            -1.0                            -1.0   \n",
       "0                            -1.0                            -1.0   \n",
       "1                            -1.0                            -1.0   \n",
       "2                            -1.0                            -1.0   \n",
       "\n",
       "   0-4_name_next_screen_coor_x_mean  0-4_name_next_screen_coor_y_mean  \n",
       "0                              -1.0                              -1.0  \n",
       "1                              -1.0                              -1.0  \n",
       "2                              -1.0                              -1.0  \n",
       "0                              -1.0                              -1.0  \n",
       "1                              -1.0                              -1.0  \n",
       "2                              -1.0                              -1.0  \n",
       "0                              -1.0                              -1.0  \n",
       "1                              -1.0                              -1.0  \n",
       "2                              -1.0                              -1.0  \n",
       "\n",
       "[9 rows x 2789 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_0_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    model_dict = {}\n",
    "    features_dict = {}\n",
    "    for g in level_group_list:\n",
    "        if mode == \"local_cv\":\n",
    "            model_paths = [cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_paths = [f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        model_dict[g] = [lgb.Booster(model_file=p) for p in model_paths]\n",
    "        features_dict[g] = model_dict[g][0].feature_name()\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "        features = features_dict[level_group]\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = model_dict[level_group][i]\n",
    "            preds += model.predict(test[features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        test[\"pred\"] = preds\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = test.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{level_group}_pred_mean\", \"max\":f\"{level_group}_pred_max\", \"min\":f\"{level_group}_pred_min\", \"std\":f\"{level_group}_pred_std\"})\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49821, number of negative: 6726\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 178509\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 2561\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.881055 -> initscore=2.002456\n",
      "[LightGBM] [Info] Start training from score 2.002456\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.284783\tvalid_1's binary_logloss: 0.307061\n",
      "[200]\ttraining's binary_logloss: 0.252536\tvalid_1's binary_logloss: 0.285915\n",
      "[300]\ttraining's binary_logloss: 0.232259\tvalid_1's binary_logloss: 0.276315\n",
      "[400]\ttraining's binary_logloss: 0.218208\tvalid_1's binary_logloss: 0.272385\n",
      "[500]\ttraining's binary_logloss: 0.20676\tvalid_1's binary_logloss: 0.270915\n",
      "[600]\ttraining's binary_logloss: 0.196777\tvalid_1's binary_logloss: 0.270134\n",
      "[700]\ttraining's binary_logloss: 0.187885\tvalid_1's binary_logloss: 0.269715\n",
      "[800]\ttraining's binary_logloss: 0.179861\tvalid_1's binary_logloss: 0.269576\n",
      "Early stopping, best iteration is:\n",
      "[780]\ttraining's binary_logloss: 0.181423\tvalid_1's binary_logloss: 0.269511\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49692, number of negative: 6855\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.182970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 178420\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 2561\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.878773 -> initscore=1.980866\n",
      "[LightGBM] [Info] Start training from score 1.980866\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.288608\tvalid_1's binary_logloss: 0.295072\n",
      "[200]\ttraining's binary_logloss: 0.255937\tvalid_1's binary_logloss: 0.27498\n",
      "[300]\ttraining's binary_logloss: 0.235472\tvalid_1's binary_logloss: 0.266042\n",
      "[400]\ttraining's binary_logloss: 0.221324\tvalid_1's binary_logloss: 0.262656\n",
      "[500]\ttraining's binary_logloss: 0.209776\tvalid_1's binary_logloss: 0.261322\n",
      "[600]\ttraining's binary_logloss: 0.199807\tvalid_1's binary_logloss: 0.26048\n",
      "[700]\ttraining's binary_logloss: 0.19078\tvalid_1's binary_logloss: 0.260174\n",
      "[800]\ttraining's binary_logloss: 0.182686\tvalid_1's binary_logloss: 0.260027\n",
      "Early stopping, best iteration is:\n",
      "[783]\ttraining's binary_logloss: 0.184013\tvalid_1's binary_logloss: 0.259987\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49772, number of negative: 6778\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032979 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 178564\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 2561\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880141 -> initscore=1.993771\n",
      "[LightGBM] [Info] Start training from score 1.993771\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.287463\tvalid_1's binary_logloss: 0.30011\n",
      "[200]\ttraining's binary_logloss: 0.255463\tvalid_1's binary_logloss: 0.27845\n",
      "[300]\ttraining's binary_logloss: 0.235303\tvalid_1's binary_logloss: 0.268496\n",
      "[400]\ttraining's binary_logloss: 0.221266\tvalid_1's binary_logloss: 0.264541\n",
      "[500]\ttraining's binary_logloss: 0.209846\tvalid_1's binary_logloss: 0.262569\n",
      "[600]\ttraining's binary_logloss: 0.199853\tvalid_1's binary_logloss: 0.261811\n",
      "[700]\ttraining's binary_logloss: 0.190999\tvalid_1's binary_logloss: 0.261498\n",
      "[800]\ttraining's binary_logloss: 0.182958\tvalid_1's binary_logloss: 0.261351\n",
      "[900]\ttraining's binary_logloss: 0.175813\tvalid_1's binary_logloss: 0.261316\n",
      "Early stopping, best iteration is:\n",
      "[859]\ttraining's binary_logloss: 0.178646\tvalid_1's binary_logloss: 0.261235\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49755, number of negative: 6795\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 178520\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 2561\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.879841 -> initscore=1.990924\n",
      "[LightGBM] [Info] Start training from score 1.990924\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.286993\tvalid_1's binary_logloss: 0.302268\n",
      "[200]\ttraining's binary_logloss: 0.254832\tvalid_1's binary_logloss: 0.282609\n",
      "[300]\ttraining's binary_logloss: 0.234668\tvalid_1's binary_logloss: 0.274026\n",
      "[400]\ttraining's binary_logloss: 0.220676\tvalid_1's binary_logloss: 0.270659\n",
      "[500]\ttraining's binary_logloss: 0.209259\tvalid_1's binary_logloss: 0.269336\n",
      "[600]\ttraining's binary_logloss: 0.19944\tvalid_1's binary_logloss: 0.268851\n",
      "[700]\ttraining's binary_logloss: 0.190586\tvalid_1's binary_logloss: 0.26856\n",
      "Early stopping, best iteration is:\n",
      "[664]\ttraining's binary_logloss: 0.193651\tvalid_1's binary_logloss: 0.268538\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49804, number of negative: 6746\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 178473\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 2561\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880707 -> initscore=1.999146\n",
      "[LightGBM] [Info] Start training from score 1.999146\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.287298\tvalid_1's binary_logloss: 0.304275\n",
      "[200]\ttraining's binary_logloss: 0.255616\tvalid_1's binary_logloss: 0.281908\n",
      "[300]\ttraining's binary_logloss: 0.235771\tvalid_1's binary_logloss: 0.271342\n",
      "[400]\ttraining's binary_logloss: 0.221903\tvalid_1's binary_logloss: 0.267097\n",
      "[500]\ttraining's binary_logloss: 0.210416\tvalid_1's binary_logloss: 0.265083\n",
      "[600]\ttraining's binary_logloss: 0.200527\tvalid_1's binary_logloss: 0.264084\n",
      "[700]\ttraining's binary_logloss: 0.191616\tvalid_1's binary_logloss: 0.263742\n",
      "[800]\ttraining's binary_logloss: 0.18361\tvalid_1's binary_logloss: 0.26367\n",
      "[900]\ttraining's binary_logloss: 0.176484\tvalid_1's binary_logloss: 0.263482\n",
      "[1000]\ttraining's binary_logloss: 0.16995\tvalid_1's binary_logloss: 0.263511\n",
      "Early stopping, best iteration is:\n",
      "[926]\ttraining's binary_logloss: 0.1747\tvalid_1's binary_logloss: 0.263413\n",
      "5-12\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.935802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 474015\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5189\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.57283\tvalid_1's binary_logloss: 0.580888\n",
      "[200]\ttraining's binary_logloss: 0.542001\tvalid_1's binary_logloss: 0.555694\n",
      "[300]\ttraining's binary_logloss: 0.524765\tvalid_1's binary_logloss: 0.544143\n",
      "[400]\ttraining's binary_logloss: 0.513741\tvalid_1's binary_logloss: 0.538793\n",
      "[500]\ttraining's binary_logloss: 0.505474\tvalid_1's binary_logloss: 0.536238\n",
      "[600]\ttraining's binary_logloss: 0.4985\tvalid_1's binary_logloss: 0.534598\n",
      "[700]\ttraining's binary_logloss: 0.492342\tvalid_1's binary_logloss: 0.53354\n",
      "[800]\ttraining's binary_logloss: 0.48673\tvalid_1's binary_logloss: 0.532859\n",
      "[900]\ttraining's binary_logloss: 0.481715\tvalid_1's binary_logloss: 0.532423\n",
      "[1000]\ttraining's binary_logloss: 0.477091\tvalid_1's binary_logloss: 0.532025\n",
      "[1100]\ttraining's binary_logloss: 0.472822\tvalid_1's binary_logloss: 0.531761\n",
      "[1200]\ttraining's binary_logloss: 0.468827\tvalid_1's binary_logloss: 0.531485\n",
      "[1300]\ttraining's binary_logloss: 0.465068\tvalid_1's binary_logloss: 0.531309\n",
      "[1400]\ttraining's binary_logloss: 0.461637\tvalid_1's binary_logloss: 0.531211\n",
      "[1500]\ttraining's binary_logloss: 0.458482\tvalid_1's binary_logloss: 0.5311\n",
      "[1600]\ttraining's binary_logloss: 0.455435\tvalid_1's binary_logloss: 0.53099\n",
      "[1700]\ttraining's binary_logloss: 0.452454\tvalid_1's binary_logloss: 0.530918\n",
      "[1800]\ttraining's binary_logloss: 0.449573\tvalid_1's binary_logloss: 0.530873\n",
      "[1900]\ttraining's binary_logloss: 0.446893\tvalid_1's binary_logloss: 0.530858\n",
      "[2000]\ttraining's binary_logloss: 0.444263\tvalid_1's binary_logloss: 0.530758\n",
      "[2100]\ttraining's binary_logloss: 0.441864\tvalid_1's binary_logloss: 0.530722\n",
      "[2200]\ttraining's binary_logloss: 0.439424\tvalid_1's binary_logloss: 0.530686\n",
      "[2300]\ttraining's binary_logloss: 0.437072\tvalid_1's binary_logloss: 0.530649\n",
      "[2400]\ttraining's binary_logloss: 0.434875\tvalid_1's binary_logloss: 0.530592\n",
      "[2500]\ttraining's binary_logloss: 0.432718\tvalid_1's binary_logloss: 0.53059\n",
      "Early stopping, best iteration is:\n",
      "[2457]\ttraining's binary_logloss: 0.433633\tvalid_1's binary_logloss: 0.530567\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122423, number of negative: 66067\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 8.953384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 473626\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5189\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649493 -> initscore=0.616813\n",
      "[LightGBM] [Info] Start training from score 0.616813\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.5764\tvalid_1's binary_logloss: 0.581043\n",
      "[200]\ttraining's binary_logloss: 0.546754\tvalid_1's binary_logloss: 0.5574\n",
      "[300]\ttraining's binary_logloss: 0.530037\tvalid_1's binary_logloss: 0.546697\n",
      "[400]\ttraining's binary_logloss: 0.519307\tvalid_1's binary_logloss: 0.541803\n",
      "[500]\ttraining's binary_logloss: 0.511289\tvalid_1's binary_logloss: 0.539472\n",
      "[600]\ttraining's binary_logloss: 0.504489\tvalid_1's binary_logloss: 0.538001\n",
      "[700]\ttraining's binary_logloss: 0.498556\tvalid_1's binary_logloss: 0.537163\n",
      "[800]\ttraining's binary_logloss: 0.493175\tvalid_1's binary_logloss: 0.536551\n",
      "[900]\ttraining's binary_logloss: 0.48838\tvalid_1's binary_logloss: 0.536137\n",
      "[1000]\ttraining's binary_logloss: 0.483942\tvalid_1's binary_logloss: 0.535831\n",
      "[1100]\ttraining's binary_logloss: 0.479792\tvalid_1's binary_logloss: 0.535689\n",
      "[1200]\ttraining's binary_logloss: 0.475965\tvalid_1's binary_logloss: 0.535439\n",
      "[1300]\ttraining's binary_logloss: 0.472405\tvalid_1's binary_logloss: 0.535315\n",
      "[1400]\ttraining's binary_logloss: 0.469039\tvalid_1's binary_logloss: 0.535221\n",
      "[1500]\ttraining's binary_logloss: 0.466022\tvalid_1's binary_logloss: 0.535219\n",
      "[1600]\ttraining's binary_logloss: 0.463082\tvalid_1's binary_logloss: 0.535113\n",
      "[1700]\ttraining's binary_logloss: 0.460278\tvalid_1's binary_logloss: 0.53499\n",
      "[1800]\ttraining's binary_logloss: 0.45753\tvalid_1's binary_logloss: 0.534987\n",
      "Early stopping, best iteration is:\n",
      "[1748]\ttraining's binary_logloss: 0.458962\tvalid_1's binary_logloss: 0.534942\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122249, number of negative: 66251\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.312378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 473920\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 5189\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.648536 -> initscore=0.612609\n",
      "[LightGBM] [Info] Start training from score 0.612609\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.576813\tvalid_1's binary_logloss: 0.579054\n",
      "[200]\ttraining's binary_logloss: 0.54706\tvalid_1's binary_logloss: 0.555506\n",
      "[300]\ttraining's binary_logloss: 0.530358\tvalid_1's binary_logloss: 0.544832\n",
      "[400]\ttraining's binary_logloss: 0.51964\tvalid_1's binary_logloss: 0.539916\n",
      "[500]\ttraining's binary_logloss: 0.511681\tvalid_1's binary_logloss: 0.537704\n",
      "[600]\ttraining's binary_logloss: 0.504952\tvalid_1's binary_logloss: 0.536376\n",
      "[700]\ttraining's binary_logloss: 0.499035\tvalid_1's binary_logloss: 0.53548\n",
      "[800]\ttraining's binary_logloss: 0.493697\tvalid_1's binary_logloss: 0.534947\n",
      "[900]\ttraining's binary_logloss: 0.488913\tvalid_1's binary_logloss: 0.534618\n",
      "[1000]\ttraining's binary_logloss: 0.484477\tvalid_1's binary_logloss: 0.534432\n",
      "[1100]\ttraining's binary_logloss: 0.480416\tvalid_1's binary_logloss: 0.534198\n",
      "[1200]\ttraining's binary_logloss: 0.476656\tvalid_1's binary_logloss: 0.534035\n",
      "[1300]\ttraining's binary_logloss: 0.473159\tvalid_1's binary_logloss: 0.533933\n",
      "[1400]\ttraining's binary_logloss: 0.469981\tvalid_1's binary_logloss: 0.53376\n",
      "[1500]\ttraining's binary_logloss: 0.466952\tvalid_1's binary_logloss: 0.533634\n",
      "[1600]\ttraining's binary_logloss: 0.464026\tvalid_1's binary_logloss: 0.533631\n",
      "[1700]\ttraining's binary_logloss: 0.461228\tvalid_1's binary_logloss: 0.533555\n",
      "[1800]\ttraining's binary_logloss: 0.458514\tvalid_1's binary_logloss: 0.533444\n",
      "[1900]\ttraining's binary_logloss: 0.455945\tvalid_1's binary_logloss: 0.533413\n",
      "[2000]\ttraining's binary_logloss: 0.453528\tvalid_1's binary_logloss: 0.533364\n",
      "[2100]\ttraining's binary_logloss: 0.451193\tvalid_1's binary_logloss: 0.533363\n",
      "Early stopping, best iteration is:\n",
      "[2056]\ttraining's binary_logloss: 0.45223\tvalid_1's binary_logloss: 0.53334\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122566, number of negative: 65934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.467875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 473938\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 5188\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650218 -> initscore=0.619995\n",
      "[LightGBM] [Info] Start training from score 0.619995\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572764\tvalid_1's binary_logloss: 0.580685\n",
      "[200]\ttraining's binary_logloss: 0.542515\tvalid_1's binary_logloss: 0.556498\n",
      "[300]\ttraining's binary_logloss: 0.524863\tvalid_1's binary_logloss: 0.544441\n",
      "[400]\ttraining's binary_logloss: 0.513963\tvalid_1's binary_logloss: 0.539082\n",
      "[500]\ttraining's binary_logloss: 0.50575\tvalid_1's binary_logloss: 0.536438\n",
      "[600]\ttraining's binary_logloss: 0.498845\tvalid_1's binary_logloss: 0.534733\n",
      "[700]\ttraining's binary_logloss: 0.492712\tvalid_1's binary_logloss: 0.533561\n",
      "[800]\ttraining's binary_logloss: 0.487173\tvalid_1's binary_logloss: 0.532841\n",
      "[900]\ttraining's binary_logloss: 0.482202\tvalid_1's binary_logloss: 0.532337\n",
      "[1000]\ttraining's binary_logloss: 0.477642\tvalid_1's binary_logloss: 0.532011\n",
      "[1100]\ttraining's binary_logloss: 0.473394\tvalid_1's binary_logloss: 0.531764\n",
      "[1200]\ttraining's binary_logloss: 0.469409\tvalid_1's binary_logloss: 0.531484\n",
      "[1300]\ttraining's binary_logloss: 0.465743\tvalid_1's binary_logloss: 0.531292\n",
      "[1400]\ttraining's binary_logloss: 0.462274\tvalid_1's binary_logloss: 0.531146\n",
      "[1500]\ttraining's binary_logloss: 0.458996\tvalid_1's binary_logloss: 0.531034\n",
      "[1600]\ttraining's binary_logloss: 0.455949\tvalid_1's binary_logloss: 0.530957\n",
      "[1700]\ttraining's binary_logloss: 0.453029\tvalid_1's binary_logloss: 0.53091\n",
      "Early stopping, best iteration is:\n",
      "[1632]\ttraining's binary_logloss: 0.454955\tvalid_1's binary_logloss: 0.530876\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122635, number of negative: 65865\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.771049 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 472734\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 5189\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650584 -> initscore=0.621605\n",
      "[LightGBM] [Info] Start training from score 0.621605\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572581\tvalid_1's binary_logloss: 0.581478\n",
      "[200]\ttraining's binary_logloss: 0.54159\tvalid_1's binary_logloss: 0.556565\n",
      "[300]\ttraining's binary_logloss: 0.524382\tvalid_1's binary_logloss: 0.545047\n",
      "[400]\ttraining's binary_logloss: 0.51329\tvalid_1's binary_logloss: 0.539648\n",
      "[500]\ttraining's binary_logloss: 0.505016\tvalid_1's binary_logloss: 0.537005\n",
      "[600]\ttraining's binary_logloss: 0.498015\tvalid_1's binary_logloss: 0.535446\n",
      "[700]\ttraining's binary_logloss: 0.49189\tvalid_1's binary_logloss: 0.534444\n",
      "[800]\ttraining's binary_logloss: 0.486391\tvalid_1's binary_logloss: 0.533755\n",
      "[900]\ttraining's binary_logloss: 0.481373\tvalid_1's binary_logloss: 0.533394\n",
      "[1000]\ttraining's binary_logloss: 0.476806\tvalid_1's binary_logloss: 0.533142\n",
      "[1100]\ttraining's binary_logloss: 0.472512\tvalid_1's binary_logloss: 0.533001\n",
      "[1200]\ttraining's binary_logloss: 0.468565\tvalid_1's binary_logloss: 0.532961\n",
      "[1300]\ttraining's binary_logloss: 0.464844\tvalid_1's binary_logloss: 0.532803\n",
      "[1400]\ttraining's binary_logloss: 0.461458\tvalid_1's binary_logloss: 0.532687\n",
      "[1500]\ttraining's binary_logloss: 0.458238\tvalid_1's binary_logloss: 0.532573\n",
      "[1600]\ttraining's binary_logloss: 0.455157\tvalid_1's binary_logloss: 0.532524\n",
      "[1700]\ttraining's binary_logloss: 0.452169\tvalid_1's binary_logloss: 0.53251\n",
      "Early stopping, best iteration is:\n",
      "[1610]\ttraining's binary_logloss: 0.454828\tvalid_1's binary_logloss: 0.532503\n",
      "13-22\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67313, number of negative: 26932\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.359551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851467\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 7862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.714234 -> initscore=0.916038\n",
      "[LightGBM] [Info] Start training from score 0.916038\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.534834\tvalid_1's binary_logloss: 0.549364\n",
      "[200]\ttraining's binary_logloss: 0.506384\tvalid_1's binary_logloss: 0.528784\n",
      "[300]\ttraining's binary_logloss: 0.487251\tvalid_1's binary_logloss: 0.517948\n",
      "[400]\ttraining's binary_logloss: 0.473063\tvalid_1's binary_logloss: 0.512241\n",
      "[500]\ttraining's binary_logloss: 0.461295\tvalid_1's binary_logloss: 0.50866\n",
      "[600]\ttraining's binary_logloss: 0.451918\tvalid_1's binary_logloss: 0.50718\n",
      "[700]\ttraining's binary_logloss: 0.443376\tvalid_1's binary_logloss: 0.506268\n",
      "[800]\ttraining's binary_logloss: 0.435883\tvalid_1's binary_logloss: 0.505826\n",
      "[900]\ttraining's binary_logloss: 0.42903\tvalid_1's binary_logloss: 0.50565\n",
      "[1000]\ttraining's binary_logloss: 0.422721\tvalid_1's binary_logloss: 0.505423\n",
      "[1100]\ttraining's binary_logloss: 0.416923\tvalid_1's binary_logloss: 0.505239\n",
      "[1200]\ttraining's binary_logloss: 0.41144\tvalid_1's binary_logloss: 0.505064\n",
      "[1300]\ttraining's binary_logloss: 0.406471\tvalid_1's binary_logloss: 0.50499\n",
      "[1400]\ttraining's binary_logloss: 0.401716\tvalid_1's binary_logloss: 0.504939\n",
      "Early stopping, best iteration is:\n",
      "[1374]\ttraining's binary_logloss: 0.402985\tvalid_1's binary_logloss: 0.504891\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 66951, number of negative: 27294\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.362183 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851180\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 7862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710393 -> initscore=0.897294\n",
      "[LightGBM] [Info] Start training from score 0.897294\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.543338\tvalid_1's binary_logloss: 0.538102\n",
      "[200]\ttraining's binary_logloss: 0.516747\tvalid_1's binary_logloss: 0.518318\n",
      "[300]\ttraining's binary_logloss: 0.498652\tvalid_1's binary_logloss: 0.507644\n",
      "[400]\ttraining's binary_logloss: 0.485501\tvalid_1's binary_logloss: 0.502236\n",
      "[500]\ttraining's binary_logloss: 0.474344\tvalid_1's binary_logloss: 0.499\n",
      "[600]\ttraining's binary_logloss: 0.465441\tvalid_1's binary_logloss: 0.497553\n",
      "[700]\ttraining's binary_logloss: 0.457225\tvalid_1's binary_logloss: 0.496621\n",
      "[800]\ttraining's binary_logloss: 0.450052\tvalid_1's binary_logloss: 0.496205\n",
      "[900]\ttraining's binary_logloss: 0.443637\tvalid_1's binary_logloss: 0.495953\n",
      "[1000]\ttraining's binary_logloss: 0.437538\tvalid_1's binary_logloss: 0.495636\n",
      "[1100]\ttraining's binary_logloss: 0.431794\tvalid_1's binary_logloss: 0.495446\n",
      "[1200]\ttraining's binary_logloss: 0.426763\tvalid_1's binary_logloss: 0.495293\n",
      "[1300]\ttraining's binary_logloss: 0.422023\tvalid_1's binary_logloss: 0.495208\n",
      "[1400]\ttraining's binary_logloss: 0.417575\tvalid_1's binary_logloss: 0.495084\n",
      "[1500]\ttraining's binary_logloss: 0.413208\tvalid_1's binary_logloss: 0.494954\n",
      "[1600]\ttraining's binary_logloss: 0.409291\tvalid_1's binary_logloss: 0.49496\n",
      "Early stopping, best iteration is:\n",
      "[1506]\ttraining's binary_logloss: 0.412937\tvalid_1's binary_logloss: 0.494919\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 66989, number of negative: 27261\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.235580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851430\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 7862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710759 -> initscore=0.899071\n",
      "[LightGBM] [Info] Start training from score 0.899071\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.53958\tvalid_1's binary_logloss: 0.539681\n",
      "[200]\ttraining's binary_logloss: 0.511622\tvalid_1's binary_logloss: 0.520102\n",
      "[300]\ttraining's binary_logloss: 0.492685\tvalid_1's binary_logloss: 0.509688\n",
      "[400]\ttraining's binary_logloss: 0.47902\tvalid_1's binary_logloss: 0.504464\n",
      "[500]\ttraining's binary_logloss: 0.467459\tvalid_1's binary_logloss: 0.501153\n",
      "[600]\ttraining's binary_logloss: 0.458163\tvalid_1's binary_logloss: 0.4998\n",
      "[700]\ttraining's binary_logloss: 0.449703\tvalid_1's binary_logloss: 0.498928\n",
      "[800]\ttraining's binary_logloss: 0.442273\tvalid_1's binary_logloss: 0.498436\n",
      "[900]\ttraining's binary_logloss: 0.435492\tvalid_1's binary_logloss: 0.498068\n",
      "[1000]\ttraining's binary_logloss: 0.429461\tvalid_1's binary_logloss: 0.497893\n",
      "[1100]\ttraining's binary_logloss: 0.423571\tvalid_1's binary_logloss: 0.497654\n",
      "[1200]\ttraining's binary_logloss: 0.418309\tvalid_1's binary_logloss: 0.497339\n",
      "[1300]\ttraining's binary_logloss: 0.413514\tvalid_1's binary_logloss: 0.497265\n",
      "[1400]\ttraining's binary_logloss: 0.408953\tvalid_1's binary_logloss: 0.497121\n",
      "[1500]\ttraining's binary_logloss: 0.404609\tvalid_1's binary_logloss: 0.497044\n",
      "[1600]\ttraining's binary_logloss: 0.400527\tvalid_1's binary_logloss: 0.497003\n",
      "[1700]\ttraining's binary_logloss: 0.396729\tvalid_1's binary_logloss: 0.496998\n",
      "[1800]\ttraining's binary_logloss: 0.392924\tvalid_1's binary_logloss: 0.496876\n",
      "[1900]\ttraining's binary_logloss: 0.389592\tvalid_1's binary_logloss: 0.496876\n",
      "[2000]\ttraining's binary_logloss: 0.386218\tvalid_1's binary_logloss: 0.496825\n",
      "[2100]\ttraining's binary_logloss: 0.383003\tvalid_1's binary_logloss: 0.496836\n",
      "Early stopping, best iteration is:\n",
      "[2061]\ttraining's binary_logloss: 0.384241\tvalid_1's binary_logloss: 0.496788\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67214, number of negative: 27036\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.882204 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851424\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 7861\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.713146 -> initscore=0.910712\n",
      "[LightGBM] [Info] Start training from score 0.910712\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.532022\tvalid_1's binary_logloss: 0.547278\n",
      "[200]\ttraining's binary_logloss: 0.499872\tvalid_1's binary_logloss: 0.52561\n",
      "[300]\ttraining's binary_logloss: 0.478839\tvalid_1's binary_logloss: 0.514178\n",
      "[400]\ttraining's binary_logloss: 0.465028\tvalid_1's binary_logloss: 0.509594\n",
      "[500]\ttraining's binary_logloss: 0.4538\tvalid_1's binary_logloss: 0.507302\n",
      "[600]\ttraining's binary_logloss: 0.443845\tvalid_1's binary_logloss: 0.50591\n",
      "[700]\ttraining's binary_logloss: 0.435021\tvalid_1's binary_logloss: 0.505208\n",
      "[800]\ttraining's binary_logloss: 0.427056\tvalid_1's binary_logloss: 0.504809\n",
      "[900]\ttraining's binary_logloss: 0.420145\tvalid_1's binary_logloss: 0.504628\n",
      "[1000]\ttraining's binary_logloss: 0.413517\tvalid_1's binary_logloss: 0.504477\n",
      "[1100]\ttraining's binary_logloss: 0.407399\tvalid_1's binary_logloss: 0.504376\n",
      "[1200]\ttraining's binary_logloss: 0.401623\tvalid_1's binary_logloss: 0.504302\n",
      "Early stopping, best iteration is:\n",
      "[1182]\ttraining's binary_logloss: 0.402527\tvalid_1's binary_logloss: 0.504243\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67245, number of negative: 27005\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.687667 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 849282\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 7862\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.713475 -> initscore=0.912321\n",
      "[LightGBM] [Info] Start training from score 0.912321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.532947\tvalid_1's binary_logloss: 0.547031\n",
      "[200]\ttraining's binary_logloss: 0.503429\tvalid_1's binary_logloss: 0.526877\n",
      "[300]\ttraining's binary_logloss: 0.4834\tvalid_1's binary_logloss: 0.516038\n",
      "[400]\ttraining's binary_logloss: 0.46899\tvalid_1's binary_logloss: 0.510426\n",
      "[500]\ttraining's binary_logloss: 0.457218\tvalid_1's binary_logloss: 0.507084\n",
      "[600]\ttraining's binary_logloss: 0.447581\tvalid_1's binary_logloss: 0.505587\n",
      "[700]\ttraining's binary_logloss: 0.4389\tvalid_1's binary_logloss: 0.504508\n",
      "[800]\ttraining's binary_logloss: 0.431153\tvalid_1's binary_logloss: 0.503994\n",
      "[900]\ttraining's binary_logloss: 0.424195\tvalid_1's binary_logloss: 0.503636\n",
      "[1000]\ttraining's binary_logloss: 0.418005\tvalid_1's binary_logloss: 0.503574\n",
      "[1100]\ttraining's binary_logloss: 0.412014\tvalid_1's binary_logloss: 0.50333\n",
      "[1200]\ttraining's binary_logloss: 0.406426\tvalid_1's binary_logloss: 0.503114\n",
      "[1300]\ttraining's binary_logloss: 0.401392\tvalid_1's binary_logloss: 0.503037\n",
      "[1400]\ttraining's binary_logloss: 0.396482\tvalid_1's binary_logloss: 0.502995\n",
      "[1500]\ttraining's binary_logloss: 0.391923\tvalid_1's binary_logloss: 0.502827\n",
      "[1600]\ttraining's binary_logloss: 0.387639\tvalid_1's binary_logloss: 0.502813\n",
      "[1700]\ttraining's binary_logloss: 0.383487\tvalid_1's binary_logloss: 0.502751\n",
      "[1800]\ttraining's binary_logloss: 0.379621\tvalid_1's binary_logloss: 0.502792\n",
      "Early stopping, best iteration is:\n",
      "[1716]\ttraining's binary_logloss: 0.382746\tvalid_1's binary_logloss: 0.502702\n",
      "logloss 0.479376\n",
      "best_score 0.700820\n",
      "best_threshold 0.630\n",
      "------------------------------\n",
      "Q1 : F1 = 0.649399\n",
      "Q2 : F1 = 0.537839\n",
      "Q3 : F1 = 0.586662\n",
      "Q4 : F1 = 0.663147\n",
      "Q5 : F1 = 0.395990\n",
      "Q6 : F1 = 0.620098\n",
      "Q7 : F1 = 0.586617\n",
      "Q8 : F1 = 0.350622\n",
      "Q9 : F1 = 0.573352\n",
      "Q10 : F1 = 0.373859\n",
      "Q11 : F1 = 0.399299\n",
      "Q12 : F1 = 0.603466\n",
      "Q13 : F1 = 0.423762\n",
      "Q14 : F1 = 0.540629\n",
      "Q15 : F1 = 0.384544\n",
      "Q16 : F1 = 0.456730\n",
      "Q17 : F1 = 0.394785\n",
      "Q18 : F1 = 0.566531\n",
      "[1 1 1]\n",
      "[1 0 1 1 0 1 0 1 1 0]\n",
      "[1 0 1 1 1]\n",
      "[1 1 1]\n",
      "[0 0 0 0 0 0 0 0 1 0]\n",
      "[0 0 1 1 1]\n",
      "[1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 0]\n",
      "[1 1 1 1 1]\n",
      "sample_inf処理時間 :  12.0 秒\n"
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    run_train()\n",
    "inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
