{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp062"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupごと推論して前のgroupの特徴量引継ぎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp062\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.01, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.prep_dir + 'cat_col_lists.pkl', 'rb') as f:\n",
    "    cat_col_lists = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTrain:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"session_id\", \"level_group\", \"elapsed_time\"], ignore_index=True)\n",
    "        self.features = self.sessions_df[[\"session_id\", \"level_group\"]].drop_duplicates().copy()\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].shift(1)\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_record_cnt\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].agg([max,min]).reset_index()\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[\"max\"] - add_features[\"min\"]\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[f\"{self.group}_group_elapsed_time\"].astype(np.float32)\n",
    "        add_features = add_features[[\"session_id\", \"level_group\", f\"{self.group}_group_elapsed_time\"]].copy()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[\"index\"].count().reset_index().rename(columns={\"index\":\"cnt\"})\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            tmp = add_features[add_features[cat_col]==cat][[\"session_id\", \"level_group\", \"cnt\"]].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp = tmp.rename(columns={\"cnt\": feat_name})\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[feat_name] = self.features[feat_name].fillna(0)\n",
    "            else:\n",
    "                self.features[feat_name] = 0\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.dropna(subset=[cat_col]).drop_duplicates([\"session_id\", \"level_group\", cat_col])\n",
    "        add_features = add_features.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_{cat_col}_nunique\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")        \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        new_cols = [f\"{self.group}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{self.group}_{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "    def get_train(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        \n",
    "        self.result = self.result.merge(self.features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesInf:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"elapsed_time\"], ignore_index=True)\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.use_cols = [\n",
    "            \"elapsed_time\", \"event_name\", \"name\", \"level\", \"page\", \"index\",\n",
    "            \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\",\n",
    "            \"hover_duration\", \"text\", \"fqid\", \"room_fqid\", \"text_fqid\"\n",
    "        ]\n",
    "\n",
    "    def _prep(self):\n",
    "        # dataframeの各列をnumpy arrayで保持\n",
    "        self.sessions = {}\n",
    "        for c in self.use_cols:\n",
    "            self.sessions[c] = self.sessions_df[c].values\n",
    "        self.sessions[\"time_diff\"] = self.sessions[\"elapsed_time\"] - self.sessions_df[\"elapsed_time\"].shift(1).values\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_feature = len(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_record_cnt\"] = add_feature\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_feature = np.max(self.sessions[\"elapsed_time\"]) - np.min(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_group_elapsed_time\"] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            add_feature = (self.sessions[cat_col] == cat).astype(int).sum()\n",
    "            self.result[feat_name] = add_feature\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        self.result[f\"{self.group}_{cat_col}_nunique\"] = self.sessions_df[cat_col].dropna().nunique()       \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        for val_col, agg in itertools.product(val_cols, aggs):\n",
    "            feat_name = f\"{self.group}_{val_col}_{agg}\"\n",
    "            if agg == \"mean\":\n",
    "                add_feature = np.nanmean(self.sessions[val_col])\n",
    "            elif agg == \"max\":\n",
    "                add_feature = np.nanmax(self.sessions[val_col])\n",
    "            elif agg == \"min\":\n",
    "                add_feature = np.nanmin(self.sessions[val_col])\n",
    "            elif agg == \"std\":\n",
    "                add_feature = np.nanstd(self.sessions[val_col], ddof=1)\n",
    "            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    tmp = self.sessions[val_col][idx]\n",
    "                    if agg == \"mean\":\n",
    "                        add_feature = np.nanmean(tmp)\n",
    "                    elif agg == \"max\":\n",
    "                        add_feature = np.nanmax(tmp)\n",
    "                    elif agg == \"min\":\n",
    "                        add_feature = np.nanmin(tmp)\n",
    "                    elif agg == \"std\":\n",
    "                        add_feature = np.nanstd(tmp, ddof=1)\n",
    "                    if np.isnan(add_feature):\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def get_test(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesTrain(sessions, labels)\n",
    "    train = feat.get_train()\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesInf(sessions, labels)\n",
    "    test = feat.get_test()\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "        gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "        fis = []\n",
    "        \n",
    "        for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "            print(f\"fold : {i}\")\n",
    "            tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "            vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "            tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "            model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                            num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)\n",
    "            # モデル出力\n",
    "            model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\")\n",
    "        \n",
    "            # valid_pred\n",
    "            oof_fold = train.iloc[vl_idx].copy()\n",
    "            oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "            oofs.append(oof_fold)\n",
    "\n",
    "            # 特徴量重要度\n",
    "            fi_fold = pd.DataFrame()\n",
    "            fi_fold[\"feature\"] = model.feature_name()\n",
    "            fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "            fi_fold[\"fold\"] = i\n",
    "            fis.append(fi_fold)\n",
    "\n",
    "        fi = pd.concat(fis)    \n",
    "        fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "        fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "        fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi_{group}.csv\", index=False)\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        \n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"session\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby([\"session_id\", \"level_group\"])]\n",
    "    subs = [df[1].drop(columns=[\"session\", \"session_level\"]).reset_index(drop=True) for df in sub.groupby([\"session\", \"level_group\"])]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    model_dict = {}\n",
    "    features_dict = {}\n",
    "    for g in level_group_list:\n",
    "        if mode == \"local_cv\":\n",
    "            model_paths = [cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_paths = [f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        model_dict[g] = [lgb.Booster(model_file=p) for p in model_paths]\n",
    "        features_dict[g] = model_dict[g][0].feature_name()\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "        features = features_dict[level_group]\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = model_dict[level_group][i]\n",
    "            preds += model.predict(test[features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49821, number of negative: 6726\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 177809\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 2557\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.881055 -> initscore=2.002456\n",
      "[LightGBM] [Info] Start training from score 2.002456\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.288716\tvalid_1's binary_logloss: 0.310467\n",
      "[200]\ttraining's binary_logloss: 0.256606\tvalid_1's binary_logloss: 0.289162\n",
      "[300]\ttraining's binary_logloss: 0.235774\tvalid_1's binary_logloss: 0.278454\n",
      "[400]\ttraining's binary_logloss: 0.221846\tvalid_1's binary_logloss: 0.273865\n",
      "[500]\ttraining's binary_logloss: 0.210123\tvalid_1's binary_logloss: 0.271639\n",
      "[600]\ttraining's binary_logloss: 0.19989\tvalid_1's binary_logloss: 0.270453\n",
      "[700]\ttraining's binary_logloss: 0.191061\tvalid_1's binary_logloss: 0.269901\n",
      "[800]\ttraining's binary_logloss: 0.182673\tvalid_1's binary_logloss: 0.269707\n",
      "Early stopping, best iteration is:\n",
      "[794]\ttraining's binary_logloss: 0.183182\tvalid_1's binary_logloss: 0.269683\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49692, number of negative: 6855\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 177723\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 2557\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.878773 -> initscore=1.980866\n",
      "[LightGBM] [Info] Start training from score 1.980866\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.291618\tvalid_1's binary_logloss: 0.298001\n",
      "[200]\ttraining's binary_logloss: 0.258872\tvalid_1's binary_logloss: 0.278\n",
      "[300]\ttraining's binary_logloss: 0.237703\tvalid_1's binary_logloss: 0.268073\n",
      "[400]\ttraining's binary_logloss: 0.223472\tvalid_1's binary_logloss: 0.264119\n",
      "[500]\ttraining's binary_logloss: 0.211577\tvalid_1's binary_logloss: 0.26222\n",
      "[600]\ttraining's binary_logloss: 0.201309\tvalid_1's binary_logloss: 0.261433\n",
      "[700]\ttraining's binary_logloss: 0.192321\tvalid_1's binary_logloss: 0.261042\n",
      "[800]\ttraining's binary_logloss: 0.183964\tvalid_1's binary_logloss: 0.260686\n",
      "[900]\ttraining's binary_logloss: 0.176618\tvalid_1's binary_logloss: 0.260644\n",
      "Early stopping, best iteration is:\n",
      "[846]\ttraining's binary_logloss: 0.180489\tvalid_1's binary_logloss: 0.260565\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49772, number of negative: 6778\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 177872\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 2557\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880141 -> initscore=1.993771\n",
      "[LightGBM] [Info] Start training from score 1.993771\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.290473\tvalid_1's binary_logloss: 0.303342\n",
      "[200]\ttraining's binary_logloss: 0.258326\tvalid_1's binary_logloss: 0.281447\n",
      "[300]\ttraining's binary_logloss: 0.237457\tvalid_1's binary_logloss: 0.270317\n",
      "[400]\ttraining's binary_logloss: 0.223425\tvalid_1's binary_logloss: 0.26568\n",
      "[500]\ttraining's binary_logloss: 0.211748\tvalid_1's binary_logloss: 0.263279\n",
      "[600]\ttraining's binary_logloss: 0.20151\tvalid_1's binary_logloss: 0.261859\n",
      "[700]\ttraining's binary_logloss: 0.19269\tvalid_1's binary_logloss: 0.261122\n",
      "[800]\ttraining's binary_logloss: 0.184374\tvalid_1's binary_logloss: 0.260804\n",
      "[900]\ttraining's binary_logloss: 0.176953\tvalid_1's binary_logloss: 0.260642\n",
      "[1000]\ttraining's binary_logloss: 0.170252\tvalid_1's binary_logloss: 0.260741\n",
      "Early stopping, best iteration is:\n",
      "[915]\ttraining's binary_logloss: 0.175867\tvalid_1's binary_logloss: 0.260564\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49755, number of negative: 6795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.333254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 177829\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 2557\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.879841 -> initscore=1.990924\n",
      "[LightGBM] [Info] Start training from score 1.990924\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.290137\tvalid_1's binary_logloss: 0.305301\n",
      "[200]\ttraining's binary_logloss: 0.257968\tvalid_1's binary_logloss: 0.285619\n",
      "[300]\ttraining's binary_logloss: 0.237106\tvalid_1's binary_logloss: 0.275776\n",
      "[400]\ttraining's binary_logloss: 0.223112\tvalid_1's binary_logloss: 0.272119\n",
      "[500]\ttraining's binary_logloss: 0.211446\tvalid_1's binary_logloss: 0.270158\n",
      "[600]\ttraining's binary_logloss: 0.201213\tvalid_1's binary_logloss: 0.2692\n",
      "[700]\ttraining's binary_logloss: 0.192388\tvalid_1's binary_logloss: 0.268832\n",
      "[800]\ttraining's binary_logloss: 0.184111\tvalid_1's binary_logloss: 0.26873\n",
      "[900]\ttraining's binary_logloss: 0.176765\tvalid_1's binary_logloss: 0.268993\n",
      "Early stopping, best iteration is:\n",
      "[806]\ttraining's binary_logloss: 0.18363\tvalid_1's binary_logloss: 0.26871\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49804, number of negative: 6746\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 177771\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 2557\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880707 -> initscore=1.999146\n",
      "[LightGBM] [Info] Start training from score 1.999146\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.290113\tvalid_1's binary_logloss: 0.307072\n",
      "[200]\ttraining's binary_logloss: 0.258429\tvalid_1's binary_logloss: 0.284737\n",
      "[300]\ttraining's binary_logloss: 0.23778\tvalid_1's binary_logloss: 0.273243\n",
      "[400]\ttraining's binary_logloss: 0.223822\tvalid_1's binary_logloss: 0.268363\n",
      "[500]\ttraining's binary_logloss: 0.21222\tvalid_1's binary_logloss: 0.265668\n",
      "[600]\ttraining's binary_logloss: 0.201991\tvalid_1's binary_logloss: 0.264125\n",
      "[700]\ttraining's binary_logloss: 0.193166\tvalid_1's binary_logloss: 0.263443\n",
      "[800]\ttraining's binary_logloss: 0.184812\tvalid_1's binary_logloss: 0.263136\n",
      "[900]\ttraining's binary_logloss: 0.177458\tvalid_1's binary_logloss: 0.262998\n",
      "Early stopping, best iteration is:\n",
      "[870]\ttraining's binary_logloss: 0.179592\tvalid_1's binary_logloss: 0.262957\n",
      "5-12\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.351744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 470954\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.573264\tvalid_1's binary_logloss: 0.581593\n",
      "[200]\ttraining's binary_logloss: 0.54276\tvalid_1's binary_logloss: 0.556645\n",
      "[300]\ttraining's binary_logloss: 0.525053\tvalid_1's binary_logloss: 0.544539\n",
      "[400]\ttraining's binary_logloss: 0.513904\tvalid_1's binary_logloss: 0.539135\n",
      "[500]\ttraining's binary_logloss: 0.505453\tvalid_1's binary_logloss: 0.536256\n",
      "[600]\ttraining's binary_logloss: 0.498522\tvalid_1's binary_logloss: 0.534768\n",
      "[700]\ttraining's binary_logloss: 0.492348\tvalid_1's binary_logloss: 0.533862\n",
      "[800]\ttraining's binary_logloss: 0.486788\tvalid_1's binary_logloss: 0.533202\n",
      "[900]\ttraining's binary_logloss: 0.481723\tvalid_1's binary_logloss: 0.532757\n",
      "[1000]\ttraining's binary_logloss: 0.477125\tvalid_1's binary_logloss: 0.532489\n",
      "[1100]\ttraining's binary_logloss: 0.472839\tvalid_1's binary_logloss: 0.532207\n",
      "[1200]\ttraining's binary_logloss: 0.468867\tvalid_1's binary_logloss: 0.531922\n",
      "[1300]\ttraining's binary_logloss: 0.465138\tvalid_1's binary_logloss: 0.531745\n",
      "[1400]\ttraining's binary_logloss: 0.461617\tvalid_1's binary_logloss: 0.531686\n",
      "[1500]\ttraining's binary_logloss: 0.458289\tvalid_1's binary_logloss: 0.531524\n",
      "[1600]\ttraining's binary_logloss: 0.455159\tvalid_1's binary_logloss: 0.531395\n",
      "[1700]\ttraining's binary_logloss: 0.452171\tvalid_1's binary_logloss: 0.531343\n",
      "Early stopping, best iteration is:\n",
      "[1679]\ttraining's binary_logloss: 0.452756\tvalid_1's binary_logloss: 0.531342\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122423, number of negative: 66067\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.433394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 470574\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 5175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649493 -> initscore=0.616813\n",
      "[LightGBM] [Info] Start training from score 0.616813\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.57334\tvalid_1's binary_logloss: 0.580495\n",
      "[200]\ttraining's binary_logloss: 0.542425\tvalid_1's binary_logloss: 0.556724\n",
      "[300]\ttraining's binary_logloss: 0.524611\tvalid_1's binary_logloss: 0.54559\n",
      "[400]\ttraining's binary_logloss: 0.51335\tvalid_1's binary_logloss: 0.540648\n",
      "[500]\ttraining's binary_logloss: 0.504891\tvalid_1's binary_logloss: 0.538096\n",
      "[600]\ttraining's binary_logloss: 0.497983\tvalid_1's binary_logloss: 0.536813\n",
      "[700]\ttraining's binary_logloss: 0.491839\tvalid_1's binary_logloss: 0.536019\n",
      "[800]\ttraining's binary_logloss: 0.486316\tvalid_1's binary_logloss: 0.535464\n",
      "[900]\ttraining's binary_logloss: 0.481256\tvalid_1's binary_logloss: 0.535142\n",
      "[1000]\ttraining's binary_logloss: 0.476599\tvalid_1's binary_logloss: 0.534815\n",
      "[1100]\ttraining's binary_logloss: 0.472264\tvalid_1's binary_logloss: 0.534629\n",
      "[1200]\ttraining's binary_logloss: 0.468299\tvalid_1's binary_logloss: 0.534427\n",
      "[1300]\ttraining's binary_logloss: 0.46453\tvalid_1's binary_logloss: 0.534394\n",
      "[1400]\ttraining's binary_logloss: 0.461042\tvalid_1's binary_logloss: 0.534347\n",
      "[1500]\ttraining's binary_logloss: 0.457804\tvalid_1's binary_logloss: 0.534297\n",
      "[1600]\ttraining's binary_logloss: 0.454652\tvalid_1's binary_logloss: 0.534292\n",
      "Early stopping, best iteration is:\n",
      "[1546]\ttraining's binary_logloss: 0.456342\tvalid_1's binary_logloss: 0.53427\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122249, number of negative: 66251\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.618260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 470868\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 5175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.648536 -> initscore=0.612609\n",
      "[LightGBM] [Info] Start training from score 0.612609\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.573717\tvalid_1's binary_logloss: 0.578623\n",
      "[200]\ttraining's binary_logloss: 0.542801\tvalid_1's binary_logloss: 0.555186\n",
      "[300]\ttraining's binary_logloss: 0.525031\tvalid_1's binary_logloss: 0.543943\n",
      "[400]\ttraining's binary_logloss: 0.513823\tvalid_1's binary_logloss: 0.539039\n",
      "[500]\ttraining's binary_logloss: 0.505417\tvalid_1's binary_logloss: 0.536417\n",
      "[600]\ttraining's binary_logloss: 0.498538\tvalid_1's binary_logloss: 0.535218\n",
      "[700]\ttraining's binary_logloss: 0.492444\tvalid_1's binary_logloss: 0.534434\n",
      "[800]\ttraining's binary_logloss: 0.486964\tvalid_1's binary_logloss: 0.53394\n",
      "[900]\ttraining's binary_logloss: 0.481868\tvalid_1's binary_logloss: 0.533429\n",
      "[1000]\ttraining's binary_logloss: 0.477282\tvalid_1's binary_logloss: 0.53324\n",
      "[1100]\ttraining's binary_logloss: 0.473005\tvalid_1's binary_logloss: 0.533003\n",
      "[1200]\ttraining's binary_logloss: 0.469033\tvalid_1's binary_logloss: 0.532782\n",
      "[1300]\ttraining's binary_logloss: 0.465312\tvalid_1's binary_logloss: 0.532631\n",
      "[1400]\ttraining's binary_logloss: 0.461862\tvalid_1's binary_logloss: 0.532418\n",
      "[1500]\ttraining's binary_logloss: 0.458573\tvalid_1's binary_logloss: 0.532342\n",
      "[1600]\ttraining's binary_logloss: 0.455456\tvalid_1's binary_logloss: 0.532235\n",
      "[1700]\ttraining's binary_logloss: 0.452602\tvalid_1's binary_logloss: 0.532213\n",
      "Early stopping, best iteration is:\n",
      "[1691]\ttraining's binary_logloss: 0.452883\tvalid_1's binary_logloss: 0.532184\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122566, number of negative: 65934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.681215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 470891\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 5174\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650218 -> initscore=0.619995\n",
      "[LightGBM] [Info] Start training from score 0.619995\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572482\tvalid_1's binary_logloss: 0.580321\n",
      "[200]\ttraining's binary_logloss: 0.540065\tvalid_1's binary_logloss: 0.553516\n",
      "[300]\ttraining's binary_logloss: 0.524812\tvalid_1's binary_logloss: 0.544061\n",
      "[400]\ttraining's binary_logloss: 0.514004\tvalid_1's binary_logloss: 0.538861\n",
      "[500]\ttraining's binary_logloss: 0.505646\tvalid_1's binary_logloss: 0.53595\n",
      "[600]\ttraining's binary_logloss: 0.498711\tvalid_1's binary_logloss: 0.534298\n",
      "[700]\ttraining's binary_logloss: 0.492663\tvalid_1's binary_logloss: 0.533297\n",
      "[800]\ttraining's binary_logloss: 0.487172\tvalid_1's binary_logloss: 0.532675\n",
      "[900]\ttraining's binary_logloss: 0.482125\tvalid_1's binary_logloss: 0.532173\n",
      "[1000]\ttraining's binary_logloss: 0.477547\tvalid_1's binary_logloss: 0.531814\n",
      "[1100]\ttraining's binary_logloss: 0.473267\tvalid_1's binary_logloss: 0.531604\n",
      "[1200]\ttraining's binary_logloss: 0.469314\tvalid_1's binary_logloss: 0.531364\n",
      "[1300]\ttraining's binary_logloss: 0.465699\tvalid_1's binary_logloss: 0.531125\n",
      "[1400]\ttraining's binary_logloss: 0.462293\tvalid_1's binary_logloss: 0.530937\n",
      "[1500]\ttraining's binary_logloss: 0.458979\tvalid_1's binary_logloss: 0.530757\n",
      "[1600]\ttraining's binary_logloss: 0.455836\tvalid_1's binary_logloss: 0.53066\n",
      "[1700]\ttraining's binary_logloss: 0.452915\tvalid_1's binary_logloss: 0.530642\n",
      "[1800]\ttraining's binary_logloss: 0.450177\tvalid_1's binary_logloss: 0.530588\n",
      "[1900]\ttraining's binary_logloss: 0.447393\tvalid_1's binary_logloss: 0.530564\n",
      "[2000]\ttraining's binary_logloss: 0.444709\tvalid_1's binary_logloss: 0.530525\n",
      "[2100]\ttraining's binary_logloss: 0.442356\tvalid_1's binary_logloss: 0.53047\n",
      "[2200]\ttraining's binary_logloss: 0.439949\tvalid_1's binary_logloss: 0.53046\n",
      "Early stopping, best iteration is:\n",
      "[2117]\ttraining's binary_logloss: 0.44188\tvalid_1's binary_logloss: 0.530455\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122635, number of negative: 65865\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.716991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 470686\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 5175\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650584 -> initscore=0.621605\n",
      "[LightGBM] [Info] Start training from score 0.621605\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572994\tvalid_1's binary_logloss: 0.581926\n",
      "[200]\ttraining's binary_logloss: 0.542347\tvalid_1's binary_logloss: 0.557475\n",
      "[300]\ttraining's binary_logloss: 0.524637\tvalid_1's binary_logloss: 0.545444\n",
      "[400]\ttraining's binary_logloss: 0.513495\tvalid_1's binary_logloss: 0.540006\n",
      "[500]\ttraining's binary_logloss: 0.505046\tvalid_1's binary_logloss: 0.537014\n",
      "[600]\ttraining's binary_logloss: 0.498098\tvalid_1's binary_logloss: 0.535591\n",
      "[700]\ttraining's binary_logloss: 0.491972\tvalid_1's binary_logloss: 0.534522\n",
      "[800]\ttraining's binary_logloss: 0.486452\tvalid_1's binary_logloss: 0.533859\n",
      "[900]\ttraining's binary_logloss: 0.481398\tvalid_1's binary_logloss: 0.533399\n",
      "[1000]\ttraining's binary_logloss: 0.4768\tvalid_1's binary_logloss: 0.533098\n",
      "[1100]\ttraining's binary_logloss: 0.472533\tvalid_1's binary_logloss: 0.53296\n",
      "[1200]\ttraining's binary_logloss: 0.468611\tvalid_1's binary_logloss: 0.532818\n",
      "[1300]\ttraining's binary_logloss: 0.464899\tvalid_1's binary_logloss: 0.532759\n",
      "[1400]\ttraining's binary_logloss: 0.461476\tvalid_1's binary_logloss: 0.53276\n",
      "[1500]\ttraining's binary_logloss: 0.458099\tvalid_1's binary_logloss: 0.532536\n",
      "[1600]\ttraining's binary_logloss: 0.454975\tvalid_1's binary_logloss: 0.532506\n",
      "[1700]\ttraining's binary_logloss: 0.452022\tvalid_1's binary_logloss: 0.532491\n",
      "[1800]\ttraining's binary_logloss: 0.449106\tvalid_1's binary_logloss: 0.532413\n",
      "[1900]\ttraining's binary_logloss: 0.446468\tvalid_1's binary_logloss: 0.532408\n",
      "[2000]\ttraining's binary_logloss: 0.443865\tvalid_1's binary_logloss: 0.532384\n",
      "Early stopping, best iteration is:\n",
      "[1959]\ttraining's binary_logloss: 0.444926\tvalid_1's binary_logloss: 0.53237\n",
      "13-22\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67313, number of negative: 26932\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.528935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 846026\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 7838\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.714234 -> initscore=0.916038\n",
      "[LightGBM] [Info] Start training from score 0.916038\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.533914\tvalid_1's binary_logloss: 0.551865\n",
      "[200]\ttraining's binary_logloss: 0.502308\tvalid_1's binary_logloss: 0.529541\n",
      "[300]\ttraining's binary_logloss: 0.481705\tvalid_1's binary_logloss: 0.518245\n",
      "[400]\ttraining's binary_logloss: 0.466361\tvalid_1's binary_logloss: 0.512046\n",
      "[500]\ttraining's binary_logloss: 0.454983\tvalid_1's binary_logloss: 0.50937\n",
      "[600]\ttraining's binary_logloss: 0.44476\tvalid_1's binary_logloss: 0.507626\n",
      "[700]\ttraining's binary_logloss: 0.435749\tvalid_1's binary_logloss: 0.506632\n",
      "[800]\ttraining's binary_logloss: 0.427647\tvalid_1's binary_logloss: 0.506111\n",
      "[900]\ttraining's binary_logloss: 0.420194\tvalid_1's binary_logloss: 0.505659\n",
      "[1000]\ttraining's binary_logloss: 0.413614\tvalid_1's binary_logloss: 0.505456\n",
      "[1100]\ttraining's binary_logloss: 0.407561\tvalid_1's binary_logloss: 0.505367\n",
      "[1200]\ttraining's binary_logloss: 0.401826\tvalid_1's binary_logloss: 0.50522\n",
      "[1300]\ttraining's binary_logloss: 0.396609\tvalid_1's binary_logloss: 0.505148\n",
      "[1400]\ttraining's binary_logloss: 0.391508\tvalid_1's binary_logloss: 0.505042\n",
      "[1500]\ttraining's binary_logloss: 0.386827\tvalid_1's binary_logloss: 0.505025\n",
      "Early stopping, best iteration is:\n",
      "[1453]\ttraining's binary_logloss: 0.388886\tvalid_1's binary_logloss: 0.505009\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 66951, number of negative: 27294\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.566897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845748\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 7838\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710393 -> initscore=0.897294\n",
      "[LightGBM] [Info] Start training from score 0.897294\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.536892\tvalid_1's binary_logloss: 0.540207\n",
      "[200]\ttraining's binary_logloss: 0.505091\tvalid_1's binary_logloss: 0.519094\n",
      "[300]\ttraining's binary_logloss: 0.484269\tvalid_1's binary_logloss: 0.508235\n",
      "[400]\ttraining's binary_logloss: 0.469022\tvalid_1's binary_logloss: 0.502311\n",
      "[500]\ttraining's binary_logloss: 0.457636\tvalid_1's binary_logloss: 0.499646\n",
      "[600]\ttraining's binary_logloss: 0.447397\tvalid_1's binary_logloss: 0.497939\n",
      "[700]\ttraining's binary_logloss: 0.43843\tvalid_1's binary_logloss: 0.496945\n",
      "[800]\ttraining's binary_logloss: 0.430533\tvalid_1's binary_logloss: 0.496562\n",
      "[900]\ttraining's binary_logloss: 0.423129\tvalid_1's binary_logloss: 0.496256\n",
      "[1000]\ttraining's binary_logloss: 0.416581\tvalid_1's binary_logloss: 0.495989\n",
      "[1100]\ttraining's binary_logloss: 0.410347\tvalid_1's binary_logloss: 0.495795\n",
      "[1200]\ttraining's binary_logloss: 0.404564\tvalid_1's binary_logloss: 0.495548\n",
      "[1300]\ttraining's binary_logloss: 0.399132\tvalid_1's binary_logloss: 0.495307\n",
      "[1400]\ttraining's binary_logloss: 0.393878\tvalid_1's binary_logloss: 0.495125\n",
      "[1500]\ttraining's binary_logloss: 0.389176\tvalid_1's binary_logloss: 0.495059\n",
      "[1600]\ttraining's binary_logloss: 0.384518\tvalid_1's binary_logloss: 0.495034\n",
      "[1700]\ttraining's binary_logloss: 0.380465\tvalid_1's binary_logloss: 0.494976\n",
      "Early stopping, best iteration is:\n",
      "[1686]\ttraining's binary_logloss: 0.381153\tvalid_1's binary_logloss: 0.49496\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 66989, number of negative: 27261\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.999292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845997\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 7838\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710759 -> initscore=0.899071\n",
      "[LightGBM] [Info] Start training from score 0.899071\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.542204\tvalid_1's binary_logloss: 0.542725\n",
      "[200]\ttraining's binary_logloss: 0.512905\tvalid_1's binary_logloss: 0.521467\n",
      "[300]\ttraining's binary_logloss: 0.493654\tvalid_1's binary_logloss: 0.510597\n",
      "[400]\ttraining's binary_logloss: 0.479518\tvalid_1's binary_logloss: 0.504906\n",
      "[500]\ttraining's binary_logloss: 0.468712\tvalid_1's binary_logloss: 0.502232\n",
      "[600]\ttraining's binary_logloss: 0.459029\tvalid_1's binary_logloss: 0.500636\n",
      "[700]\ttraining's binary_logloss: 0.450734\tvalid_1's binary_logloss: 0.499766\n",
      "[800]\ttraining's binary_logloss: 0.443109\tvalid_1's binary_logloss: 0.499147\n",
      "[900]\ttraining's binary_logloss: 0.436228\tvalid_1's binary_logloss: 0.498716\n",
      "[1000]\ttraining's binary_logloss: 0.430071\tvalid_1's binary_logloss: 0.498509\n",
      "[1100]\ttraining's binary_logloss: 0.42431\tvalid_1's binary_logloss: 0.498121\n",
      "[1200]\ttraining's binary_logloss: 0.41905\tvalid_1's binary_logloss: 0.497999\n",
      "[1300]\ttraining's binary_logloss: 0.414083\tvalid_1's binary_logloss: 0.497791\n",
      "[1400]\ttraining's binary_logloss: 0.409295\tvalid_1's binary_logloss: 0.497631\n",
      "[1500]\ttraining's binary_logloss: 0.404864\tvalid_1's binary_logloss: 0.497447\n",
      "[1600]\ttraining's binary_logloss: 0.400692\tvalid_1's binary_logloss: 0.497372\n",
      "[1700]\ttraining's binary_logloss: 0.396895\tvalid_1's binary_logloss: 0.49731\n",
      "[1800]\ttraining's binary_logloss: 0.393202\tvalid_1's binary_logloss: 0.497255\n",
      "[1900]\ttraining's binary_logloss: 0.389785\tvalid_1's binary_logloss: 0.497164\n",
      "[2000]\ttraining's binary_logloss: 0.386393\tvalid_1's binary_logloss: 0.497004\n",
      "[2100]\ttraining's binary_logloss: 0.383354\tvalid_1's binary_logloss: 0.496965\n",
      "[2200]\ttraining's binary_logloss: 0.380322\tvalid_1's binary_logloss: 0.496983\n",
      "Early stopping, best iteration is:\n",
      "[2144]\ttraining's binary_logloss: 0.381924\tvalid_1's binary_logloss: 0.49693\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67214, number of negative: 27036\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.364793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845996\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 7837\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.713146 -> initscore=0.910712\n",
      "[LightGBM] [Info] Start training from score 0.910712\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.53347\tvalid_1's binary_logloss: 0.548875\n",
      "[200]\ttraining's binary_logloss: 0.50097\tvalid_1's binary_logloss: 0.526883\n",
      "[300]\ttraining's binary_logloss: 0.481663\tvalid_1's binary_logloss: 0.517508\n",
      "[400]\ttraining's binary_logloss: 0.466894\tvalid_1's binary_logloss: 0.511922\n",
      "[500]\ttraining's binary_logloss: 0.455548\tvalid_1's binary_logloss: 0.509448\n",
      "[600]\ttraining's binary_logloss: 0.445155\tvalid_1's binary_logloss: 0.507698\n",
      "[700]\ttraining's binary_logloss: 0.436348\tvalid_1's binary_logloss: 0.506883\n",
      "[800]\ttraining's binary_logloss: 0.428411\tvalid_1's binary_logloss: 0.50641\n",
      "[900]\ttraining's binary_logloss: 0.421122\tvalid_1's binary_logloss: 0.506024\n",
      "[1000]\ttraining's binary_logloss: 0.414432\tvalid_1's binary_logloss: 0.505879\n",
      "[1100]\ttraining's binary_logloss: 0.408196\tvalid_1's binary_logloss: 0.505746\n",
      "[1200]\ttraining's binary_logloss: 0.40256\tvalid_1's binary_logloss: 0.505651\n",
      "[1300]\ttraining's binary_logloss: 0.397148\tvalid_1's binary_logloss: 0.505593\n",
      "Early stopping, best iteration is:\n",
      "[1263]\ttraining's binary_logloss: 0.399085\tvalid_1's binary_logloss: 0.505523\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67245, number of negative: 27005\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.339446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845867\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 7838\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.713475 -> initscore=0.912321\n",
      "[LightGBM] [Info] Start training from score 0.912321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.535139\tvalid_1's binary_logloss: 0.549862\n",
      "[200]\ttraining's binary_logloss: 0.503685\tvalid_1's binary_logloss: 0.528011\n",
      "[300]\ttraining's binary_logloss: 0.483351\tvalid_1's binary_logloss: 0.516932\n",
      "[400]\ttraining's binary_logloss: 0.468271\tvalid_1's binary_logloss: 0.510765\n",
      "[500]\ttraining's binary_logloss: 0.456973\tvalid_1's binary_logloss: 0.508073\n",
      "[600]\ttraining's binary_logloss: 0.446853\tvalid_1's binary_logloss: 0.506189\n",
      "[700]\ttraining's binary_logloss: 0.43818\tvalid_1's binary_logloss: 0.505185\n",
      "[800]\ttraining's binary_logloss: 0.430392\tvalid_1's binary_logloss: 0.504494\n",
      "[900]\ttraining's binary_logloss: 0.42291\tvalid_1's binary_logloss: 0.503956\n",
      "[1000]\ttraining's binary_logloss: 0.416426\tvalid_1's binary_logloss: 0.503769\n",
      "[1100]\ttraining's binary_logloss: 0.410451\tvalid_1's binary_logloss: 0.503567\n",
      "[1200]\ttraining's binary_logloss: 0.404819\tvalid_1's binary_logloss: 0.503473\n",
      "[1300]\ttraining's binary_logloss: 0.399516\tvalid_1's binary_logloss: 0.503371\n",
      "[1400]\ttraining's binary_logloss: 0.394611\tvalid_1's binary_logloss: 0.503194\n",
      "[1500]\ttraining's binary_logloss: 0.39003\tvalid_1's binary_logloss: 0.503162\n",
      "[1600]\ttraining's binary_logloss: 0.385675\tvalid_1's binary_logloss: 0.503049\n",
      "[1700]\ttraining's binary_logloss: 0.381331\tvalid_1's binary_logloss: 0.502914\n",
      "[1800]\ttraining's binary_logloss: 0.377382\tvalid_1's binary_logloss: 0.502939\n",
      "Early stopping, best iteration is:\n",
      "[1711]\ttraining's binary_logloss: 0.380835\tvalid_1's binary_logloss: 0.502861\n",
      "logloss 0.478890\n",
      "best_score 0.698002\n",
      "best_threshold 0.630\n",
      "------------------------------\n",
      "Q1 : F1 = 0.634497\n",
      "Q2 : F1 = 0.545445\n",
      "Q3 : F1 = 0.586155\n",
      "Q4 : F1 = 0.658797\n",
      "Q5 : F1 = 0.398356\n",
      "Q6 : F1 = 0.624552\n",
      "Q7 : F1 = 0.581788\n",
      "Q8 : F1 = 0.347973\n",
      "Q9 : F1 = 0.579821\n",
      "Q10 : F1 = 0.357744\n",
      "Q11 : F1 = 0.403997\n",
      "Q12 : F1 = 0.594024\n",
      "Q13 : F1 = 0.421570\n",
      "Q14 : F1 = 0.524498\n",
      "Q15 : F1 = 0.381308\n",
      "Q16 : F1 = 0.444637\n",
      "Q17 : F1 = 0.388124\n",
      "Q18 : F1 = 0.558136\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlocal_cv\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     run_train()\n\u001b[0;32m----> 3\u001b[0m inference(cfg\u001b[39m.\u001b[39;49mmode)\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36minference\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m (test_sessions, sample_submission) \u001b[39min\u001b[39;00m iter_test:\n\u001b[1;32m     22\u001b[0m     level_group \u001b[39m=\u001b[39m test_sessions[\u001b[39m\"\u001b[39m\u001b[39mlevel_group\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m     test \u001b[39m=\u001b[39m get_test_dataset(test_sessions, sample_submission)\n\u001b[1;32m     24\u001b[0m     features \u001b[39m=\u001b[39m features_dict[level_group]\n\u001b[1;32m     25\u001b[0m     preds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(test))\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mget_test_dataset\u001b[0;34m(sessions, labels)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# 特徴量生成\u001b[39;00m\n\u001b[1;32m     17\u001b[0m feat \u001b[39m=\u001b[39m FeaturesInf(sessions, labels)\n\u001b[0;32m---> 18\u001b[0m test \u001b[39m=\u001b[39m feat\u001b[39m.\u001b[39;49mget_test()\n\u001b[1;32m     19\u001b[0m test[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m test[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m test\n",
      "Cell \u001b[0;32mIn[8], line 126\u001b[0m, in \u001b[0;36mFeaturesInf.get_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cat_agg_features(val_cols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39melapsed_time\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    120\u001b[0m                        aggs\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    121\u001b[0m                        cat_col\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfqid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cat_agg_features(val_cols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtime_diff\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    124\u001b[0m                        aggs\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    125\u001b[0m                        cat_col\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext_fqid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cat_agg_features(val_cols\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39melapsed_time\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    127\u001b[0m                        aggs\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    128\u001b[0m                        cat_col\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext_fqid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    130\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cat_agg_features(val_cols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtime_diff\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    131\u001b[0m                        aggs\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    132\u001b[0m                        cat_col\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cat_agg_features(val_cols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39melapsed_time\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    134\u001b[0m                        aggs\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    135\u001b[0m                        cat_col\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 74\u001b[0m, in \u001b[0;36mFeaturesInf._cat_agg_features\u001b[0;34m(self, val_cols, aggs, cat_col, not_use_cats)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m val_col, agg \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mproduct(val_cols, aggs):\n\u001b[1;32m     73\u001b[0m     feat_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mcat_col\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mcat\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mval_col\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00magg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 74\u001b[0m     tmp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msessions[val_col][idx]\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m agg \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     76\u001b[0m         add_feature \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmean(tmp)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'index'"
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    run_train()\n",
    "inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
