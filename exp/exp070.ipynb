{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp070"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp064を単一モデルで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp070\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.01, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.prep_dir + 'cat_col_lists.pkl', 'rb') as f:\n",
    "    cat_col_lists = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTrain:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"session_id\", \"level_group\", \"elapsed_time\"], ignore_index=True)\n",
    "        self.features = self.sessions_df[[\"session_id\", \"level_group\"]].drop_duplicates().copy()\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].shift(1)\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_record_cnt\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].agg([max,min]).reset_index()\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[\"max\"] - add_features[\"min\"]\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[f\"{self.group}_group_elapsed_time\"].astype(np.float32)\n",
    "        add_features = add_features[[\"session_id\", \"level_group\", f\"{self.group}_group_elapsed_time\"]].copy()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[\"index\"].count().reset_index().rename(columns={\"index\":\"cnt\"})\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            tmp = add_features[add_features[cat_col]==cat][[\"session_id\", \"level_group\", \"cnt\"]].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp = tmp.rename(columns={\"cnt\": feat_name})\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[feat_name] = self.features[feat_name].fillna(0)\n",
    "            else:\n",
    "                self.features[feat_name] = 0\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.dropna(subset=[cat_col]).drop_duplicates([\"session_id\", \"level_group\", cat_col])\n",
    "        add_features = add_features.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_{cat_col}_nunique\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")        \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        new_cols = [f\"{self.group}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{self.group}_{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "    def get_train(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "        \n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        \n",
    "        self.result = self.result.merge(self.features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesInf:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"elapsed_time\"], ignore_index=True)\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.use_cols = [\n",
    "            \"elapsed_time\", \"event_name\", \"name\", \"level\", \"page\", \"index\",\n",
    "            \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\",\n",
    "            \"hover_duration\", \"text\", \"fqid\", \"room_fqid\", \"text_fqid\"\n",
    "        ]\n",
    "\n",
    "    def _prep(self):\n",
    "        # dataframeの各列をnumpy arrayで保持\n",
    "        self.sessions = {}\n",
    "        for c in self.use_cols:\n",
    "            self.sessions[c] = self.sessions_df[c].values\n",
    "        self.sessions[\"time_diff\"] = self.sessions[\"elapsed_time\"] - self.sessions_df[\"elapsed_time\"].shift(1).values\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_feature = len(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_record_cnt\"] = add_feature\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_feature = np.max(self.sessions[\"elapsed_time\"]) - np.min(self.sessions[\"elapsed_time\"])\n",
    "        self.result[f\"{self.group}_group_elapsed_time\"] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            add_feature = (self.sessions[cat_col] == cat).astype(int).sum()\n",
    "            self.result[feat_name] = add_feature\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        self.result[f\"{self.group}_{cat_col}_nunique\"] = self.sessions_df[cat_col].dropna().nunique()       \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        for val_col, agg in itertools.product(val_cols, aggs):\n",
    "            feat_name = f\"{self.group}_{val_col}_{agg}\"\n",
    "            if agg == \"mean\":\n",
    "                add_feature = np.nanmean(self.sessions[val_col])\n",
    "            elif agg == \"max\":\n",
    "                add_feature = np.nanmax(self.sessions[val_col])\n",
    "            elif agg == \"min\":\n",
    "                add_feature = np.nanmin(self.sessions[val_col])\n",
    "            elif agg == \"std\":\n",
    "                add_feature = np.nanstd(self.sessions[val_col], ddof=1)\n",
    "            elif agg == \"sum\":\n",
    "                add_feature = np.nansum(self.sessions[val_col])\n",
    "            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    tmp = self.sessions[val_col][idx]\n",
    "                    if agg == \"mean\":\n",
    "                        add_feature = np.nanmean(tmp)\n",
    "                    elif agg == \"max\":\n",
    "                        add_feature = np.nanmax(tmp)\n",
    "                    elif agg == \"min\":\n",
    "                        add_feature = np.nanmin(tmp)\n",
    "                    elif agg == \"std\":\n",
    "                        add_feature = np.nanstd(tmp, ddof=1)\n",
    "                    elif agg == \"sum\":\n",
    "                        add_feature = np.nansum(tmp)\n",
    "                    if np.isnan(add_feature):\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def get_test(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"])\n",
    "\n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesTrain(sessions, labels)\n",
    "    train = feat.get_train()\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesInf(sessions, labels)\n",
    "    test = feat.get_test()\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    dfs = []\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train_group = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train_group = train_group.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train_group[\"0-4_question_duration_time\"] = train_group[\"5-12_elapsed_time_min\"] - train_group[\"0-4_elapsed_time_max\"]\n",
    "            train_group[\"0-4_question_duration_index\"] = train_group[\"5-12_index_min\"] - train_group[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train_group[\"5-12_question_duration_time\"] = train_group[\"13-22_elapsed_time_min\"] - train_group[\"5-12_elapsed_time_max\"]\n",
    "            train_group[\"5-12_question_duration_index\"] = train_group[\"13-22_index_min\"] - train_group[\"5-12_index_max\"]\n",
    "    \n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train_group.columns if c not in not_use_cols]\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train_group[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "\n",
    "        dfs.append(train_group)\n",
    "    train = pd.concat(dfs, ignore_index=True)\n",
    "    # concatするとcategory型がリセットされてしまうので再度cast\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    target = \"correct\"\n",
    "    not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "    features = [c for c in train.columns if c not in not_use_cols]    \n",
    "\n",
    "    gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "    fis = []\n",
    "    \n",
    "    for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "        oof_groups = []\n",
    "        print(f\"fold : {i}\")\n",
    "        tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "        vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)\n",
    "        # モデル出力\n",
    "        model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{i}.lgb\")\n",
    "    \n",
    "        # valid_pred\n",
    "        oof_fold = train.iloc[vl_idx].copy()\n",
    "        oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oofs.append(oof_fold)\n",
    "\n",
    "        # 特徴量重要度\n",
    "        fi_fold = pd.DataFrame()\n",
    "        fi_fold[\"feature\"] = model.feature_name()\n",
    "        fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "        fi_fold[\"fold\"] = i\n",
    "        fis.append(fi_fold)\n",
    "\n",
    "    fi = pd.concat(fis)    \n",
    "    fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "    fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi.csv\", index=False)\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"level_group2\"] = test[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"level_group2\"] = sub[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in test.groupby(\"level_group2\")]\n",
    "    subs = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in sub.groupby(\"level_group2\")]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    models = []\n",
    "    for i in range(cfg.n_splits):\n",
    "        if mode == \"local_cv\":\n",
    "            model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{i}.lgb\"\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_path = f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{i}.lgb\"\n",
    "        models.append(lgb.Booster(model_file=model_path))\n",
    "    use_features = models[0].feature_name()\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in test.columns if c not in not_use_cols]\n",
    "\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        \n",
    "        # そのlevel_group時点で存在しない列を追加\n",
    "        complement_features = list(set(use_features) - set(test.columns.tolist()))\n",
    "        test[complement_features] = np.nan\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = models[i]\n",
    "            preds += model.predict(test[use_features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        test[\"pred\"] = preds\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n",
      "[1 0 1 1 0 1 0 0 1 0]\n",
      "[1 0 1 1 1]\n",
      "[0 1 1]\n",
      "[0 0 0 0 0 0 0 0 1 0]\n",
      "[0 0 0 0 1]\n",
      "[1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 0]\n",
      "[1 1 1 1 1]\n",
      "sample_inf処理時間 :  24.5 秒\n"
     ]
    }
   ],
   "source": [
    "inference(cfg.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4\n",
      "5-12\n",
      "13-22\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239789, number of negative: 99493\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 9.627831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 848532\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 8139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.706754 -> initscore=0.879672\n",
      "[LightGBM] [Info] Start training from score 0.879672\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.525301\tvalid_1's binary_logloss: 0.532663\n",
      "[200]\ttraining's binary_logloss: 0.49589\tvalid_1's binary_logloss: 0.506147\n",
      "[300]\ttraining's binary_logloss: 0.481164\tvalid_1's binary_logloss: 0.494421\n",
      "[400]\ttraining's binary_logloss: 0.472662\tvalid_1's binary_logloss: 0.489267\n",
      "[500]\ttraining's binary_logloss: 0.466334\tvalid_1's binary_logloss: 0.486252\n",
      "[600]\ttraining's binary_logloss: 0.461328\tvalid_1's binary_logloss: 0.484688\n",
      "[700]\ttraining's binary_logloss: 0.456971\tvalid_1's binary_logloss: 0.483577\n",
      "[800]\ttraining's binary_logloss: 0.453037\tvalid_1's binary_logloss: 0.482857\n",
      "[900]\ttraining's binary_logloss: 0.44937\tvalid_1's binary_logloss: 0.482256\n",
      "[1000]\ttraining's binary_logloss: 0.446011\tvalid_1's binary_logloss: 0.481854\n",
      "[1100]\ttraining's binary_logloss: 0.442854\tvalid_1's binary_logloss: 0.481541\n",
      "[1200]\ttraining's binary_logloss: 0.439927\tvalid_1's binary_logloss: 0.481301\n",
      "[1300]\ttraining's binary_logloss: 0.437144\tvalid_1's binary_logloss: 0.481116\n",
      "[1400]\ttraining's binary_logloss: 0.434491\tvalid_1's binary_logloss: 0.480892\n",
      "[1500]\ttraining's binary_logloss: 0.431969\tvalid_1's binary_logloss: 0.480714\n",
      "[1600]\ttraining's binary_logloss: 0.429604\tvalid_1's binary_logloss: 0.480548\n",
      "[1700]\ttraining's binary_logloss: 0.427267\tvalid_1's binary_logloss: 0.480444\n",
      "[1800]\ttraining's binary_logloss: 0.425117\tvalid_1's binary_logloss: 0.480361\n",
      "[1900]\ttraining's binary_logloss: 0.422945\tvalid_1's binary_logloss: 0.480214\n",
      "[2000]\ttraining's binary_logloss: 0.420853\tvalid_1's binary_logloss: 0.480165\n",
      "[2100]\ttraining's binary_logloss: 0.418839\tvalid_1's binary_logloss: 0.480072\n",
      "[2200]\ttraining's binary_logloss: 0.416851\tvalid_1's binary_logloss: 0.479999\n",
      "[2300]\ttraining's binary_logloss: 0.41484\tvalid_1's binary_logloss: 0.479928\n",
      "[2400]\ttraining's binary_logloss: 0.412953\tvalid_1's binary_logloss: 0.479863\n",
      "[2500]\ttraining's binary_logloss: 0.411129\tvalid_1's binary_logloss: 0.479827\n",
      "[2600]\ttraining's binary_logloss: 0.409405\tvalid_1's binary_logloss: 0.479784\n",
      "[2700]\ttraining's binary_logloss: 0.40763\tvalid_1's binary_logloss: 0.479766\n",
      "[2800]\ttraining's binary_logloss: 0.405972\tvalid_1's binary_logloss: 0.479764\n",
      "[2900]\ttraining's binary_logloss: 0.404305\tvalid_1's binary_logloss: 0.479687\n",
      "[3000]\ttraining's binary_logloss: 0.402612\tvalid_1's binary_logloss: 0.479655\n",
      "[3100]\ttraining's binary_logloss: 0.40103\tvalid_1's binary_logloss: 0.479621\n",
      "[3200]\ttraining's binary_logloss: 0.399488\tvalid_1's binary_logloss: 0.479636\n",
      "Early stopping, best iteration is:\n",
      "[3160]\ttraining's binary_logloss: 0.400079\tvalid_1's binary_logloss: 0.479618\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239066, number of negative: 100216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 8.592773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 848135\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 8139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.704623 -> initscore=0.869412\n",
      "[LightGBM] [Info] Start training from score 0.869412\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.526541\tvalid_1's binary_logloss: 0.52714\n",
      "[200]\ttraining's binary_logloss: 0.496808\tvalid_1's binary_logloss: 0.501684\n",
      "[300]\ttraining's binary_logloss: 0.481825\tvalid_1's binary_logloss: 0.490709\n",
      "[400]\ttraining's binary_logloss: 0.473254\tvalid_1's binary_logloss: 0.485994\n",
      "[500]\ttraining's binary_logloss: 0.466921\tvalid_1's binary_logloss: 0.483326\n",
      "[600]\ttraining's binary_logloss: 0.461908\tvalid_1's binary_logloss: 0.481845\n",
      "[700]\ttraining's binary_logloss: 0.457517\tvalid_1's binary_logloss: 0.48087\n",
      "[800]\ttraining's binary_logloss: 0.453578\tvalid_1's binary_logloss: 0.480149\n",
      "[900]\ttraining's binary_logloss: 0.449946\tvalid_1's binary_logloss: 0.479652\n",
      "[1000]\ttraining's binary_logloss: 0.446551\tvalid_1's binary_logloss: 0.479276\n",
      "[1100]\ttraining's binary_logloss: 0.443395\tvalid_1's binary_logloss: 0.478951\n",
      "[1200]\ttraining's binary_logloss: 0.440462\tvalid_1's binary_logloss: 0.478765\n",
      "[1300]\ttraining's binary_logloss: 0.437705\tvalid_1's binary_logloss: 0.478604\n",
      "[1400]\ttraining's binary_logloss: 0.435111\tvalid_1's binary_logloss: 0.478443\n",
      "[1500]\ttraining's binary_logloss: 0.432552\tvalid_1's binary_logloss: 0.478338\n",
      "[1600]\ttraining's binary_logloss: 0.430201\tvalid_1's binary_logloss: 0.47823\n",
      "[1700]\ttraining's binary_logloss: 0.427866\tvalid_1's binary_logloss: 0.478149\n",
      "[1800]\ttraining's binary_logloss: 0.425658\tvalid_1's binary_logloss: 0.478046\n",
      "[1900]\ttraining's binary_logloss: 0.423487\tvalid_1's binary_logloss: 0.477976\n",
      "[2000]\ttraining's binary_logloss: 0.421375\tvalid_1's binary_logloss: 0.477866\n",
      "[2100]\ttraining's binary_logloss: 0.419371\tvalid_1's binary_logloss: 0.477849\n",
      "[2200]\ttraining's binary_logloss: 0.41748\tvalid_1's binary_logloss: 0.477817\n",
      "[2300]\ttraining's binary_logloss: 0.415529\tvalid_1's binary_logloss: 0.477785\n",
      "[2400]\ttraining's binary_logloss: 0.413623\tvalid_1's binary_logloss: 0.477781\n",
      "Early stopping, best iteration is:\n",
      "[2358]\ttraining's binary_logloss: 0.41443\tvalid_1's binary_logloss: 0.477757\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239010, number of negative: 100290\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 10.203579 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 848340\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 8139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.704421 -> initscore=0.868439\n",
      "[LightGBM] [Info] Start training from score 0.868439\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.526556\tvalid_1's binary_logloss: 0.527068\n",
      "[200]\ttraining's binary_logloss: 0.496866\tvalid_1's binary_logloss: 0.501617\n",
      "[300]\ttraining's binary_logloss: 0.482001\tvalid_1's binary_logloss: 0.490691\n",
      "[400]\ttraining's binary_logloss: 0.473426\tvalid_1's binary_logloss: 0.485911\n",
      "[500]\ttraining's binary_logloss: 0.467034\tvalid_1's binary_logloss: 0.483149\n",
      "[600]\ttraining's binary_logloss: 0.462066\tvalid_1's binary_logloss: 0.481745\n",
      "[700]\ttraining's binary_logloss: 0.457646\tvalid_1's binary_logloss: 0.480662\n",
      "[800]\ttraining's binary_logloss: 0.453706\tvalid_1's binary_logloss: 0.48001\n",
      "[900]\ttraining's binary_logloss: 0.450051\tvalid_1's binary_logloss: 0.479516\n",
      "[1000]\ttraining's binary_logloss: 0.446669\tvalid_1's binary_logloss: 0.479142\n",
      "[1100]\ttraining's binary_logloss: 0.443525\tvalid_1's binary_logloss: 0.478876\n",
      "[1200]\ttraining's binary_logloss: 0.440581\tvalid_1's binary_logloss: 0.478635\n",
      "[1300]\ttraining's binary_logloss: 0.437856\tvalid_1's binary_logloss: 0.478475\n",
      "[1400]\ttraining's binary_logloss: 0.435276\tvalid_1's binary_logloss: 0.478362\n",
      "[1500]\ttraining's binary_logloss: 0.432783\tvalid_1's binary_logloss: 0.478206\n",
      "[1600]\ttraining's binary_logloss: 0.430429\tvalid_1's binary_logloss: 0.478096\n",
      "[1700]\ttraining's binary_logloss: 0.428086\tvalid_1's binary_logloss: 0.47802\n",
      "[1800]\ttraining's binary_logloss: 0.425926\tvalid_1's binary_logloss: 0.47795\n",
      "[1900]\ttraining's binary_logloss: 0.423787\tvalid_1's binary_logloss: 0.477849\n",
      "[2000]\ttraining's binary_logloss: 0.421689\tvalid_1's binary_logloss: 0.477744\n",
      "[2100]\ttraining's binary_logloss: 0.419734\tvalid_1's binary_logloss: 0.47771\n",
      "[2200]\ttraining's binary_logloss: 0.417799\tvalid_1's binary_logloss: 0.477653\n",
      "[2300]\ttraining's binary_logloss: 0.415844\tvalid_1's binary_logloss: 0.477592\n",
      "[2400]\ttraining's binary_logloss: 0.41399\tvalid_1's binary_logloss: 0.477568\n",
      "[2500]\ttraining's binary_logloss: 0.41216\tvalid_1's binary_logloss: 0.477495\n",
      "[2600]\ttraining's binary_logloss: 0.410493\tvalid_1's binary_logloss: 0.477485\n",
      "[2700]\ttraining's binary_logloss: 0.408731\tvalid_1's binary_logloss: 0.477429\n",
      "[2800]\ttraining's binary_logloss: 0.407056\tvalid_1's binary_logloss: 0.477387\n",
      "[2900]\ttraining's binary_logloss: 0.405426\tvalid_1's binary_logloss: 0.477401\n",
      "Early stopping, best iteration is:\n",
      "[2815]\ttraining's binary_logloss: 0.40677\tvalid_1's binary_logloss: 0.477377\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239535, number of negative: 99765\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 10.154328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 848590\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 8139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.705968 -> initscore=0.875882\n",
      "[LightGBM] [Info] Start training from score 0.875882\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.525593\tvalid_1's binary_logloss: 0.531259\n",
      "[200]\ttraining's binary_logloss: 0.496053\tvalid_1's binary_logloss: 0.505235\n",
      "[300]\ttraining's binary_logloss: 0.481279\tvalid_1's binary_logloss: 0.493835\n",
      "[400]\ttraining's binary_logloss: 0.472744\tvalid_1's binary_logloss: 0.48897\n",
      "[500]\ttraining's binary_logloss: 0.466439\tvalid_1's binary_logloss: 0.486062\n",
      "[600]\ttraining's binary_logloss: 0.461455\tvalid_1's binary_logloss: 0.484526\n",
      "[700]\ttraining's binary_logloss: 0.457065\tvalid_1's binary_logloss: 0.483422\n",
      "[800]\ttraining's binary_logloss: 0.453187\tvalid_1's binary_logloss: 0.482767\n",
      "[900]\ttraining's binary_logloss: 0.449567\tvalid_1's binary_logloss: 0.482263\n",
      "[1000]\ttraining's binary_logloss: 0.446204\tvalid_1's binary_logloss: 0.481868\n",
      "[1100]\ttraining's binary_logloss: 0.443045\tvalid_1's binary_logloss: 0.481516\n",
      "[1200]\ttraining's binary_logloss: 0.440137\tvalid_1's binary_logloss: 0.481313\n",
      "[1300]\ttraining's binary_logloss: 0.437386\tvalid_1's binary_logloss: 0.481159\n",
      "[1400]\ttraining's binary_logloss: 0.434751\tvalid_1's binary_logloss: 0.480999\n",
      "[1500]\ttraining's binary_logloss: 0.432229\tvalid_1's binary_logloss: 0.480873\n",
      "[1600]\ttraining's binary_logloss: 0.429862\tvalid_1's binary_logloss: 0.48072\n",
      "[1700]\ttraining's binary_logloss: 0.427545\tvalid_1's binary_logloss: 0.480608\n",
      "[1800]\ttraining's binary_logloss: 0.425342\tvalid_1's binary_logloss: 0.480502\n",
      "[1900]\ttraining's binary_logloss: 0.423229\tvalid_1's binary_logloss: 0.480417\n",
      "[2000]\ttraining's binary_logloss: 0.421141\tvalid_1's binary_logloss: 0.480345\n",
      "[2100]\ttraining's binary_logloss: 0.419183\tvalid_1's binary_logloss: 0.480315\n",
      "[2200]\ttraining's binary_logloss: 0.417228\tvalid_1's binary_logloss: 0.480215\n",
      "[2300]\ttraining's binary_logloss: 0.415279\tvalid_1's binary_logloss: 0.480184\n",
      "[2400]\ttraining's binary_logloss: 0.413359\tvalid_1's binary_logloss: 0.480138\n",
      "[2500]\ttraining's binary_logloss: 0.411593\tvalid_1's binary_logloss: 0.48011\n",
      "[2600]\ttraining's binary_logloss: 0.409916\tvalid_1's binary_logloss: 0.480092\n",
      "[2700]\ttraining's binary_logloss: 0.408135\tvalid_1's binary_logloss: 0.480068\n",
      "Early stopping, best iteration is:\n",
      "[2692]\ttraining's binary_logloss: 0.408291\tvalid_1's binary_logloss: 0.480057\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239684, number of negative: 99616\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 9.776666 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 848418\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 8139\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.706407 -> initscore=0.877999\n",
      "[LightGBM] [Info] Start training from score 0.877999\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.525407\tvalid_1's binary_logloss: 0.532074\n",
      "[200]\ttraining's binary_logloss: 0.495938\tvalid_1's binary_logloss: 0.505519\n",
      "[300]\ttraining's binary_logloss: 0.481251\tvalid_1's binary_logloss: 0.493897\n",
      "[400]\ttraining's binary_logloss: 0.472794\tvalid_1's binary_logloss: 0.488694\n",
      "[500]\ttraining's binary_logloss: 0.466484\tvalid_1's binary_logloss: 0.485582\n",
      "[600]\ttraining's binary_logloss: 0.461476\tvalid_1's binary_logloss: 0.483938\n",
      "[700]\ttraining's binary_logloss: 0.45707\tvalid_1's binary_logloss: 0.482736\n",
      "[800]\ttraining's binary_logloss: 0.453149\tvalid_1's binary_logloss: 0.482024\n",
      "[900]\ttraining's binary_logloss: 0.449528\tvalid_1's binary_logloss: 0.481445\n",
      "[1000]\ttraining's binary_logloss: 0.446148\tvalid_1's binary_logloss: 0.481032\n",
      "[1100]\ttraining's binary_logloss: 0.443003\tvalid_1's binary_logloss: 0.480718\n",
      "[1200]\ttraining's binary_logloss: 0.440084\tvalid_1's binary_logloss: 0.480489\n",
      "[1300]\ttraining's binary_logloss: 0.437328\tvalid_1's binary_logloss: 0.480236\n",
      "[1400]\ttraining's binary_logloss: 0.434698\tvalid_1's binary_logloss: 0.480069\n",
      "[1500]\ttraining's binary_logloss: 0.432167\tvalid_1's binary_logloss: 0.479965\n",
      "[1600]\ttraining's binary_logloss: 0.429779\tvalid_1's binary_logloss: 0.479873\n",
      "[1700]\ttraining's binary_logloss: 0.427447\tvalid_1's binary_logloss: 0.479761\n",
      "[1800]\ttraining's binary_logloss: 0.42527\tvalid_1's binary_logloss: 0.479655\n",
      "[1900]\ttraining's binary_logloss: 0.423175\tvalid_1's binary_logloss: 0.479622\n",
      "[2000]\ttraining's binary_logloss: 0.421101\tvalid_1's binary_logloss: 0.47951\n",
      "[2100]\ttraining's binary_logloss: 0.419081\tvalid_1's binary_logloss: 0.479456\n",
      "[2200]\ttraining's binary_logloss: 0.417102\tvalid_1's binary_logloss: 0.479378\n",
      "[2300]\ttraining's binary_logloss: 0.41518\tvalid_1's binary_logloss: 0.479279\n",
      "[2400]\ttraining's binary_logloss: 0.413296\tvalid_1's binary_logloss: 0.479215\n",
      "[2500]\ttraining's binary_logloss: 0.411493\tvalid_1's binary_logloss: 0.479164\n",
      "[2600]\ttraining's binary_logloss: 0.409772\tvalid_1's binary_logloss: 0.479185\n",
      "Early stopping, best iteration is:\n",
      "[2547]\ttraining's binary_logloss: 0.410683\tvalid_1's binary_logloss: 0.479155\n",
      "logloss 0.478793\n",
      "best_score 0.698249\n",
      "best_threshold 0.630\n",
      "------------------------------\n",
      "Q1 : F1 = 0.613478\n",
      "Q2 : F1 = 0.531552\n",
      "Q3 : F1 = 0.582320\n",
      "Q4 : F1 = 0.660230\n",
      "Q5 : F1 = 0.402462\n",
      "Q6 : F1 = 0.622246\n",
      "Q7 : F1 = 0.574876\n",
      "Q8 : F1 = 0.336230\n",
      "Q9 : F1 = 0.576954\n",
      "Q10 : F1 = 0.358802\n",
      "Q11 : F1 = 0.403305\n",
      "Q12 : F1 = 0.593002\n",
      "Q13 : F1 = 0.421406\n",
      "Q14 : F1 = 0.534416\n",
      "Q15 : F1 = 0.370434\n",
      "Q16 : F1 = 0.431734\n",
      "Q17 : F1 = 0.378972\n",
      "Q18 : F1 = 0.532872\n"
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    run_train()\n",
    "#binference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
