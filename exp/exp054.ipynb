{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp054"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meta_feature検討\n",
    "|  取り組み  |  cv  |  logloss |　採否 |\n",
    "| ---- | ---- | ---- | ----- |\n",
    "|  exp052  |  0.694623  |  0.483687 | - |   \n",
    "|  meta feature追加  |  0.694133  |  0.482887 | - |   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp054\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.1, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used_total = [\n",
    "    'record_cnt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col_lists = {'event_name': ['cutscene_click',\n",
    "  'person_click',\n",
    "  'navigate_click',\n",
    "  'observation_click',\n",
    "  'notification_click',\n",
    "  'object_click',\n",
    "  'object_hover',\n",
    "  'map_hover',\n",
    "  'map_click',\n",
    "  'checkpoint',\n",
    "  'notebook_click'],\n",
    " 'name': ['basic', 'undefined', 'close', 'open', 'prev', 'next'],\n",
    " 'level': [0,\n",
    "  1,\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  5,\n",
    "  6,\n",
    "  7,\n",
    "  8,\n",
    "  9,\n",
    "  10,\n",
    "  11,\n",
    "  12,\n",
    "  13,\n",
    "  14,\n",
    "  15,\n",
    "  16,\n",
    "  17,\n",
    "  18,\n",
    "  19,\n",
    "  20,\n",
    "  21,\n",
    "  22],\n",
    " 'page': [0.0, 1.0, 3.0, 4.0, 5.0, 6.0, 2.0],\n",
    " 'text': ['undefined',\n",
    "  'Whatcha doing over there, Jo?',\n",
    "  'Just talking to Teddy.',\n",
    "  'I gotta run to my meeting!',\n",
    "  'Can I come, Gramps?',\n",
    "  'Sure thing, Jo. Grab your notebook and come upstairs!',\n",
    "  'See you later, Teddy.',\n",
    "  \"I get to go to Gramps's meeting!\",\n",
    "  'Now where did I put my notebook?',\n",
    "  '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00b4',\n",
    "  'I love these photos of me and Teddy!',\n",
    "  'Found it!',\n",
    "  'Gramps is in trouble for losing papers?',\n",
    "  \"This can't be right!\",\n",
    "  'Gramps is a great historian!',\n",
    "  \"Hmm. Button's still not working.\",\n",
    "  \"Let's get started. The Wisconsin Wonders exhibit opens tomorrow!\",\n",
    "  'Who wants to investigate the shirt artifact?',\n",
    "  \"Not Leopold here. He's been losing papers lately.\",\n",
    "  'Hey!',\n",
    "  \"It's true, they do keep going missing lately.\",\n",
    "  'See?',\n",
    "  'Besides, I already figured out the shirt.',\n",
    "  \"It's a women's basketball jersey!\",\n",
    "  'That settles it.',\n",
    "  'Wells, finish up your report.',\n",
    "  \"Leopold, why don't you help me set up in the Capitol?\",\n",
    "  'We need to talk about that missing paperwork.',\n",
    "  'Will do, Boss.',\n",
    "  \"Hey Jo, let's take a look at the shirt!\",\n",
    "  'Your grampa is waiting for you in the collection room.',\n",
    "  \"Why don't you go catch up with your grampa?\",\n",
    "  'What a fascinating artifact!',\n",
    "  \"Wow, that's so cool, Gramps!\",\n",
    "  'Can I take a closer look?',\n",
    "  \"Hmmm. Shouldn't you be doing your homework?\",\n",
    "  \"It's already all done!\",\n",
    "  'Plus, my teacher said I could help you out for extra credit!',\n",
    "  \"Well, that's good enough for me.\",\n",
    "  'Go ahead, take a peek at the shirt!',\n",
    "  'This looks like a clue!',\n",
    "  \"I'll record this in my notebook.\",\n",
    "  'Find anything?',\n",
    "  'Yes! This old slip from 1916.',\n",
    "  'I knew it!',\n",
    "  \"I'm not so sure that this is a basketball jersey.\",\n",
    "  'Wait, you mean Wells is wrong?!',\n",
    "  'Could be. But we need evidence!',\n",
    "  \"Why don't you head to the Basketball Center and rustle up some clues?\",\n",
    "  'Sure!',\n",
    "  \"I'll be at the Capitol. Let me know if you find anything!\",\n",
    "  'Better check back later.',\n",
    "  \"That's it!\",\n",
    "  \"The slip is from 1916 but the team didn't start until 1974!\",\n",
    "  'Our shirt is too old to be a basketball jersey!',\n",
    "  'I need to get to the Capitol and tell Gramps!',\n",
    "  'What are you still doing here,  Jolie?',\n",
    "  'Go find your grampa and get to work!',\n",
    "  'Oh no!',\n",
    "  'What happened here?!',\n",
    "  \"I don't know!\",\n",
    "  'I got here and the whole place was a mess!',\n",
    "  'Can you help me tidy up?',\n",
    "  \"Teddy's scarf! Somebody must've taken him!\",\n",
    "  'Try not to panic, Jo.',\n",
    "  'Maybe he just got scared and ran off.',\n",
    "  'But he never goes anywhere without his scarf!',\n",
    "  \"I think he's in trouble!\",\n",
    "  'Is this your coffee, Gramps?',\n",
    "  \"Nope, that's from Bean Town. I only drink Holdgers!\",\n",
    "  \"Who could've done this?\",\n",
    "  \"It must've been Wells.\",\n",
    "  \"He's always trying to get you in trouble, and he doesn't like animals!\",\n",
    "  'Slow down, Jo.',\n",
    "  'But what if Wells kidnapped Teddy?',\n",
    "  'Then we need evidence.',\n",
    "  \"You're right, Gramps. Let's investigate!\",\n",
    "  \"I'm afraid my papers have gone missing in this mess.\",\n",
    "  \"You'll have to get started without me.\",\n",
    "  \"Okay. I'll find Teddy!\",\n",
    "  \"And I'll figure out the shirt, too.\",\n",
    "  'I knew I could count on you, Jo!',\n",
    "  \"Why don't you go upstairs and see the archivist?\",\n",
    "  \"He's our expert record keeper.\",\n",
    "  'I need your help!',\n",
    "  'Who are you?',\n",
    "  \"I'm Leopold's grandkid!\",\n",
    "  \"Sorry, I'm too busy for kids right now.\",\n",
    "  'Now if only I could read this thing.',\n",
    "  \"Can't believe I lost my reading glasses.\",\n",
    "  'I bet the archivist could use this!',\n",
    "  \"Ah, that's better!\",\n",
    "  'Did you have a question?',\n",
    "  'Yes! I was wondering-',\n",
    "  'Wait a minute!',\n",
    "  'Where did you get that coffee?',\n",
    "  \"Oh, that's from Bean Town.\",\n",
    "  'I ran into Wells there this morning.',\n",
    "  'Wells? I knew it!',\n",
    "  'Do you know anything about this slip?',\n",
    "  'I found it on an old shirt.',\n",
    "  'An old shirt? Try the university.',\n",
    "  'You can talk to a textile expert there.',\n",
    "  \"What's a textile expert?\",\n",
    "  'They study clothes and fabric.',\n",
    "  'Great! Thanks for the help!',\n",
    "  'Head over to the university.',\n",
    "  'Hello there!',\n",
    "  'Wow! What is all this stuff?',\n",
    "  \"It's our Norwegian Craft exhibit!\",\n",
    "  'Can I give you the tour?',\n",
    "  \"Sorry, I'm in a hurry.\",\n",
    "  'Do you know what this slip is?',\n",
    "  'Looks like a dry cleaning receipt.',\n",
    "  'Thanks.',\n",
    "  'Now I Just need to find all the cleaners from way back in 1916.',\n",
    "  'Maybe I can help!',\n",
    "  \"I've got a stack of business cards from my favorite cleaners.\",\n",
    "  \"Why don't you take a look?\",\n",
    "  'This place was around in 1916! I can start there!',\n",
    "  \"You haven't seen any badgers around here, have you?\",\n",
    "  'Badgers? No.',\n",
    "  'Okay. Thanks anyway.',\n",
    "  'Hi! How can I help you?',\n",
    "  'I need to find the owner of this slip.',\n",
    "  \"Well, I can't show our log books to just anybody.\",\n",
    "  'Please?',\n",
    "  \"It's for Grampa Leo. He's a historian!\",\n",
    "  'Leo... you mean Leopold?',\n",
    "  'Your gramps is awesome! Always full of stories.',\n",
    "  \"Guess it couldn't hurt to let you take a look.\",\n",
    "  \"Here's the log book.\",\n",
    "  \"It's a match!\",\n",
    "  'Theodora Youmans must be the owner!',\n",
    "  'Do you know who Theodora Youmans is?',\n",
    "  \"Hmmm... not sure. Why don't you try the library?\",\n",
    "  'Thanks for the help!',\n",
    "  'Oh, hello there!',\n",
    "  'How can I help you?',\n",
    "  'Have you seen a badger around here?',\n",
    "  \"I'm afraid not.\",\n",
    "  'Please let me know if you do.',\n",
    "  \"I'm also looking for Theodora Youmans. Have you heard of her?\",\n",
    "  'Theodora Youmans? Of course!',\n",
    "  \"Check out our microfiche. It's right through that door.\",\n",
    "  'Youmans was a suffragist!',\n",
    "  'She helped get votes for women!',\n",
    "  'Wells! What was he doing here? I should ask the librarian.',\n",
    "  'What was Wells doing here?',\n",
    "  'He was looking for a taxidermist.',\n",
    "  \"What's a taxidermist?\",\n",
    "  'Not sure. Here, let me look it up.',\n",
    "  '\\\\Taxidermy: the art of preparing, stuffing, and mounting the skins of animals.\\\\',\n",
    "  'Oh no... Teddy!',\n",
    "  'Can you help me find Wells?',\n",
    "  'You could ask the archivist. He knows everybody!',\n",
    "  \"Jolie! I was hoping you'd stop by. Any news on the shirt artifact?\",\n",
    "  \"I haven't quite figured it out just yet...\",\n",
    "  \"Well, get on it. I'm counting on you and your gramps to figure this out!\",\n",
    "  'Can you help me? I need to find Wells!',\n",
    "  \"I haven't seen him.\",\n",
    "  'Please? This is really important.',\n",
    "  \"Sorry, can't help you.\",\n",
    "  'Do you have any info on Theodora Youmans?',\n",
    "  'Theodora Youmans? Is that who owned the shirt?',\n",
    "  'Yep.',\n",
    "  \"Why didn't you say so?\",\n",
    "  'Youmans was a suffragist here in Wisconsin.',\n",
    "  'She led marches and helped women get the right to vote!',\n",
    "  \"Wait a sec. Women couldn't vote?!\",\n",
    "  'Nope. But Youmans and other suffragists worked hard to change that.',\n",
    "  'Thanks to them, Wisconsin was the first state to approve votes for women!',\n",
    "  'Wow!',\n",
    "  \"Here's a call number to find more info in the Stacks.\",\n",
    "  'Where are the Stacks?',\n",
    "  'Right outside the door.',\n",
    "  'Hey, this is Youmans!',\n",
    "  \"And look! She's wearing the shirt!\",\n",
    "  'I should go to the Capitol and tell everyone!',\n",
    "  'Jo!',\n",
    "  'Check out the next artifact!',\n",
    "  'What is it?',\n",
    "  \"I think it's a flag! Pretty interesting, huh?\",\n",
    "  \"It's really cool, Gramps. But I'm worried about Teddy.\",\n",
    "  \"He's still missing!\",\n",
    "  \"We'll find him, Jo.\",\n",
    "  'Want to look for more clues?',\n",
    "  \"We'll find Teddy.\",\n",
    "  'We just have to keep our eyes open!',\n",
    "  'Hey, look at those scratches!',\n",
    "  'The kidnapper probably took Teddy on the elevator!',\n",
    "  \"You're right, Jo!\",\n",
    "  \"Why isn't the button working?\",\n",
    "  \"We'll need a key card.\",\n",
    "  'I had one, but Teddy chewed it up.',\n",
    "  \"I've got Wells's ID!\",\n",
    "  'What should we do next?',\n",
    "  \"I need to take the artifact upstairs. Why don't you investigate those scratch marks?\",\n",
    "  \"Okay. I'll try.\",\n",
    "  'Teddy, here I come!',\n",
    "  'I wonder whose glasses these are.',\n",
    "  'Teddy!!!',\n",
    "  \"Hang on. I'll get you out of there!\",\n",
    "  'Whoever lost these glasses probably took Teddy!',\n",
    "  'How can I find out whose glasses these are?',\n",
    "  'Oh! There was a staff directory in the entryway!',\n",
    "  \"I'll go look at everyone's pictures!\",\n",
    "  'Those are the same glasses!',\n",
    "  \"The archivist must've taken Teddy!\",\n",
    "  \"Yes! It's the key for Teddy's cage!\",\n",
    "  'I found the key!',\n",
    "  \"Come on, let's get out of here!\",\n",
    "  \"Here's your scarf back!\",\n",
    "  'What are you doing down here?',\n",
    "  'And how did that badger get free?',\n",
    "  \"I'm here to rescue my friend!\",\n",
    "  \"What's going on here?\",\n",
    "  'Thanks for coming, Boss.',\n",
    "  'I told you!',\n",
    "  'I captured a badger in our museum!',\n",
    "  \"He's been eating my lunch every day this week!\",\n",
    "  'He has??',\n",
    "  \"I've seen him eating homework and important papers, too.\",\n",
    "  \"Jolie- keep your badger under control, or he'll have to go.\",\n",
    "  'And you, Frank-',\n",
    "  \"You can't just steal Jolie's pet.\",\n",
    "  'Ugh. Fine.',\n",
    "  'Alright, Jolie. Back to work.',\n",
    "  \"Come on, Teddy. Let's go help Gramps!\",\n",
    "  \"Let's go help Gramps!\",\n",
    "  'Gramps must be up in the collection room.',\n",
    "  \"Let's go find him!\",\n",
    "  \"Teddy! I'm glad to see you.\",\n",
    "  'The archivist had him locked up!',\n",
    "  'Poor badger.',\n",
    "  \"You're becoming quite the detective, Jo.\",\n",
    "  'Notice any clues about this flag?',\n",
    "  'Well... it looks hand-stitched.',\n",
    "  'Good catch!',\n",
    "  'Go on, tell the boss what you found!',\n",
    "  \"I'm telling you, Boss. Taxidermy is the way to go!\",\n",
    "  'Nonsense. I want live animals at the exhibit, not stuffed ones.',\n",
    "  \"Ah, Jolie! I'm glad you're here.\",\n",
    "  \"I'm putting you in charge of the flag case.\",\n",
    "  'Make sure to get some old photos for the exhibit, like last time!',\n",
    "  \"Wait! Can't I do it?\",\n",
    "  'The symbol on the flag looks sort of like a deer hoof.',\n",
    "  'It could be an early design for the Wisconsin state flag!',\n",
    "  'Wells, you already have a job to do.',\n",
    "  'What now, kid?',\n",
    "  'Do you really think that symbol is a deer hoof?',\n",
    "  'Not sure.',\n",
    "  'Do you know where I can find a deer expert?',\n",
    "  'Hmm. You could try the Aldo Leopold Wildlife Center.',\n",
    "  'I have to head over there and check out the animals.',\n",
    "  \"I'll ride with you!\",\n",
    "  \"Come on, kid. Let's go.\",\n",
    "  'Head over to the Wildlife Center!',\n",
    "  \"I'm sure they'll be able to help.\",\n",
    "  'People sure drink a lot of coffee around here.',\n",
    "  \"I can't believe this.\",\n",
    "  'Ugh...',\n",
    "  'Oh no! What happened to that crane?',\n",
    "  'Her beak is stuck in a coffee cup.',\n",
    "  \"It's lucky we found her.\",\n",
    "  'Ugh! Those cups are all over the place.',\n",
    "  \"I need to get her free. She won't hold still!\",\n",
    "  'Can Teddy and I help?',\n",
    "  'Sure! Give it a try.',\n",
    "  'Careful. That beak is sharp!',\n",
    "  'We need to calm her down, Teddy.',\n",
    "  'Any ideas?',\n",
    "  '\\\\u00f0\\\\u0178\\\\u00a6\\\\u2014',\n",
    "  'Oh yeah, cranes eat insects!',\n",
    "  'Luckily there are tons of insects around here...',\n",
    "  'Got one!',\n",
    "  \"Maybe she'll let me take off the cup!\",\n",
    "  \"It's OK, girl! Look, I found you a cricket!\",\n",
    "  'You did it! Thanks, kid.',\n",
    "  'Can I help you with anything?',\n",
    "  \"I'm investigating this symbol.\",\n",
    "  'Does it look like a deer hoof?',\n",
    "  \"There's a diagram of animal tracks over there.\",\n",
    "  'Go take a look!',\n",
    "  \"That hoofprint doesn't match the flag!\",\n",
    "  'Thanks for your help, kid!',\n",
    "  \"So? What'd you find out?\",\n",
    "  \"Looks like it's not a deer hoof.\",\n",
    "  \"Oh no. If I don't impress the boss soon,  I'm gonna get fired!\",\n",
    "  'Hey, Wells...',\n",
    "  'I think I might be able to help you.',\n",
    "  \"No thanks. I don't need help from kids.\",\n",
    "  'Are you sure? I know where you can find a real, live badger for the exhibit!',\n",
    "  'Wait! What?! Really?',\n",
    "  'Wells, meet Teddy.',\n",
    "  '\\\\u00f0\\\\u0178\\\\u02dc\\\\u0160',\n",
    "  \"He says he'd be willing to help out.\",\n",
    "  'Yes!!!',\n",
    "  'We still need to figure out that flag. Do you know anyone who could help?',\n",
    "  \"Hmm. Let's see...\",\n",
    "  'Actually, I went to school with somebody who LOVES old flags.',\n",
    "  \"Why don't you go talk to her? I'll let her know you're coming.\",\n",
    "  'Hey, nice dog! What breed is he?',\n",
    "  \"Actually, he's a badger.\",\n",
    "  \"Oh, cool! I've never seen a badger in real life.\",\n",
    "  \"You've got a million flags here!\",\n",
    "  \"Yep. I'm a vexillophile!\",\n",
    "  \"What's a vexillophile? \",\n",
    "  'It just means flag expert. How can I help?',\n",
    "  \"I'm investigating this flag.\",\n",
    "  'Can you take a look?',\n",
    "  \"Hey, I've seen that symbol before! Check it out!\",\n",
    "  '\\\\Ecology flag, by Ron Cobb.\\\\',\n",
    "  \"It's an ecology flag!\",\n",
    "  'Do you know what this flag was used for?',\n",
    "  \"I'm not sure.\",\n",
    "  \"If I were you, I'd go to the library and do some digging.\",\n",
    "  'Good idea. Thanks!',\n",
    "  'Welcome back, Dear! How can I help you?',\n",
    "  'I need to learn more about this flag!',\n",
    "  'It has something to do with ecology.',\n",
    "  'Hmm... those stripes remind me of the American flag.',\n",
    "  'Your flag must have been part of a national movement!',\n",
    "  \"Go check the microfiche. Maybe you'll find something!\",\n",
    "  \"Hey! That's Governor Nelson in front of our flag!\",\n",
    "  'I found the flag! Governor Nelson used it on the first Earth Day!',\n",
    "  'Wow! You figured it out!',\n",
    "  'Now I just need some old photos, like last time.',\n",
    "  'The boss is gonna love it!',\n",
    "  'You could try the archives.',\n",
    "  'Though the archivist might be too busy to help...',\n",
    "  'Okay. Thanks!',\n",
    "  'What are you doing here?',\n",
    "  \"We're looking for some photos.\",\n",
    "  \"It's for the flag display!\",\n",
    "  'Wait a minute...',\n",
    "  \"YOU'RE the new history detective everybody's talking about?\",\n",
    "  \"Teddy's helping too.\",\n",
    "  'What kind of photos do you need?',\n",
    "  'Something to do with ecology and Wisconsin.',\n",
    "  \"Here's a call number for the Stacks. Go find some photos.\",\n",
    "  'Look at all those activists!',\n",
    "  'This is perfect for the exhibit.',\n",
    "  'I should go to the Capitol and tell Mrs. M!',\n",
    "  'I should see what Grampa is up to!',\n",
    "  'What should I do first?',\n",
    "  'Head upstairs and talk to the archivist. He might be able to help!',\n",
    "  \"It's locked!\",\n",
    "  \"Jolie! I was hoping you'd stop by. Any news on the flag artifact?\",\n",
    "  \"Well, get on it. I'm counting on you to figure this out!\",\n",
    "  'Nice seeing you, Jolie!',\n",
    "  \"It's such a nice fall day.\",\n",
    "  'I love these photos of me and Teddy.',\n",
    "  \"Why don't you go talk to the boss?\",\n",
    "  \"She's right outside.\",\n",
    "  'My friend is a flag expert.',\n",
    "  'She should be able to help you out.',\n",
    "  'There are some old newspapers loaded up in the microfiche.',\n",
    "  'The Stacks are right outside the door. Go find some photos!',\n",
    "  'I should help Gramps clean.',\n",
    "  \"Maybe there's a clue in this mess!\",\n",
    "  \"Poor Gramps! I should make sure he's okay.\",\n",
    "  'The archivist said I should look in the stacks.',\n",
    "  'There should be some info about that symbol in my book.',\n",
    "  'Meetings are BORING!',\n",
    "  'Grab your notebook and come upstairs!',\n",
    "  'Hang tight, Teddy.',\n",
    "  \"I'll hurry back and then we can go exploring!\",\n",
    "  'Well, Leopold here is always losing papers...',\n",
    "  'Ha. Told you so!',\n",
    "  \"Look at that! It's the bee's knees!\",\n",
    "  'Can we hurry up, Gramps?',\n",
    "  'Teddy and I were gonna go climb that huge tree out back!',\n",
    "  \"Hmmm. Don't forget about your homework.\",\n",
    "  'Your teacher said you missed 7 assignments in a row!',\n",
    "  \"Well, I did SOME of those. I just couldn't find them!\",\n",
    "  'Did you do all of them?',\n",
    "  'No... because history is boring!',\n",
    "  'I suppose historians are boring, too?',\n",
    "  \"No way, Gramps. You're the best!\",\n",
    "  'Then do it for me!',\n",
    "  'Your teacher said you could help me for extra credit.',\n",
    "  'Hooray, a boring old shirt.',\n",
    "  'Just this old slip from 1916.',\n",
    "  'Hot Dog! I knew it!',\n",
    "  'Ooh, I like clues!',\n",
    "  'What the-',\n",
    "  'I got here and the whole place was ransacked!',\n",
    "  'I have an idea.',\n",
    "  \"He's wrong about old shirts and his name rhymes with \\\\smells\\\\...\",\n",
    "  'Hold your horses, Jo.',\n",
    "  'BUT WELLS STOLE TEDDY!',\n",
    "  'Could be. But we need evidence.',\n",
    "  \"Fine. Let's investigate!\",\n",
    "  \"Don't worry, Gramps. I'll find Teddy!\",\n",
    "  'This button never works!',\n",
    "  '*grumble grumble*',\n",
    "  'And you are?',\n",
    "  \"I don't have time for kids.\",\n",
    "  'Now if only I could read this thing. Blasted tiny letters...',\n",
    "  'Knew what?',\n",
    "  'Did you have a question or not?',\n",
    "  'Yes!',\n",
    "  'Ooh, thanks!',\n",
    "  'Now I just need to find all the cleaners from wayyyy back in 1916.',\n",
    "  'Yikes... this could take a while.',\n",
    "  'Hi! *cough*',\n",
    "  'Can you help-',\n",
    "  '*cough cough*',\n",
    "  'Can you help me-',\n",
    "  '*COUGH COUGH COUGH*',\n",
    "  'Um, are you okay?',\n",
    "  \"Oh, I'm fine! Just a little hoarse.\",\n",
    "  'Ha! What do you call a pony with a sore throat?',\n",
    "  'Huh?',\n",
    "  'A little horse!',\n",
    "  \"Ha! You're funny.\",\n",
    "  'I got that one from my Gramps!',\n",
    "  'Can you help me? I need to find the owner of this slip.',\n",
    "  \"Yup, that's him!\",\n",
    "  \"Unless you're too busy horsing around.\",\n",
    "  'Ha! Good one.',\n",
    "  \"You look like you're on a mission.\",\n",
    "  'Two missions, actually!',\n",
    "  \"Please let me know if you do. It's important!\",\n",
    "  'Oh my!',\n",
    "  'I need to find Wells right away!! Do you know where he is?',\n",
    "  'I need to find Wells!!!',\n",
    "  \"Calm down, kid. I haven't seen him.\",\n",
    "  \"I can't calm down. This is important!\",\n",
    "  \"I think it's a flag! Pretty spiffy, eh?\",\n",
    "  \"I don't have time for this, Gramps.\",\n",
    "  'Teddy is still missing!',\n",
    "  \"Great Scott, you're right!\",\n",
    "  \"Let's follow those scratch marks!\",\n",
    "  \"Jo! I can't go with you. I need to take the artifact upstairs.\",\n",
    "  \"It's okay, Gramps. I'll go by myself.\",\n",
    "  '\\\\u00f0\\\\u0178\\\\u02dc\\\\u00ad',\n",
    "  '\\\\u00e2\\\\u009d\\\\u00a4\\\\u00ef\\\\u00b8\\\\u008f',\n",
    "  'GRRRRRRR',\n",
    "  'GAH! And what is THAT doing out of its cage?!',\n",
    "  'You stole Teddy! How could you?!',\n",
    "  \"No he hasn't!\",\n",
    "  '\\\\u00f0\\\\u0178\\\\u02dc\\\\u0090',\n",
    "  'Teddy! Did you really eat his lunch?',\n",
    "  \"Did you steal Gramps's paperwork too?!\",\n",
    "  'And my homework?!?!',\n",
    "  'See?!',\n",
    "  \"That thing's a monster!\",\n",
    "  \"I don't have time for this.\",\n",
    "  'YEAH!',\n",
    "  'Wait- me?',\n",
    "  \"You can't just steal Jolie's pet. Don't you know badgers are protected animals?\",\n",
    "  'Besides, he looks friendly to me.',\n",
    "  'Wha?!',\n",
    "  '\\\\u00f0\\\\u0178\\\\u02dc\\\\u009d',\n",
    "  'Come on, Teddy.',\n",
    "  \"Let's go find Gramps!\",\n",
    "  \"Teddy! I'm sure glad to see you.\",\n",
    "  'Gadzooks! Poor critter.',\n",
    "  'Aha! Good catch, Jo.',\n",
    "  'Not sure. Do I look like a deer expert to you?',\n",
    "  'Ugh. I have to head over there and check out the animals.',\n",
    "  'FINE. That possum better not scratch my leather seats...',\n",
    "  \"He's a badger!\",\n",
    "  '\\\\u00f0\\\\u0178\\\\u00a7\\\\u02dc',\n",
    "  'Yoga does sound nice.',\n",
    "  \"But cranes can't do yoga, Teddy!\",\n",
    "  '\\\\u00f0\\\\u0178\\\\u008d\\\\u00a9',\n",
    "  \"Cranes don't eat donuts!\",\n",
    "  'Besides, you just ate my last snack.',\n",
    "  \"Gah. I can't believe this.\",\n",
    "  \"I'm a historian, not a zookeeper!\",\n",
    "  'And this place is dirty, and itchy, and-',\n",
    "  'I love it!',\n",
    "  \"Of course you do. You've got a rodent following you around.\",\n",
    "  \"Actually, badgers aren't rodents-\",\n",
    "  'Whatever.',\n",
    "  'Great. Just great. Could this day get any worse?!',\n",
    "  'I think I can help with your animal problem.',\n",
    "  \"Ha! I don't need your help.\",\n",
    "  \"Fine. Then I guess you don't want a real, live badger for the exhibit.\",\n",
    "  \"Yes!!! I'm saved!\",\n",
    "  'A real, live ferret!',\n",
    "  \"He's. A. Badger.\",\n",
    "  'And we still need to figure out that flag!',\n",
    "  \"Fine, fine. Let's see...\",\n",
    "  'A vexy-wha?',\n",
    "  'Ooh... \\\\Ecology flag, by Ron Cobb.\\\\',\n",
    "  'The boss is gonna love it!!!',\n",
    "  \"Oh, trust me. He'll make time.\",\n",
    "  \"You again! Don't let him hurt me!\",\n",
    "  '\\\\u00f0\\\\u0178\\\\u2122\\\\u201e',\n",
    "  \"Actually, we're just here for some photos.\",\n",
    "  'Guess so!',\n",
    "  'YOU?!',\n",
    "  \"Just please, don't let your badger eat them!\",\n",
    "  'Ugh. Meetings are so boring.',\n",
    "  'So? History is boring!',\n",
    "  'A boring old shirt.',\n",
    "  'Do I have to?',\n",
    "  'I need to find Wells right away! Do you know where he is?',\n",
    "  'Where should I go again?',\n",
    "  'You could try the archivist. Maybe he can help you find Wells!',\n",
    "  \"I can't go with you. I need to take the artifact upstairs.\",\n",
    "  \"Yes, he has. I've seen him eating homework and important papers, too.\",\n",
    "  'Yeah. Thanks anyway.',\n",
    "  \"I don't need that right now.\",\n",
    "  'Yes! This cool old slip from 1916.',\n",
    "  'I should stay and look for clues!',\n",
    "  'Are you okay?',\n",
    "  \"I'll be in the collection room. Come find me when you're ready to check out the artifact.\",\n",
    "  'Good luck!',\n",
    "  'What?!',\n",
    "  'Can I ride with you?',\n",
    "  \"Don't worry, he won't! (And he's a badger, by the way.)\",\n",
    "  'Ugh... I think that lynx is looking at me funny.',\n",
    "  \"Don't worry, Teddy won't eat your lunch anymore!\",\n",
    "  \"We're just looking for photos for the flag display.\",\n",
    "  'Gramps is the best historian ever!',\n",
    "  'I used to have a magnifying glass around here\\\\u00e2\\\\u20ac\\\\u00a6',\n",
    "  \"You're still here? I'm trying to work!\",\n",
    "  'Run along to the university.',\n",
    "  \"But I hear the museum's got one on the loose!\",\n",
    "  \"Come on, kid. You're slowing me down.\",\n",
    "  \"I feel like I'm forgetting something.\",\n",
    "  \"Did you drop something, Dear? There's a card on the floor.\",\n",
    "  \"Weren't you going to check out our microfiche?\",\n",
    "  \"I'm sure you'll find Theodora in there somewhere!\",\n",
    "  'Head back to the museum. Your gramps is waiting for you.',\n",
    "  'I should see what Gramps is up to!',\n",
    "  'I should go talk to Gramps!',\n",
    "  'Um... what did you want me to do again?',\n",
    "  'Head over to the Basketball Center.',\n",
    "  'Hopefully you can rustle up some clues!',\n",
    "  'Well? What are you still doing here?',\n",
    "  'So much cleaning to do...',\n",
    "  'Take a look!',\n",
    "  'I found it!',\n",
    "  'Theodora wearing the shirt!',\n",
    "  'You better get to the capitol!',\n",
    "  'I should check out that pair of glasses.',\n",
    "  \"Check out the archives. They've got tons of old photos!\",\n",
    "  'Did you drop something, Dear?',\n",
    "  'Hopefully you can find some clues!',\n",
    "  'What are you waiting for? The Stacks are right outside the door.',\n",
    "  \"Why don't you go play with your grampa?\",\n",
    "  'Gramps said to look for clues. Better look around.',\n",
    "  'I should ask the librarian where to go next.',\n",
    "  'Ooh, nice decorations!',\n",
    "  'The libarian said I could find some information on Youmans in here...',\n",
    "  'I should ask the librarian why Wells was here.',\n",
    "  'Have a look at the artifact!',\n",
    "  \"I wonder if there's a clue in those business cards...\",\n",
    "  'Hi, Mrs. M.',\n",
    "  'Thanks. Did you figure out the shirt?',\n",
    "  'I should find out if she can help me!',\n",
    "  'What is it, Teddy?',\n",
    "  'Oh no... they got sick from polluted water?',\n",
    "  'Poor foxes!',\n",
    "  'Jolie! Where have you been?',\n",
    "  'The exhibit opens tomorrow.',\n",
    "  'Welcome back, Jolie. Did you figure out the shirt?',\n",
    "  'Nice decorations.',\n",
    "  'Wells got in trouble for littering at the Wildlife Center.',\n",
    "  'I should check that logbook to see who owned this slip...',\n",
    "  'AND I know who took Teddy!',\n",
    "  'Who is Teddy?',\n",
    "  \"And where's your grampa?\",\n",
    "  'Sorry for the delay, Boss.',\n",
    "  'I had some cleaning up to do in my office.',\n",
    "  'Mrs. M, I think Wells kidnapped Teddy.',\n",
    "  \"And he messed up Gramps's office, too!\",\n",
    "  'One step at a time, Jo.',\n",
    "  'Did you figure out the shirt?',\n",
    "  'I knew you could do it, Jo!',\n",
    "  'Now can I tell you what happened to Teddy?',\n",
    "  'He needs our help!',\n",
    "  \"Sorry I'm late.\",\n",
    "  \"Wells! Where's Teddy? Is he okay?\",\n",
    "  'I figured out that you kidnapped him!',\n",
    "  'Easy, Jo.',\n",
    "  \"Why don't you prove your case?\",\n",
    "  \"It'll be okay, Jo. We'll find Teddy!\",\n",
    "  'Nice work on the shirt, Jolie!',\n",
    "  'Leopold, can you run back to the museum?',\n",
    "  'Sounds good, Boss.',\n",
    "  'Jo, meet me back at my office.',\n",
    "  'I hope you find your badger, kid.',\n",
    "  'Thanks!',\n",
    "  \"Are you going home now? Tomorrow's the big day!\",\n",
    "  'He got a park named after him? Cool!',\n",
    "  'Come on, Jo!',\n",
    "  \"Meet me back in my office and we'll get started!\"],\n",
    " 'fqid': ['intro',\n",
    "  'gramps',\n",
    "  'teddy',\n",
    "  'photo',\n",
    "  'notebook',\n",
    "  'retirement_letter',\n",
    "  'tobasement',\n",
    "  'janitor',\n",
    "  'toentry',\n",
    "  'groupconvo',\n",
    "  'report',\n",
    "  'boss',\n",
    "  'wells',\n",
    "  'directory',\n",
    "  'tocollection',\n",
    "  'cs',\n",
    "  'tunic',\n",
    "  'tunic.hub.slip',\n",
    "  'tostacks',\n",
    "  'outtolunch',\n",
    "  'tocloset',\n",
    "  'tomap',\n",
    "  'tunic.historicalsociety',\n",
    "  'tunic.kohlcenter',\n",
    "  'plaque',\n",
    "  'plaque.face.date',\n",
    "  'togrampa',\n",
    "  'tunic.capitol_0',\n",
    "  'chap1_finale',\n",
    "  'chap1_finale_c',\n",
    "  'tocloset_dirty',\n",
    "  'what_happened',\n",
    "  'trigger_scarf',\n",
    "  'trigger_coffee',\n",
    "  'tunic.capitol_1',\n",
    "  'tofrontdesk',\n",
    "  'archivist',\n",
    "  'magnify',\n",
    "  'tunic.humanecology',\n",
    "  'worker',\n",
    "  'businesscards',\n",
    "  'businesscards.card_0.next',\n",
    "  'businesscards.card_1.next',\n",
    "  'businesscards.card_bingo.next',\n",
    "  'businesscards.card_bingo.bingo',\n",
    "  'tohallway',\n",
    "  'tunic.drycleaner',\n",
    "  'logbook',\n",
    "  'logbook.page.bingo',\n",
    "  'tunic.library',\n",
    "  'tomicrofiche',\n",
    "  'reader',\n",
    "  'reader.paper0.next',\n",
    "  'reader.paper1.next',\n",
    "  'reader.paper2.bingo',\n",
    "  'wellsbadge',\n",
    "  'journals',\n",
    "  'journals.hub.topics',\n",
    "  'journals.pic_0.next',\n",
    "  'journals.pic_1.next',\n",
    "  'journals.pic_2.bingo',\n",
    "  'chap2_finale_c',\n",
    "  'ch3start',\n",
    "  'seescratches',\n",
    "  'tocage',\n",
    "  'glasses',\n",
    "  'directory.closeup.archivist',\n",
    "  'key',\n",
    "  'unlockdoor',\n",
    "  'confrontation',\n",
    "  'savedteddy',\n",
    "  'tocollectionflag',\n",
    "  'groupconvo_flag',\n",
    "  'tunic.capitol_2',\n",
    "  'tunic.wildlife',\n",
    "  'coffee',\n",
    "  'crane_ranger',\n",
    "  'remove_cup',\n",
    "  'expert',\n",
    "  'tracks',\n",
    "  'tracks.hub.deer',\n",
    "  'tunic.flaghouse',\n",
    "  'flag_girl',\n",
    "  'colorbook',\n",
    "  'reader_flag',\n",
    "  'reader_flag.paper0.next',\n",
    "  'reader_flag.paper1.next',\n",
    "  'reader_flag.paper2.bingo',\n",
    "  'archivist_glasses',\n",
    "  'journals_flag',\n",
    "  'journals_flag.hub.topics_old',\n",
    "  'journals_flag.hub.topics',\n",
    "  'journals_flag.pic_0.bingo',\n",
    "  'journals_flag.pic_0.next',\n",
    "  'chap4_finale_c',\n",
    "  'block_tocollection',\n",
    "  'reader.paper2.next',\n",
    "  'journals.pic_2.next',\n",
    "  'lockeddoor',\n",
    "  'reader.paper2.prev',\n",
    "  'reader.paper0.prev',\n",
    "  'reader_flag.paper1.prev',\n",
    "  'journals_flag.pic_0_old.next',\n",
    "  'journals_flag.pic_1_old.next',\n",
    "  'door_block_clean',\n",
    "  'door_block_talk',\n",
    "  'block',\n",
    "  'reader_flag.paper2.next',\n",
    "  'journals_flag.pic_1.bingo',\n",
    "  'journals_flag.pic_1.next',\n",
    "  'journals_flag.pic_2.bingo',\n",
    "  'journals_flag.pic_2.next',\n",
    "  'reader_flag.paper0.prev',\n",
    "  'reader.paper1.prev',\n",
    "  'block_magnify',\n",
    "  'journals_flag.pic_2_old.next',\n",
    "  'block_0',\n",
    "  'doorblock',\n",
    "  'block_tomap1',\n",
    "  'block_tomap2',\n",
    "  'reader_flag.paper2.prev',\n",
    "  'need_glasses',\n",
    "  'block_badge',\n",
    "  'block_nelson',\n",
    "  'block_badge_2',\n",
    "  'block_1',\n",
    "  'fox'],\n",
    " 'room_fqid': ['tunic.historicalsociety.closet',\n",
    "  'tunic.historicalsociety.basement',\n",
    "  'tunic.historicalsociety.entry',\n",
    "  'tunic.historicalsociety.collection',\n",
    "  'tunic.historicalsociety.stacks',\n",
    "  'tunic.kohlcenter.halloffame',\n",
    "  'tunic.capitol_0.hall',\n",
    "  'tunic.historicalsociety.closet_dirty',\n",
    "  'tunic.historicalsociety.frontdesk',\n",
    "  'tunic.humanecology.frontdesk',\n",
    "  'tunic.drycleaner.frontdesk',\n",
    "  'tunic.library.frontdesk',\n",
    "  'tunic.library.microfiche',\n",
    "  'tunic.capitol_1.hall',\n",
    "  'tunic.historicalsociety.cage',\n",
    "  'tunic.historicalsociety.collection_flag',\n",
    "  'tunic.wildlife.center',\n",
    "  'tunic.flaghouse.entry',\n",
    "  'tunic.capitol_2.hall'],\n",
    " 'text_fqid': ['tunic.historicalsociety.closet.intro',\n",
    "  'tunic.historicalsociety.closet.gramps.intro_0_cs_0',\n",
    "  'tunic.historicalsociety.closet.teddy.intro_0_cs_0',\n",
    "  'tunic.historicalsociety.closet.teddy.intro_0_cs_5',\n",
    "  'tunic.historicalsociety.closet.photo',\n",
    "  'tunic.historicalsociety.closet.notebook',\n",
    "  'tunic.historicalsociety.closet.retirement_letter.hub',\n",
    "  'tunic.historicalsociety.basement.janitor',\n",
    "  'tunic.historicalsociety.entry.groupconvo',\n",
    "  'tunic.historicalsociety.entry.boss.talktogramps',\n",
    "  'tunic.historicalsociety.entry.wells.talktogramps',\n",
    "  'tunic.historicalsociety.collection.cs',\n",
    "  'tunic.historicalsociety.collection.tunic.slip',\n",
    "  'tunic.historicalsociety.collection.gramps.found',\n",
    "  'tunic.historicalsociety.stacks.outtolunch',\n",
    "  'tunic.kohlcenter.halloffame.plaque.face.date',\n",
    "  'tunic.kohlcenter.halloffame.togrampa',\n",
    "  'tunic.capitol_0.hall.boss.talktogramps',\n",
    "  'tunic.historicalsociety.closet_dirty.what_happened',\n",
    "  'tunic.historicalsociety.closet_dirty.gramps.helpclean',\n",
    "  'tunic.historicalsociety.closet_dirty.trigger_scarf',\n",
    "  'tunic.historicalsociety.closet_dirty.trigger_coffee',\n",
    "  'tunic.historicalsociety.closet_dirty.gramps.news',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.hello',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.need_glass_0',\n",
    "  'tunic.historicalsociety.frontdesk.magnify',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.have_glass',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.have_glass_recap',\n",
    "  'tunic.humanecology.frontdesk.worker.intro',\n",
    "  'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo',\n",
    "  'tunic.humanecology.frontdesk.worker.badger',\n",
    "  'tunic.drycleaner.frontdesk.worker.hub',\n",
    "  'tunic.drycleaner.frontdesk.logbook.page.bingo',\n",
    "  'tunic.drycleaner.frontdesk.worker.done',\n",
    "  'tunic.library.frontdesk.worker.hello',\n",
    "  'tunic.library.microfiche.reader.paper2.bingo',\n",
    "  'tunic.library.frontdesk.wellsbadge.hub',\n",
    "  'tunic.library.frontdesk.worker.wells',\n",
    "  'tunic.capitol_1.hall.boss.haveyougotit',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.newspaper',\n",
    "  'tunic.historicalsociety.stacks.journals.pic_2.bingo',\n",
    "  'tunic.historicalsociety.basement.ch3start',\n",
    "  'tunic.historicalsociety.basement.gramps.whatdo',\n",
    "  'tunic.historicalsociety.basement.seescratches',\n",
    "  'tunic.historicalsociety.cage.glasses.beforeteddy',\n",
    "  'tunic.historicalsociety.cage.teddy.trapped',\n",
    "  'tunic.historicalsociety.cage.glasses.afterteddy',\n",
    "  'tunic.historicalsociety.entry.directory.closeup.archivist',\n",
    "  'tunic.historicalsociety.frontdesk.key',\n",
    "  'tunic.historicalsociety.cage.unlockdoor',\n",
    "  'tunic.historicalsociety.cage.confrontation',\n",
    "  'tunic.historicalsociety.basement.savedteddy',\n",
    "  'tunic.historicalsociety.collection_flag.gramps.flag',\n",
    "  'tunic.historicalsociety.entry.groupconvo_flag',\n",
    "  'tunic.historicalsociety.entry.wells.flag',\n",
    "  'tunic.historicalsociety.entry.boss.flag',\n",
    "  'tunic.historicalsociety.entry.wells.flag_recap',\n",
    "  'tunic.historicalsociety.entry.boss.flag_recap',\n",
    "  'tunic.wildlife.center.coffee',\n",
    "  'tunic.wildlife.center.wells.animals',\n",
    "  'tunic.wildlife.center.wells.animals2',\n",
    "  'tunic.wildlife.center.crane_ranger.crane',\n",
    "  'tunic.wildlife.center.remove_cup',\n",
    "  'tunic.wildlife.center.expert.removed_cup',\n",
    "  'tunic.wildlife.center.tracks.hub.deer',\n",
    "  'tunic.wildlife.center.expert.recap',\n",
    "  'tunic.wildlife.center.wells.nodeer',\n",
    "  'tunic.flaghouse.entry.flag_girl.hello',\n",
    "  'tunic.flaghouse.entry.colorbook',\n",
    "  'tunic.flaghouse.entry.flag_girl.symbol',\n",
    "  'tunic.flaghouse.entry.flag_girl.symbol_recap',\n",
    "  'tunic.library.frontdesk.worker.flag',\n",
    "  'tunic.library.microfiche.reader_flag.paper2.bingo',\n",
    "  'tunic.library.frontdesk.worker.nelson',\n",
    "  'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation',\n",
    "  'tunic.historicalsociety.stacks.journals_flag.pic_0.bingo',\n",
    "  'tunic.historicalsociety.entry.block_tocollection',\n",
    "  'tunic.historicalsociety.closet_dirty.gramps.archivist',\n",
    "  'tunic.historicalsociety.cage.lockeddoor',\n",
    "  'tunic.capitol_2.hall.boss.haveyougotit',\n",
    "  'tunic.drycleaner.frontdesk.worker.done2',\n",
    "  'tunic.library.frontdesk.worker.preflag',\n",
    "  'tunic.historicalsociety.closet_dirty.photo',\n",
    "  'tunic.historicalsociety.collection_flag.gramps.recap',\n",
    "  'tunic.wildlife.center.wells.nodeer_recap',\n",
    "  'tunic.library.frontdesk.worker.flag_recap',\n",
    "  'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap',\n",
    "  'tunic.historicalsociety.closet_dirty.door_block_clean',\n",
    "  'tunic.historicalsociety.closet_dirty.door_block_talk',\n",
    "  'tunic.historicalsociety.stacks.block',\n",
    "  'tunic.flaghouse.entry.flag_girl.hello_recap',\n",
    "  'tunic.historicalsociety.stacks.journals_flag.pic_1.bingo',\n",
    "  'tunic.historicalsociety.stacks.journals_flag.pic_2.bingo',\n",
    "  'tunic.historicalsociety.collection.tunic',\n",
    "  'tunic.library.frontdesk.worker.wells_recap',\n",
    "  'tunic.historicalsociety.frontdesk.block_magnify',\n",
    "  'tunic.humanecology.frontdesk.block_0',\n",
    "  'tunic.historicalsociety.basement.gramps.seeyalater',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.need_glass_1',\n",
    "  'tunic.historicalsociety.closet.doorblock',\n",
    "  'tunic.library.frontdesk.worker.droppedbadge',\n",
    "  'tunic.library.frontdesk.worker.hello_short',\n",
    "  'tunic.capitol_1.hall.boss.writeitup',\n",
    "  'tunic.historicalsociety.entry.block_tomap1',\n",
    "  'tunic.historicalsociety.entry.block_tomap2',\n",
    "  'tunic.capitol_0.hall.chap1_finale_c',\n",
    "  'tunic.historicalsociety.collection.gramps.lost',\n",
    "  'tunic.historicalsociety.closet_dirty.gramps.nothing',\n",
    "  'tunic.drycleaner.frontdesk.worker.takealook',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.foundtheodora',\n",
    "  'tunic.historicalsociety.cage.need_glasses',\n",
    "  'tunic.library.frontdesk.worker.nelson_recap',\n",
    "  'tunic.library.frontdesk.block_badge',\n",
    "  'tunic.historicalsociety.frontdesk.archivist.newspaper_recap',\n",
    "  'tunic.kohlcenter.halloffame.block_0',\n",
    "  'tunic.library.frontdesk.block_nelson',\n",
    "  'tunic.capitol_1.hall.chap2_finale_c',\n",
    "  'tunic.library.microfiche.block_0',\n",
    "  'tunic.library.frontdesk.block_badge_2',\n",
    "  'tunic.capitol_2.hall.chap4_finale_c',\n",
    "  'tunic.historicalsociety.collection.gramps.look_0',\n",
    "  'tunic.humanecology.frontdesk.block_1',\n",
    "  'tunic.drycleaner.frontdesk.block_0',\n",
    "  'tunic.wildlife.center.fox.concern',\n",
    "  'tunic.historicalsociety.entry.gramps.hub',\n",
    "  'tunic.drycleaner.frontdesk.block_1'],\n",
    "  'event_name+fqid':[\n",
    "  'cutscene_click_intro', \n",
    "  'person_click_gramps',\n",
    "  'person_click_teddy',\n",
    "  'navigate_click_teddy',\n",
    "  'navigate_click_photo', \n",
    "  'observation_click_photo',\n",
    "  'navigate_click_notebook', \n",
    "  'object_click_notebook',\n",
    "  'navigate_click_retirement_letter',\n",
    "  'object_click_retirement_letter',\n",
    "  'navigate_click_tobasement',\n",
    "  'navigate_click_janitor',\n",
    "  'observation_click_janitor',\n",
    "  'navigate_click_toentry', \n",
    "  'navigate_click_groupconvo',\n",
    "  'cutscene_click_groupconvo', \n",
    "  'object_hover_groupconvo',\n",
    "  'object_click_report', \n",
    "  'navigate_click_boss', \n",
    "  'person_click_boss',\n",
    "  'navigate_click_wells', \n",
    "  'person_click_wells',\n",
    "  'navigate_click_directory', \n",
    "  'object_click_directory',\n",
    "  'navigate_click_tocollection', \n",
    "  'cutscene_click_cs',\n",
    "  'navigate_click_tunic', \n",
    "  'object_hover_tunic', \n",
    "  'object_click_tunic',\n",
    "  'object_click_tunic.hub.slip', \n",
    "  'object_hover_tunic.hub.slip',\n",
    "  'navigate_click_gramps', \n",
    "  'navigate_click_tostacks',\n",
    "  'navigate_click_outtolunch', \n",
    "  'observation_click_outtolunch',\n",
    "  'navigate_click_tocloset', \n",
    "  'navigate_click_tomap',\n",
    "  'map_hover_tunic.historicalsociety', \n",
    "  'map_hover_tunic.kohlcenter',\n",
    "  'map_click_tunic.kohlcenter', \n",
    "  'navigate_click_plaque',\n",
    "  'object_click_plaque.face.date', \n",
    "  'object_click_plaque',\n",
    "  'object_hover_plaque.face.date', \n",
    "  'cutscene_click_togrampa',\n",
    "  'map_hover_toentry', \n",
    "  'map_click_tunic.capitol_0',\n",
    "  'navigate_click_chap1_finale', \n",
    "  'checkpoint_chap1_finale_c',\n",
    "  'map_click_tunic.historicalsociety',\n",
    "  'navigate_click_tocloset_dirty', \n",
    "  'cutscene_click_what_happened',\n",
    "  'navigate_click_trigger_scarf', \n",
    "  'cutscene_click_trigger_scarf',\n",
    "  'navigate_click_trigger_coffee', \n",
    "  'cutscene_click_trigger_coffee',\n",
    "  'map_hover_tomap', \n",
    "  'map_hover_tunic.capitol_1',\n",
    "  'navigate_click_tofrontdesk', \n",
    "  'navigate_click_archivist',\n",
    "  'person_click_archivist', \n",
    "  'navigate_click_magnify',\n",
    "  'observation_click_magnify', \n",
    "  'map_click_tunic.humanecology',\n",
    "  'navigate_click_worker', \n",
    "  'person_click_worker',\n",
    "  'navigate_click_businesscards',\n",
    "  'object_hover_businesscards.card_0.next',\n",
    "  'object_click_businesscards.card_0.next',\n",
    "  'object_click_businesscards.card_1.next',\n",
    "  'object_hover_businesscards.card_1.next',\n",
    "  'object_click_businesscards',\n",
    "  'object_hover_businesscards.card_bingo.next',\n",
    "  'object_click_businesscards.card_bingo.bingo',\n",
    "  'object_hover_businesscards.card_bingo.bingo',\n",
    "  'navigate_click_tohallway', \n",
    "  'map_click_tunic.drycleaner',\n",
    "  'navigate_click_logbook', \n",
    "  'object_hover_logbook.page.bingo',\n",
    "  'object_click_logbook', \n",
    "  'object_click_logbook.page.bingo',\n",
    "  'map_click_tunic.library', \n",
    "  'navigate_click_tomicrofiche',\n",
    "  'navigate_click_reader', \n",
    "  'object_hover_reader',\n",
    "  'object_hover_reader.paper0.next',\n",
    "  'object_click_reader.paper0.next',\n",
    "  'object_click_reader.paper1.next',\n",
    "  'object_click_reader.paper2.bingo',\n",
    "  'object_hover_reader.paper2.bingo', \n",
    "  'object_click_reader',\n",
    "  'navigate_click_wellsbadge', \n",
    "  'object_click_wellsbadge',\n",
    "  'map_hover_tunic.library', \n",
    "  'map_click_tunic.capitol_1',\n",
    "  'map_hover_tunic.drycleaner', \n",
    "  'navigate_click_journals',\n",
    "  'object_hover_journals', \n",
    "  'object_hover_journals.hub.topics',\n",
    "  'object_click_journals', \n",
    "  'object_click_journals.hub.topics',\n",
    "  'object_hover_journals.pic_0.next',\n",
    "  'object_click_journals.pic_0.next',\n",
    "  'object_click_journals.pic_1.next',\n",
    "  'object_click_journals.pic_2.bingo',\n",
    "  'object_hover_journals.pic_2.bingo',\n",
    "  'navigate_click_chap2_finale_c', \n",
    "  'checkpoint_chap2_finale_c',\n",
    "  'cutscene_click_ch3start', \n",
    "  'navigate_click_seescratches',\n",
    "  'cutscene_click_seescratches', \n",
    "  'navigate_click_tocage',\n",
    "  'navigate_click_glasses', \n",
    "  'person_click_glasses',\n",
    "  'object_hover_directory',\n",
    "  'object_hover_directory.closeup.archivist',\n",
    "  'object_click_directory.closeup.archivist', \n",
    "  'navigate_click_key',\n",
    "  'observation_click_key', \n",
    "  'navigate_click_unlockdoor',\n",
    "  'cutscene_click_unlockdoor', \n",
    "  'navigate_click_confrontation',\n",
    "  'cutscene_click_confrontation', \n",
    "  'cutscene_click_savedteddy',\n",
    "  'navigate_click_tocollectionflag',\n",
    "  'navigate_click_groupconvo_flag', \n",
    "  'cutscene_click_groupconvo_flag',\n",
    "  'map_hover_tunic.capitol_2', \n",
    "  'map_click_tunic.wildlife',\n",
    "  'navigate_click_coffee', \n",
    "  'observation_click_coffee',\n",
    "  'navigate_click_crane_ranger', \n",
    "  'person_click_crane_ranger',\n",
    "  'navigate_click_remove_cup', \n",
    "  'observation_click_remove_cup',\n",
    "  'navigate_click_expert', \n",
    "  'person_click_expert',\n",
    "  'navigate_click_tracks', \n",
    "  'object_hover_tracks',\n",
    "  'object_hover_tracks.hub.deer', \n",
    "  'object_click_tracks.hub.deer',\n",
    "  'object_click_tracks', \n",
    "  'map_hover_tunic.flaghouse',\n",
    "  'map_click_tunic.flaghouse', \n",
    "  'navigate_click_flag_girl',\n",
    "  'person_click_flag_girl', \n",
    "  'navigate_click_colorbook',\n",
    "  'object_click_colorbook', \n",
    "  'navigate_click_reader_flag',\n",
    "  'object_click_reader_flag', \n",
    "  'object_click_reader_flag.paper0.next',\n",
    "  'object_click_reader_flag.paper1.next',\n",
    "  'object_hover_reader_flag.paper0.next',\n",
    "  'object_click_reader_flag.paper2.bingo',\n",
    "  'object_hover_reader_flag.paper2.bingo',\n",
    "  'navigate_click_archivist_glasses',\n",
    "  'person_click_archivist_glasses', \n",
    "  'navigate_click_journals_flag',\n",
    "  'object_hover_journals_flag.hub.topics_old',\n",
    "  'object_click_journals_flag.hub.topics',\n",
    "  'object_hover_journals_flag.hub.topics',\n",
    "  'object_hover_journals_flag.pic_0.bingo',\n",
    "  'object_hover_journals_flag.pic_0.next',\n",
    "  'object_click_journals_flag.pic_0.bingo',\n",
    "  'object_click_journals_flag', \n",
    "  'map_click_tunic.capitol_2',\n",
    "  'navigate_click_chap4_finale_c', \n",
    "  'checkpoint_chap4_finale_c',\n",
    "  'navigate_click_block_tocollection',\n",
    "  'observation_click_block_tocollection',\n",
    "  'object_click_businesscards.card_bingo.next',\n",
    "  'map_hover_tohallway', \n",
    "  'object_hover_logbook',\n",
    "  'object_hover_reader.paper1.next',\n",
    "  'object_hover_reader.paper2.next', \n",
    "  'map_hover_tunic.humanecology',\n",
    "  'object_hover_journals.pic_1.next',\n",
    "  'object_click_journals.pic_2.next',\n",
    "  'object_hover_journals.pic_2.next', \n",
    "  'map_hover_tobasement',\n",
    "  'navigate_click_lockeddoor', \n",
    "  'observation_click_lockeddoor',\n",
    "  'object_hover_plaque', \n",
    "  'object_hover_reader.paper2.prev',\n",
    "  'object_click_reader.paper2.next',\n",
    "  'object_hover_reader.paper0.prev', \n",
    "  'map_hover_tunic.wildlife',\n",
    "  'object_hover_reader_flag', \n",
    "  'object_hover_reader_flag.paper1.prev',\n",
    "  'object_hover_reader_flag.paper1.next',\n",
    "  'object_click_journals_flag.hub.topics_old',\n",
    "  'object_click_journals_flag.pic_0_old.next',\n",
    "  'object_click_journals_flag.pic_1_old.next',\n",
    "  'object_hover_journals_flag.pic_0_old.next',\n",
    "  'object_hover_notebook', \n",
    "  'navigate_click_door_block_clean',\n",
    "  'cutscene_click_door_block_clean',\n",
    "  'navigate_click_door_block_talk', \n",
    "  'cutscene_click_door_block_talk',\n",
    "  'navigate_click_block', \n",
    "  'observation_click_block',\n",
    "  'object_hover_reader_flag.paper2.next',\n",
    "  'object_click_journals_flag.pic_0.next',\n",
    "  'object_click_journals_flag.pic_1.bingo',\n",
    "  'object_hover_journals_flag.pic_1.bingo',\n",
    "  'object_click_journals_flag.pic_1.next',\n",
    "  'object_hover_journals_flag.pic_1.next',\n",
    "  'object_click_journals_flag.pic_2.bingo',\n",
    "  'object_hover_journals_flag.pic_2.bingo',\n",
    "  'object_click_journals_flag.pic_2.next',\n",
    "  'object_hover_journals_flag.pic_2.next', \n",
    "  'object_hover_gramps',\n",
    "  'map_hover_tunic.capitol_0', \n",
    "  'object_hover_businesscards',\n",
    "  'object_hover_wellsbadge', \n",
    "  'object_hover_colorbook',\n",
    "  'object_click_reader_flag.paper2.next',\n",
    "  'object_click_reader_flag.paper0.prev',\n",
    "  'object_hover_reader_flag.paper0.prev',\n",
    "  'object_hover_journals_flag', \n",
    "  'object_hover_reader.paper1.prev',\n",
    "  'object_hover_retirement_letter',\n",
    "  'object_click_reader.paper1.prev', \n",
    "  'object_hover_boss',\n",
    "  'navigate_click_block_magnify', \n",
    "  'observation_click_block_magnify',\n",
    "  'object_hover_journals_flag.pic_1_old.next',\n",
    "  'object_click_journals_flag.pic_2_old.next',\n",
    "  'object_hover_journals_flag.pic_2_old.next',\n",
    "  'navigate_click_block_0', \n",
    "  'observation_click_block_0',\n",
    "  'navigate_click_doorblock', \n",
    "  'observation_click_doorblock',\n",
    "  'object_hover_doorblock', \n",
    "  'object_click_reader.paper0.prev',\n",
    "  'navigate_click_block_tomap1', \n",
    "  'observation_click_block_tomap1',\n",
    "  'navigate_click_block_tomap2', \n",
    "  'observation_click_block_tomap2',\n",
    "  'cutscene_click_chap1_finale_c', \n",
    "  'map_hover_boss',\n",
    "  'object_hover_reader_flag.paper2.prev',\n",
    "  'object_click_reader_flag.paper2.prev', \n",
    "  'object_hover_wells',\n",
    "  'object_click_reader.paper2.prev', \n",
    "  'navigate_click_need_glasses',\n",
    "  'observation_click_need_glasses',\n",
    "  'object_click_reader_flag.paper1.prev', \n",
    "  'object_hover_tostacks',\n",
    "  'navigate_click_block_badge', \n",
    "  'cutscene_click_block_badge',\n",
    "  'object_hover_worker', \n",
    "  'navigate_click_block_nelson',\n",
    "  'observation_click_block_nelson', \n",
    "  'cutscene_click_chap2_finale_c',\n",
    "  'map_hover_flag_girl', \n",
    "  'object_hover_tobasement',\n",
    "  'navigate_click_block_badge_2', \n",
    "  'observation_click_block_badge_2',\n",
    "  'cutscene_click_chap4_finale_c', \n",
    "  'map_hover_tostacks',\n",
    "  'object_hover_block_0', \n",
    "  'object_hover_toentry',\n",
    "  'object_hover_tofrontdesk', \n",
    "  'navigate_click_block_1',\n",
    "  'observation_click_block_1', \n",
    "  'map_hover_worker',\n",
    "  'map_hover_logbook', \n",
    "  'object_hover_block',\n",
    "  'object_hover_tocollection', \n",
    "  'navigate_click_fox',\n",
    "  'person_click_fox', \n",
    "  'object_hover_tomap', \n",
    "  'object_hover_flag_girl',\n",
    "  'object_hover_expert', \n",
    "  'object_hover_crane_ranger',\n",
    "  'map_hover_wells', \n",
    "  'object_hover_block_tomap1', \n",
    "  'map_hover_coffee',\n",
    "  'map_hover_expert', \n",
    "  'map_hover_directory',\n",
    "  'object_hover_tocollectionflag', \n",
    "  'map_hover_colorbook',\n",
    "  'object_hover_tomicrofiche', \n",
    "  'object_hover_block_1']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTrain:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"session_id\", \"level_group\", \"elapsed_time\"], ignore_index=True)\n",
    "        self.features = self.sessions_df[[\"session_id\", \"level_group\"]].drop_duplicates().copy()\n",
    "        self.result = labels\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].shift(1)\n",
    "        self.sessions_df['event_name+fqid'] = self.sessions_df['event_name']+'_'+self.sessions_df['fqid']\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":\"record_cnt\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].agg([max,min]).reset_index()\n",
    "        add_features[\"group_elapsed_time\"] = add_features[\"max\"] - add_features[\"min\"]\n",
    "        add_features[\"group_elapsed_time\"] = add_features[\"group_elapsed_time\"].astype(np.float32)\n",
    "        add_features = add_features[[\"session_id\", \"level_group\", \"group_elapsed_time\"]].copy()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[cat_col]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[\"index\"].count().reset_index().rename(columns={\"index\":\"cnt\"})\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{cat_col}_{str(cat)}_record_cnt\"\n",
    "            tmp = add_features[add_features[cat_col]==cat][[\"session_id\", \"level_group\", \"cnt\"]].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp = tmp.rename(columns={\"cnt\": feat_name})\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[feat_name] = self.features[feat_name].fillna(0)\n",
    "            else:\n",
    "                self.features[feat_name] = 0\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.dropna(subset=[cat_col]).drop_duplicates([\"session_id\", \"level_group\", cat_col])\n",
    "        add_features = add_features.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{cat_col}_nunique\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")        \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        new_cols = [f\"{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "    def _cat_first_click_point(self, cat_col):\n",
    "        \"\"\"\n",
    "        {cat_col}ごとの初回のクリック座標\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[cat_col]\n",
    "        first_record = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col]).tail(1).reset_index(drop=True)\n",
    "        first_record = first_record[[\"session_id\", \"level_group\", cat_col, \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"]].copy()\n",
    "        \n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{cat_col}_{cat}_first_click_{val_name}\" for val_name in [\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"]]\n",
    "            add_features = first_record[first_record[cat_col]==cat].drop(columns=[cat_col])\n",
    "            if len(add_features) == 0:\n",
    "                self.features[new_cols] = np.nan\n",
    "            else:\n",
    "                add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "                self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def get_train(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\", \"elapsed_time\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\", \"elapsed_time\"],\n",
    "                            aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                            cat_col=\"event_name+fqid\")\n",
    "\n",
    "        self.result = self.result.merge(self.features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "  \n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesInf:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"elapsed_time\"], ignore_index=True)\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.use_cols = [\n",
    "            \"elapsed_time\", \"event_name\", \"name\", \"level\", \"page\",\n",
    "            \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\",\n",
    "            \"hover_duration\", \"text\", \"fqid\", \"room_fqid\", \"text_fqid\", \"event_name+fqid\"\n",
    "        ]\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df['event_name+fqid'] = self.sessions_df['event_name']+'_'+self.sessions_df['fqid']\n",
    "        # dataframeの各列をnumpy arrayで保持\n",
    "        self.sessions = {}\n",
    "        for c in self.use_cols:\n",
    "            self.sessions[c] = self.sessions_df[c].values\n",
    "        self.sessions[\"time_diff\"] = self.sessions[\"elapsed_time\"] - self.sessions_df[\"elapsed_time\"].shift(1).values\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_feature = len(self.sessions[\"elapsed_time\"])\n",
    "        self.result[\"record_cnt\"] = add_feature\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_feature = np.max(self.sessions[\"elapsed_time\"]) - np.min(self.sessions[\"elapsed_time\"])\n",
    "        self.result[\"group_elapsed_time\"] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[cat_col]\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{cat_col}_{str(cat)}_record_cnt\"\n",
    "            add_feature = (self.sessions[cat_col] == cat).astype(int).sum()\n",
    "            self.result[feat_name] = add_feature\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        self.result[f\"{cat_col}_nunique\"] = self.sessions_df[cat_col].dropna().nunique()       \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        for val_col, agg in itertools.product(val_cols, aggs):\n",
    "            feat_name = f\"{val_col}_{agg}\"\n",
    "            if agg == \"mean\":\n",
    "                add_feature = np.nanmean(self.sessions[val_col])\n",
    "            elif agg == \"max\":\n",
    "                add_feature = np.nanmax(self.sessions[val_col])\n",
    "            elif agg == \"min\":\n",
    "                add_feature = np.nanmin(self.sessions[val_col])\n",
    "            elif agg == \"std\":\n",
    "                add_feature = np.nanstd(self.sessions[val_col], ddof=1)\n",
    "            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    tmp = self.sessions[val_col][idx]\n",
    "                    if agg == \"mean\":\n",
    "                        add_feature = np.nanmean(tmp)\n",
    "                    elif agg == \"max\":\n",
    "                        add_feature = np.nanmax(tmp)\n",
    "                    elif agg == \"min\":\n",
    "                        add_feature = np.nanmin(tmp)\n",
    "                    elif agg == \"std\":\n",
    "                        add_feature = np.nanstd(tmp, ddof=1)\n",
    "                    if np.isnan(add_feature):\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def get_test(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\", \"elapsed_time\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")      \n",
    "        \n",
    "        self._cat_agg_features(val_cols=[\"time_diff\", \"elapsed_time\"],\n",
    "                        aggs=[\"mean\", \"max\", \"min\", \"std\"],\n",
    "                        cat_col=\"event_name+fqid\")    \n",
    "  \n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self):\n",
    "        self.store = {}\n",
    "        for c in features_used_total:\n",
    "            self.store[c] = defaultdict(int)\n",
    "\n",
    "    def record(self, train):\n",
    "        df = train.drop_duplicates(\"session_id\").set_index(\"session_id\")[features_used_total]\n",
    "        for session in df.index:\n",
    "            for c in features_used_total:\n",
    "                self.store[c][session] += df.at[session, c]\n",
    "\n",
    "    def add_total_features(self, train):\n",
    "        for c in features_used_total:\n",
    "            train[f\"total_{c}\"] = train[\"session_id\"].map(self.store[c])\n",
    "        return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels, hist):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesTrain(sessions, labels)\n",
    "    train = feat.get_train()\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    # level_groupの特徴量記録＆過去のgroup含めたtotal値の特徴量取得\n",
    "    hist.record(train)\n",
    "    train = hist.add_total_features(train)\n",
    "    return train, hist\n",
    "\n",
    "def get_test_dataset(sessions, labels, hist):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesInf(sessions, labels)\n",
    "    test = feat.get_test()\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    # level_groupの特徴量記録＆過去のgroup含めたtotal値の特徴量取得\n",
    "    hist.record(test)\n",
    "    test = hist.add_total_features(test)\n",
    "    return test, hist   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train_meta_features(oof):\n",
    "    oof[\"level_group\"] = oof[\"level_group\"].map({\"0-4\":0, \"5-12\":1, \"13-22\":2}).astype(int)\n",
    "    meta_df = oof.groupby([\"session_id\", \"level_group\"])[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "    meta_df = meta_df.rename(columns={\"mean\":\"prev_group_pred_mean\", \"max\":\"prev_group_pred_max\", \"min\":\"prev_group_pred_min\", \"std\":\"prev_group_pred_std\"})\n",
    "    meta_df[\"level_group\"] = meta_df[\"level_group\"] + 1\n",
    "    oof = oof.merge(meta_df, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "    oof = oof.drop(columns=[\"pred\"])\n",
    "    oof[\"level_group\"] = oof[\"level_group\"].astype(\"category\")\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    with open(cfg.prep_dir + 'iter_train.pkl', 'rb') as f:\n",
    "        iter_train = pickle.load(f) \n",
    "    \n",
    "    dfs = []\n",
    "    hist = History()\n",
    "    for group in level_group_list:\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train_group, hist = get_train_dataset(train_sessions, labels, hist)\n",
    "        dfs.append(train_group)\n",
    "    train = pd.concat(dfs, ignore_index=True)\n",
    "    # concatするとcategory型がリセットされてしまうので再度cast\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    # 1st round (meta_features算出用)\n",
    "    target = \"correct\"\n",
    "    not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "    gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "    fis = []\n",
    "    oofs = []\n",
    "    for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "        print(f\"fold : {i}\")\n",
    "        tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "        vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=2000000, early_stopping_rounds=100, verbose_eval=100)\n",
    "        \n",
    "        # valid_pred\n",
    "        oof_fold = train.iloc[vl_idx].copy()\n",
    "        oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oofs.append(oof_fold)\n",
    "    oof_1st_round = pd.concat(oofs)\n",
    "    oof_1st_round = oof_1st_round.reset_index(drop=True)\n",
    "\n",
    "    # 2nd round\n",
    "    train = prep_train_meta_features(oof_1st_round)\n",
    "    not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "    gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "    fis = []\n",
    "    oofs = []\n",
    "    for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "        print(f\"fold : {i}\")\n",
    "        tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "        vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "        tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "        vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "        model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                        num_boost_round=2000000, early_stopping_rounds=100, verbose_eval=100)\n",
    "        # モデル出力\n",
    "        model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model{i}.lgb\")\n",
    "        \n",
    "        # valid_pred\n",
    "        oof_fold = train.iloc[vl_idx].copy()\n",
    "        oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "        oofs.append(oof_fold)\n",
    "\n",
    "        # 特徴量重要度\n",
    "        fi_fold = pd.DataFrame()\n",
    "        fi_fold[\"feature\"] = model.feature_name()\n",
    "        fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "        fi_fold[\"fold\"] = i\n",
    "        fis.append(fi_fold)\n",
    "\n",
    "    fi = pd.concat(fis)    \n",
    "    fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi.csv\", index=False)\n",
    "    fi_n = fi['feature'].nunique()\n",
    "    order = list(fi.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\n",
    "    #plt.figure(figsize=(10, fi_n*0.2))\n",
    "    #sns.barplot(x=\"importance\", y=\"feature\", data=fi, order=order)\n",
    "    #plt.title(f\"LGBM importance\")\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(cfg.output_dir + f'{cfg.exp_name}/lgbm_importance.png')\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"session\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby([\"session_id\", \"level_group\"])]\n",
    "    subs = [df[1].drop(columns=[\"session\", \"session_level\"]).reset_index(drop=True) for df in sub.groupby([\"session\", \"level_group\"])]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_train_test_process_identity():\n",
    "    \"\"\"trainとtestのデータ加工の結果が一致することを確認する\n",
    "    \"\"\"\n",
    "    iter_train = get_mock_iter_train()\n",
    "    iter_test = get_mock_iter_test()\n",
    "\n",
    "    dfs = []\n",
    "    hist = History()\n",
    "    for (sessions, sub) in iter_train:\n",
    "        df_iter, hist = get_train_dataset(sessions, sub, hist)\n",
    "        dfs.append(df_iter)\n",
    "    train_process_df = pd.concat(dfs, ignore_index=True)\n",
    "    # concatするとcategory型がリセットされてしまうので再度cast\n",
    "    train_process_df[\"question\"] = train_process_df[\"question\"].astype(\"category\")\n",
    "    train_process_df = train_process_df.drop(columns=\"level_group\")\n",
    "\n",
    "    dfs = []\n",
    "    hist = History()\n",
    "    for (sessions, sub) in iter_test:\n",
    "        df_iter, hist = get_test_dataset(sessions, sub, hist)\n",
    "        dfs.append(df_iter)\n",
    "    test_process_df = pd.concat(dfs, ignore_index=True)\n",
    "    # concatするとcategory型がリセットされてしまうので再度cast\n",
    "    test_process_df[\"question\"] = train_process_df[\"question\"].astype(\"category\")\n",
    "\n",
    "    train_process_df = train_process_df.sort_values([\"session_id\", \"question\"]).reset_index(drop=True)\n",
    "    test_process_df = test_process_df.sort_values([\"session_id\", \"question\"]).reset_index(drop=True)\n",
    "    train_process_df = train_process_df[sorted(train_process_df.columns.tolist())]\n",
    "    test_process_df = test_process_df[sorted(test_process_df.columns.tolist())]\n",
    "\n",
    "    assert train_process_df.equals(test_process_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    models = []\n",
    "    for i in range(cfg.n_splits):\n",
    "        if mode == \"local_cv\":\n",
    "            model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model{i}.lgb\"\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_path = f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model{i}.lgb\"\n",
    "        models.append(lgb.Booster(model_file=model_path))\n",
    "    features = models[0].feature_name()\n",
    "\n",
    "    prev_group_pred_mean = {}\n",
    "    prev_group_pred_max = {}\n",
    "    prev_group_pred_min = {}\n",
    "    prev_group_pred_std = {}    \n",
    "    hist = History()\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        session_id = test_sessions[\"session_id\"].values[0]\n",
    "\n",
    "        test, hist = get_test_dataset(test_sessions, sample_submission, hist)\n",
    "\n",
    "        # 一つ前のlevel_groupの予測値\n",
    "        if level_group == \"0-4\":\n",
    "            # 0-4は前の予測が存在しないのでnullを入れる\n",
    "            test[\"prev_group_pred_mean\"] = np.nan\n",
    "            test[\"prev_group_pred_max\"] = np.nan\n",
    "            test[\"prev_group_pred_min\"] = np.nan\n",
    "            test[\"prev_group_pred_std\"] = np.nan\n",
    "        else:\n",
    "            test[\"prev_group_pred_mean\"] = prev_group_pred_mean[session_id]\n",
    "            test[\"prev_group_pred_max\"] = prev_group_pred_max[session_id]\n",
    "            test[\"prev_group_pred_min\"] = prev_group_pred_min[session_id]\n",
    "            test[\"prev_group_pred_std\"] = prev_group_pred_std[session_id]\n",
    "\n",
    "        preds = np.zeros(len(test))\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = models[i]\n",
    "            preds += model.predict(test[features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "\n",
    "        # 次のグループでの特徴量に使用するため結果を保存\n",
    "        prev_group_pred_mean[session_id] = np.mean(preds)\n",
    "        prev_group_pred_max[session_id] = np.max(preds)\n",
    "        prev_group_pred_min[session_id] = np.min(preds)\n",
    "        prev_group_pred_std[session_id] = np.std(preds)\n",
    "\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239789, number of negative: 99493\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 5.086432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945892\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.706754 -> initscore=0.879672\n",
      "[LightGBM] [Info] Start training from score 0.879672\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.451733\tvalid_1's binary_logloss: 0.486185\n",
      "[200]\ttraining's binary_logloss: 0.42749\tvalid_1's binary_logloss: 0.485734\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttraining's binary_logloss: 0.439104\tvalid_1's binary_logloss: 0.485557\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239066, number of negative: 100216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.948624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945596\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.704623 -> initscore=0.869412\n",
      "[LightGBM] [Info] Start training from score 0.869412\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.452873\tvalid_1's binary_logloss: 0.482794\n",
      "[200]\ttraining's binary_logloss: 0.428386\tvalid_1's binary_logloss: 0.482829\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's binary_logloss: 0.443693\tvalid_1's binary_logloss: 0.48237\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239010, number of negative: 100290\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.998316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945548\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.704421 -> initscore=0.868439\n",
      "[LightGBM] [Info] Start training from score 0.868439\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.453207\tvalid_1's binary_logloss: 0.482232\n",
      "[200]\ttraining's binary_logloss: 0.428951\tvalid_1's binary_logloss: 0.482062\n",
      "Early stopping, best iteration is:\n",
      "[142]\ttraining's binary_logloss: 0.441792\tvalid_1's binary_logloss: 0.481806\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239535, number of negative: 99765\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.945269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945358\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.705968 -> initscore=0.875882\n",
      "[LightGBM] [Info] Start training from score 0.875882\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.452116\tvalid_1's binary_logloss: 0.485631\n",
      "[200]\ttraining's binary_logloss: 0.428202\tvalid_1's binary_logloss: 0.485361\n",
      "Early stopping, best iteration is:\n",
      "[127]\ttraining's binary_logloss: 0.444545\tvalid_1's binary_logloss: 0.485046\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239684, number of negative: 99616\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.243107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945586\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 4312\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.706407 -> initscore=0.877999\n",
      "[LightGBM] [Info] Start training from score 0.877999\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.452243\tvalid_1's binary_logloss: 0.484458\n",
      "[200]\ttraining's binary_logloss: 0.428082\tvalid_1's binary_logloss: 0.483947\n",
      "Early stopping, best iteration is:\n",
      "[147]\ttraining's binary_logloss: 0.439682\tvalid_1's binary_logloss: 0.483657\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239789, number of negative: 99493\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.302112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 947421\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 4316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.706754 -> initscore=0.879672\n",
      "[LightGBM] [Info] Start training from score 0.879672\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.450783\tvalid_1's binary_logloss: 0.484754\n",
      "[200]\ttraining's binary_logloss: 0.426803\tvalid_1's binary_logloss: 0.484863\n",
      "Early stopping, best iteration is:\n",
      "[121]\ttraining's binary_logloss: 0.444836\tvalid_1's binary_logloss: 0.484646\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239066, number of negative: 100216\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.443024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 946949\n",
      "[LightGBM] [Info] Number of data points in the train set: 339282, number of used features: 4316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.704623 -> initscore=0.869412\n",
      "[LightGBM] [Info] Start training from score 0.869412\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.451646\tvalid_1's binary_logloss: 0.482246\n",
      "[200]\ttraining's binary_logloss: 0.427976\tvalid_1's binary_logloss: 0.481999\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttraining's binary_logloss: 0.432096\tvalid_1's binary_logloss: 0.481728\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239010, number of negative: 100290\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.114220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 947221\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 4316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.704421 -> initscore=0.868439\n",
      "[LightGBM] [Info] Start training from score 0.868439\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.452192\tvalid_1's binary_logloss: 0.481091\n",
      "[200]\ttraining's binary_logloss: 0.428272\tvalid_1's binary_logloss: 0.480918\n",
      "Early stopping, best iteration is:\n",
      "[163]\ttraining's binary_logloss: 0.435757\tvalid_1's binary_logloss: 0.480493\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239535, number of negative: 99765\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.660797 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 946896\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 4316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.705968 -> initscore=0.875882\n",
      "[LightGBM] [Info] Start training from score 0.875882\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.451402\tvalid_1's binary_logloss: 0.484439\n",
      "[200]\ttraining's binary_logloss: 0.427526\tvalid_1's binary_logloss: 0.484487\n",
      "Early stopping, best iteration is:\n",
      "[139]\ttraining's binary_logloss: 0.440894\tvalid_1's binary_logloss: 0.484334\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 239684, number of negative: 99616\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 5.108045 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 946983\n",
      "[LightGBM] [Info] Number of data points in the train set: 339300, number of used features: 4316\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.706407 -> initscore=0.877999\n",
      "[LightGBM] [Info] Start training from score 0.877999\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.451297\tvalid_1's binary_logloss: 0.483754\n",
      "[200]\ttraining's binary_logloss: 0.42741\tvalid_1's binary_logloss: 0.483375\n",
      "Early stopping, best iteration is:\n",
      "[159]\ttraining's binary_logloss: 0.436279\tvalid_1's binary_logloss: 0.483232\n",
      "logloss 0.482887\n",
      "best_score 0.694133\n",
      "best_threshold 0.630\n",
      "------------------------------\n",
      "Q1 : F1 = 0.612054\n",
      "Q2 : F1 = 0.531938\n",
      "Q3 : F1 = 0.577950\n",
      "Q4 : F1 = 0.652017\n",
      "Q5 : F1 = 0.383151\n",
      "Q6 : F1 = 0.617690\n",
      "Q7 : F1 = 0.564565\n",
      "Q8 : F1 = 0.336168\n",
      "Q9 : F1 = 0.568885\n",
      "Q10 : F1 = 0.352383\n",
      "Q11 : F1 = 0.394064\n",
      "Q12 : F1 = 0.586338\n",
      "Q13 : F1 = 0.420255\n",
      "Q14 : F1 = 0.513369\n",
      "Q15 : F1 = 0.355339\n",
      "Q16 : F1 = 0.416319\n",
      "Q17 : F1 = 0.349759\n",
      "Q18 : F1 = 0.554963\n",
      "[1 1 1]\n",
      "[1 0 1 1 1]\n",
      "[1 0 1 1 0 1 0 0 1 0]\n",
      "[0 1 1]\n",
      "[1 0 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 1 0]\n",
      "[1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 0]\n",
      "sample_inf処理時間 :  20.1 秒\n"
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    valid_train_test_process_identity()\n",
    "    run_train()\n",
    "inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
