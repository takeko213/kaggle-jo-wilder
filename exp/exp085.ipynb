{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp085"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量検討 5-12のfold0のみで"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  取り組み  | logloss |　採否 |\n",
    "| ---- |  ---- | ----- |\n",
    "| init |  0.532449 | ----- |  \n",
    "| object_hoverのduration関連の特徴量（各fqidごと） |  0.532201 | ----- |\n",
    "| agg修正・追加 |  0.532384 | ----- |\n",
    "| quantile025,075追加 |  0.532052 | ----- |\n",
    "| 相関係数の足切りを0.95に変更 |  0.532390 | ----- |  \n",
    "| カテゴリ集計からevent_name+fqid等を削除 | 0.532147  | ----- |\n",
    "| quantileを0.1刻みに変更 | 0.532385 | ----- |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  取り組み  | 元の特徴量数 | logloss | 特徴量100 | 特徴量250 | 特徴量500 | 採否 |\n",
    "|  ----  | ---- | ---- | ---- | ---- | ---- | ---- |  \n",
    "quantileを0.1刻みに変更|3229|0.532385|0.535384|0.532597|0.531612|\n",
    "quantileを0.25,0.75に戻す|2454|0.532302|0.534048|0.532634|0.531285| 〇\n",
    "座標の集計特徴量を削除|2446|0.532329|0.534069|0.532793|0.531133| 〇\n",
    "minigame特徴量|2458|0.532352|0.534219|0.532282|0.531338|\n",
    "minigameのクリック座標特徴量追加|2494|0.532106|0.534204|0.532412|0.530986|〇\n",
    "map hoverの特徴量追加|2561|0.532412|0.533874|0.532410|0.531198|\n",
    "minigameクリックの正解時の座標|2506|0.532053|0.534096|0.532521|0.531153|\n",
    "session_idの分解特徴量を追加|2498|0.532178|0.534549|0.532432|0.531023|\n",
    "group跨ぎの特徴量加工|2496|0.532015|0.534258|0.532286|0.531266|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  取り組み  | 特徴量数 | logloss | 採否 |\n",
    "|  ----  | ---- | ---- | ---- |\n",
    "|init|2494|0.532105|\n",
    "|カテゴリの変化回数にfqidを追加|2496|0.532127|\n",
    "|level+fqidの集計特徴量追加|4070|0.532216|\n",
    "|level+room_fqidの集計特徴量追加|3095|0.531910|〇\n",
    "|event_name+levelの集計特徴量追加|3589|0.532076|\n",
    "|level+room_fqidの変化量|3095|0.531910|\n",
    "|room_fqidの系列|3097|0.534676|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo = \"room_fqidの系列\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp085\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "    base_exp = None # 特徴量重要度を使う元のexp\n",
    "    n_features = 500 # 特徴量削減の数\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "    import cudf\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective' : 'binary:logistic',\n",
    "    'tree_method': 'gpu_hist', \n",
    "    'eval_metric':'logloss',\n",
    "    'n_estimators': 100000, \n",
    "    'early_stopping_rounds': 100,\n",
    "    'learning_rate': 0.02,\n",
    "    'seed': cfg.seed,\n",
    "    \"enable_categorical\": True,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 0.633766964312668,\n",
    "    'gamma': 0.1299699438623672,\n",
    "    'colsample_bytree': 0.7992922523509169,\n",
    "    'subsample': 0.7061042367364462,\n",
    "    'alpha': 2.0781568344639023,\n",
    "    'lambda': 4.600879934143353\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    with open(cfg.prep_dir + 'cat_col_lists_v3.pkl', 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) \n",
    "    with open(cfg.prep_dir + 'sequence_encoders.pkl', 'rb') as f:\n",
    "        sequence_encoders = pickle.load(f) \n",
    "    with open(cfg.prep_dir + 'room_fqid_encoder.pkl', 'rb') as f:\n",
    "        room_fqid_encoder = pickle.load(f) \n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    with open(\"/kaggle/input/psp-cat-col-lists/cat_col_lists_v3.pkl\", 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_maxmin(s):\n",
    "    try:\n",
    "        return s.max() - s.min()\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features:\n",
    "    def __init__(self, sessions_df, need_create_features=None):\n",
    "        self.sessions_df = pl.from_pandas(sessions_df).sort([\"session_id\", \"index\"])\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.need_create_features = need_create_features\n",
    "        self.room_fqid_seq_encoder = sequence_encoders[group][\"room_fqid\"]\n",
    "\n",
    "    def prep(self):\n",
    "        self.sessions_df = self.sessions_df.with_columns(\n",
    "            [(pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(1)).clip(0, 1e9).fill_null(0).over([\"session_id\"]).alias(\"time_diff\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"name\")).alias(\"event_name+name\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"room_fqid\")).alias(\"event_name+room_fqid\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"fqid\")).alias(\"event_name+fqid\"),\n",
    "             (pl.col(\"level\").cast(pl.Utf8) + \"_\" + pl.col(\"room_fqid\")).alias(\"level+room_fqid\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"level\").cast(pl.Utf8)).alias(\"event_name+level\"),\n",
    "             (pl.col(\"room_fqid\").map_dict(room_fqid_encoder).alias(\"room_fqid_encode\"))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "    def get_aggs(self):\n",
    "        g = self.group\n",
    "        cats = cat_col_lists[g]\n",
    "        aggs = []\n",
    "\n",
    "        # トータルレコード数\n",
    "        aggs += [pl.col(\"index\").count().alias(f\"{g}_record_cnt\")]\n",
    "\n",
    "        # グループ全体の経過時間\n",
    "        aggs += [pl.col(\"elapsed_time\").apply(lambda s:s.max() - s.min()).alias(f\"{g}_elapsed_time\")]\n",
    "\n",
    "        # 各categoryごとのレコード数\n",
    "        for c in [\"event_name\", \"name\", \"page\", \"level\", \"room_fqid\", \"fqid\", \"event_name+name\", \"event_name+room_fqid\", \"event_name+fqid\", \"level+room_fqid\"]:\n",
    "            aggs += [pl.col(\"index\").filter(pl.col(c)==v).count().alias(f\"{g}_{c}_{str(v)}_record_cnt\") for v in cats[c]]\n",
    "        \n",
    "        # 各categoryごとのユニーク数\n",
    "        for c in [\"event_name\", \"name\", \"page\", \"level\", \"room_fqid\", \"fqid\", \"event_name+name\", \"event_name+room_fqid\", \"event_name+fqid\", \"level+room_fqid\"]:\n",
    "            aggs += [pl.col(c).drop_nulls().n_unique().alias(f\"{g}_{c}_nunique\")]\n",
    "\n",
    "        # 集計量\n",
    "        for v in [\"elapsed_time\", \"index\"]:\n",
    "            aggs += [pl.col(v).max().alias(f\"{g}_{v}_max\").cast(pl.Float32), \n",
    "                     pl.col(v).max().alias(f\"{g}_{v}_min\").cast(pl.Float32)]\n",
    "\n",
    "        for v in [\"time_diff\", \"hover_duration\"]:\n",
    "            aggs += [pl.col(v).max().alias(f\"{g}_{v}_max\").cast(pl.Float32), \n",
    "                     pl.col(v).min().alias(f\"{g}_{v}_min\").cast(pl.Float32), \n",
    "                     pl.col(v).std().alias(f\"{g}_{v}_std\").cast(pl.Float32),\n",
    "                     pl.col(v).mean().alias(f\"{g}_{v}_mean\").cast(pl.Float32), \n",
    "                     pl.col(v).sum().alias(f\"{g}_{v}_sum\").cast(pl.Float32), \n",
    "                     pl.col(v).median().alias(f\"{g}_{v}_median\").cast(pl.Float32)]\n",
    "            \n",
    "            aggs += [pl.col(v).quantile(0.25, \"nearest\").alias(f\"{g}_{v}_quantile025\"),\n",
    "                     pl.col(v).quantile(0.75, \"nearest\").alias(f\"{g}_{v}_quantile075\")\n",
    "            ]\n",
    "            \n",
    "        # カテゴリ×集計量\n",
    "        cs = [\"event_name\", \"room_fqid\", \"fqid\", \"text_fqid\", \"level\", \"name\", \"event_name+name\", \"event_name+room_fqid\", \"level+room_fqid\"]\n",
    "        vs = [\"time_diff\"]\n",
    "        for c, v in itertools.product(cs, vs):\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).max().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_max\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).min().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_min\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).std().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_std\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).mean().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_mean\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).median().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_median\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).sum().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_sum\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).quantile(0.25, \"nearest\").fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_quantile025\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).quantile(0.75, \"nearest\").fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_quantile075\").cast(pl.Float32) for cat in cats[c]]\n",
    "\n",
    "        cs = [\"event_name\", \"room_fqid\", \"fqid\", \"text_fqid\", \"level\", \"name\", \"event_name+name\", \"event_name+room_fqid\", \"level+room_fqid\"]\n",
    "        vs = [\"elapsed_time\"]\n",
    "        for c, v in itertools.product(cs, vs):\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).max().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_max\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).min().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_min\").cast(pl.Float32) for cat in cats[c]]\n",
    "\n",
    "        # カテゴリの変化回数\n",
    "        for c in [\"room_fqid\", \"text_fqid\"]:\n",
    "            aggs += [(pl.col(c) != pl.col(c).shift(1)).sum().alias(f\"{g}_{c}_change_cnt\")]\n",
    "\n",
    "        # object_hoverのduration関連の特徴量（各fqidごと）\n",
    "        fqids = [c.removeprefix(\"object_hover_\") for c in cats[\"event_name+fqid\"] if \"object_hover\" in c]\n",
    "        for fqid in fqids:\n",
    "            aggs += [pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).max().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_max\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).min().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_min\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_std\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_mean\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).median().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_median\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).sum().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_sum\")\n",
    "                    ]\n",
    "\n",
    "        # miniゲーム中のクリック座標\n",
    "        if g == \"0-4\":\n",
    "            fqids = [\"tunic\", \"plaque\"]\n",
    "        elif g == \"5-12\":\n",
    "            fqids = [\"businesscards\", \"logbook\", \"reader\", \"journals\"]\n",
    "        elif g == \"13-22\":\n",
    "            fqids = [\"tracks\", \"reader_flag\", \"journals_flag\", \"directory\"]\n",
    "\n",
    "        for fqid in fqids:\n",
    "            aggs += [\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).first().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_first_click_room_coor_x\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).first().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_first_click_room_coor_y\"),\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).last().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_last_click_room_coor_x\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).last().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_last_click_room_coor_y\"),\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_x_mean\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_y_mean\"),\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_x_std\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_y_std\"),\n",
    "            ]\n",
    "      \n",
    "        aggs += [pl.col(\"room_fqid_encode\").filter(pl.col(\"room_fqid_encode\")!=pl.col(\"room_fqid_encode\").shift(1)).str.concat(\"\").map_dict(self.room_fqid_seq_encoder).alias(f\"{g}_room_fqid_seq\")]\n",
    "\n",
    "\n",
    "        # 生成する特徴量を限定\n",
    "        if self.need_create_features is not None:\n",
    "            feats = [re.findall(r'alias\\(\"(.*)\"\\)', str(a))[0] for a in aggs]\n",
    "            aggs = [aggs[i] for i, f in enumerate(feats) if f in self.need_create_features]\n",
    "\n",
    "        return aggs\n",
    "\n",
    "    def get_features(self):\n",
    "        self.prep()\n",
    "        aggs = self.get_aggs()\n",
    "        features = self.sessions_df.groupby([\"session_id\"], maintain_order=True).agg(aggs)\n",
    "        features = features.to_pandas()\n",
    "        features[f\"{self.group}_room_fqid_seq\"] = features[f\"{self.group}_room_fqid_seq\"].astype(\"category\")\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    features = Features(sessions).get_features()\n",
    "    train = labels.merge(features, on=[\"session_id\"], how=\"left\")\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels, feature_select=False, need_create_features=None):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    features = Features(sessions, need_create_features).get_features()\n",
    "    test = labels.merge(features, on=[\"session_id\"], how=\"left\")\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesSelect:\n",
    "    def __init__(self, df, init_features, corr_th=0.99):\n",
    "        self.init_features = init_features\n",
    "        self.df = cudf.from_pandas(df)\n",
    "        self.corr_th = corr_th\n",
    "        self.drop_cols = []\n",
    "    \n",
    "    def _high_corr_features_drop(self):\n",
    "        num_cols = self.df[self.init_features].select_dtypes(include=\"number\").columns\n",
    "\n",
    "        # 特徴量間の相関行列を計算\n",
    "        corr_matrix = self.df[num_cols].fillna(-1).corr().abs().to_pandas()\n",
    "        # 相関行列の上三角行列を取得します。（相関行列が対称であるため、重複する相関を取り除くため）\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        drop_cols = []\n",
    "        for c in num_cols:\n",
    "            if any(upper[c] > self.corr_th):\n",
    "                drop_cols.append(c)\n",
    "                upper = upper.drop(index=c)\n",
    "        print(f\"特徴量間の相関性が高い特徴量を{str(len(drop_cols))}個抽出\")\n",
    "        self.df = self.df.drop(columns=drop_cols)\n",
    "        self.drop_cols = list(set(self.drop_cols + drop_cols))\n",
    "\n",
    "    def features_select(self):\n",
    "        self._high_corr_features_drop()\n",
    "        selected_features = list(set(self.init_features) - set(self.drop_cols))\n",
    "        print(f\"{str(len(self.init_features))} -> {str(len(selected_features))}\")\n",
    "\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}_cleaned.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "        \n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "        # 特徴量選択\n",
    "        if cfg.base_exp is None:\n",
    "            features = FeaturesSelect(train, features).features_select()\n",
    "        else:\n",
    "            # 使用する特徴量の抽出\n",
    "            features = pd.read_csv(cfg.output_dir + f\"{cfg.base_exp}/fi_{group}.csv\").head(cfg.n_features)[\"feature\"].tolist()\n",
    "\n",
    "        gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "        fis = []\n",
    "        \n",
    "        oof_groups = []\n",
    "        for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "            model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.json\"\n",
    "            \n",
    "            print(f\"fold : {i}\")\n",
    "            tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "            vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "\n",
    "\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(tr_x, tr_y, eval_set=[(vl_x, vl_y)], verbose=500)\n",
    "            # モデル出力\n",
    "            model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.json\")\n",
    "        \n",
    "            # valid_pred\n",
    "            oof_fold = train.iloc[vl_idx].copy()\n",
    "            oof_fold[\"pred\"] = model.predict_proba(vl_x)[:,1]\n",
    "            oof_groups.append(oof_fold)\n",
    "\n",
    "            # 特徴量重要度\n",
    "            fi_fold = pd.DataFrame()\n",
    "            fi_fold[\"feature\"] = model.feature_names_in_\n",
    "            fi_fold[\"importance\"] = model.feature_importances_\n",
    "            fi_fold[\"fold\"] = i\n",
    "            fis.append(fi_fold)\n",
    "\n",
    "        fi = pd.concat(fis)    \n",
    "        fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "        fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "        fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi_{group}.csv\", index=False)\n",
    "\n",
    "        oof_group = pd.concat(oof_groups)\n",
    "        oofs.append(oof_group)\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\", \"level_group\"])\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = oof_group.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{group}_pred_mean\", \"max\":f\"{group}_pred_max\", \"min\":f\"{group}_pred_min\", \"std\":f\"{group}_pred_std\"})\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"level_group2\"] = test[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"level_group2\"] = sub[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in test.groupby(\"level_group2\")]\n",
    "    subs = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in sub.groupby(\"level_group2\")]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    model_dict = {}\n",
    "    features_dict = {}\n",
    "    for g in level_group_list:\n",
    "        if mode == \"local_cv\":\n",
    "            model_paths = [cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.json\" for i in range(cfg.n_splits)]\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_paths = [f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.json\" for i in range(cfg.n_splits)]\n",
    "        models = []\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = XGBClassifier()\n",
    "            model.load_model(model_paths[i])\n",
    "            # 推論はCPU\n",
    "            model.get_booster().set_param({'tree_method': 'hist'})\n",
    "            models.append(model)\n",
    "        model_dict[g] = models\n",
    "        features_dict[g] = list(model.feature_names_in_)\n",
    "    need_create_features = features_dict[\"0-4\"] + features_dict[\"5-12\"] + features_dict[\"13-22\"]\n",
    "    not_drop_cols = [\"0-4_elapsed_time_max\", \"0-4_index_max\", \"5-12_elapsed_time_max\", \"5-12_index_max\", \"13-22_elapsed_time_max\", \"13-22_index_max\",\n",
    "                     \"0-4_elapsed_time_min\", \"0-4_index_min\", \"5-12_elapsed_time_min\", \"5-12_index_min\", \"13-22_elapsed_time_min\", \"13-22_index_min\"]\n",
    "    need_create_features = need_create_features + not_drop_cols\n",
    "    need_create_features = list(set(need_create_features))\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission, feature_select=True, need_create_features=need_create_features)\n",
    "        features = features_dict[level_group]\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "\n",
    "        prev_features_df = test.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\"])\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = model_dict[level_group][i]\n",
    "            preds += model.predict_proba(test[features])[:,1] / cfg.n_splits\n",
    "        test[\"pred\"] = preds\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = test.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{level_group}_pred_mean\", \"max\":f\"{level_group}_pred_max\", \"min\":f\"{level_group}_pred_min\", \"std\":f\"{level_group}_pred_std\"})\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_train_test_process_identity():\n",
    "    iter_train = get_mock_iter_train()\n",
    "    iter_test = get_mock_iter_test()\n",
    "\n",
    "    print(\"train_iter\")\n",
    "    train_df_dict = {}\n",
    "    train_features_dict = {}\n",
    "    prev_features_df = None\n",
    "    for (sessions, sub) in iter_train:\n",
    "        group = sessions[\"level_group\"].values[0]\n",
    "        print(group)\n",
    "        train = get_train_dataset(sessions, sub)\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "            # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "        train_df_dict[group] = train[[\"session_id\"]+features].sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "        prev_features_df = train[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        train_features_dict[group] = features\n",
    "\n",
    "\n",
    "    print(\"test_iter\")\n",
    "    test_dfs_0_4 = []\n",
    "    test_dfs_5_12 = []\n",
    "    test_dfs_13_22 = []\n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        session_id = test_sessions[\"session_id\"].values[0]\n",
    "        print(session_id, level_group)\n",
    "        features = train_features_dict[level_group]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in test.columns if c not in not_use_cols]\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        if level_group == \"0-4\":\n",
    "            test_dfs_0_4.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"5-12\":\n",
    "            test_dfs_5_12.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"13-22\":\n",
    "            test_dfs_13_22.append(test[[\"session_id\"]+features])\n",
    "        \n",
    "\n",
    "    test_dfs_0_4 = pd.concat(test_dfs_0_4, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_5_12 = pd.concat(test_dfs_5_12, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_13_22 = pd.concat(test_dfs_13_22, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "\n",
    "    assert train_df_dict[\"0-4\"][train_features_dict[\"0-4\"]].equals(test_dfs_0_4[train_features_dict[\"0-4\"]])\n",
    "    assert train_df_dict[\"5-12\"][train_features_dict[\"5-12\"]].equals(test_dfs_5_12[train_features_dict[\"5-12\"]])\n",
    "    assert train_df_dict[\"13-22\"][train_features_dict[\"13-22\"]].equals(test_dfs_13_22[train_features_dict[\"13-22\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量間の相関性が高い特徴量を1145個抽出\n",
      "2089 -> 944\n",
      "5-12\n",
      "特徴量間の相関性が高い特徴量を2894個抽出\n",
      "5991 -> 3097\n",
      "fold : 0\n",
      "[0]\tvalidation_0-logloss:0.68809\n",
      "[500]\tvalidation_0-logloss:0.53543\n",
      "[1000]\tvalidation_0-logloss:0.53486\n",
      "[1046]\tvalidation_0-logloss:0.53486\n",
      "fold0 score 0.534654\n",
      "fold : 1\n",
      "[0]\tvalidation_0-logloss:0.68815\n",
      "[500]\tvalidation_0-logloss:0.53630\n",
      "[863]\tvalidation_0-logloss:0.53611\n",
      "fold1 score 0.535984\n",
      "fold : 2\n",
      "[0]\tvalidation_0-logloss:0.68809\n",
      "[500]\tvalidation_0-logloss:0.53451\n",
      "[665]\tvalidation_0-logloss:0.53444\n",
      "fold2 score 0.534396\n",
      "fold : 3\n",
      "[0]\tvalidation_0-logloss:0.68812\n",
      "[500]\tvalidation_0-logloss:0.53373\n",
      "[854]\tvalidation_0-logloss:0.53343\n",
      "fold3 score 0.533312\n",
      "fold : 4\n",
      "[0]\tvalidation_0-logloss:0.68812\n",
      "[500]\tvalidation_0-logloss:0.53556\n",
      "[942]\tvalidation_0-logloss:0.53519\n",
      "fold4 score 0.535033\n",
      "total score 0.534676\n"
     ]
    }
   ],
   "source": [
    "oofs = []\n",
    "prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "for group in level_group_list:\n",
    "    print(group)\n",
    "    # データ読み込み\n",
    "    train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}_cleaned.csv\")\n",
    "    labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "    train = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "    # 一つ前のlevel_groupの特徴量を追加\n",
    "    if prev_features_df is not None:\n",
    "        train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "    if group == \"5-12\":\n",
    "        train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "        train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "    elif group == \"13-22\":\n",
    "        train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "        train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "\n",
    "    target = \"correct\"\n",
    "    not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "    features = [c for c in train.columns if c not in not_use_cols]\n",
    "    # 特徴量選択\n",
    "    if cfg.base_exp is None:\n",
    "        features = FeaturesSelect(train, features).features_select()\n",
    "    else:\n",
    "        # 使用する特徴量の抽出\n",
    "        features = pd.read_csv(cfg.output_dir + f\"{cfg.base_exp}/fi_{group}.csv\").head(cfg.n_features)[\"feature\"].tolist()\n",
    "\n",
    "    # 次のlevel_groupで使う用に特徴量を保持\n",
    "    prev_features_df = train.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\", \"level_group\"])\n",
    "\n",
    "    if group == \"5-12\":\n",
    "        break\n",
    "\n",
    "gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "fis = []\n",
    "total_score = 0.0\n",
    "for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "    model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.json\"\n",
    "    \n",
    "    print(f\"fold : {i}\")\n",
    "    tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "    vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(tr_x, tr_y, eval_set=[(vl_x, vl_y)], verbose=500)\n",
    "    print(f\"fold{i} score\", format(model.best_score, \".6f\"))\n",
    "    total_score += model.best_score / cfg.n_splits\n",
    "\n",
    "    # 特徴量重要度\n",
    "    fi_fold = pd.DataFrame()\n",
    "    fi_fold[\"feature\"] = model.feature_names_in_\n",
    "    fi_fold[\"importance\"] = model.feature_importances_\n",
    "    fi_fold[\"fold\"] = i\n",
    "    fis.append(fi_fold)\n",
    "\n",
    "fi = pd.concat(fis)    \n",
    "fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi_{group}.csv\", index=False)\n",
    "\n",
    "print(f\"total score\", format(total_score, \".6f\"))\n",
    "org_score = format(total_score, \".6f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_imp_params = {\n",
    "    'objective' : 'binary:logistic',\n",
    "    'tree_method': 'gpu_hist', \n",
    "    'eval_metric':'logloss',\n",
    "    'n_estimators': 100, \n",
    "    'learning_rate': 0.1,\n",
    "    'seed': cfg.seed,\n",
    "    \"enable_categorical\": True,\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 0.633766964312668,\n",
    "    'gamma': 0.1299699438623672,\n",
    "    'colsample_bytree': 0.7992922523509169,\n",
    "    'subsample': 0.7061042367364462,\n",
    "    'alpha': 2.0781568344639023,\n",
    "    'lambda': 4.600879934143353\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features = fi[\"feature\"].tolist()\n",
    "model = XGBClassifier(**null_imp_params)\n",
    "model.fit(train[features], train[target].values, verbose=100)\n",
    "fi_org = pd.DataFrame()\n",
    "fi_org[\"feature\"] = model.feature_names_in_\n",
    "fi_org[\"importance\"] = model.feature_importances_\n",
    "fi_org = fi_org.sort_values(\"feature\", ignore_index=True)\n",
    "\n",
    "null_imps = []\n",
    "for i in range(10):\n",
    "    model = XGBClassifier(**null_imp_params)\n",
    "    model.fit(train[features], train[target].sample(frac=1).values, verbose=100)\n",
    "    fi_tmp = pd.DataFrame()\n",
    "    fi_tmp[\"feature\"] = model.feature_names_in_\n",
    "    fi_tmp[\"importance\"] = model.feature_importances_\n",
    "    null_imps.append(fi_tmp)\n",
    "null_imp = pd.concat(null_imps)\n",
    "null_imp_mean = null_imp.groupby(\"feature\")[\"importance\"].mean().reset_index().sort_values(\"feature\", ignore_index=True)\n",
    "fi_org[\"null_imp\"] = null_imp_mean[\"importance\"]\n",
    "selected_features = fi_org[fi_org[\"importance\"]>fi_org[\"null_imp\"]][\"feature\"].tolist()\n",
    "\n",
    "null_imp_mean = null_imp.groupby(\"feature\")[\"importance\"].quantile(0.8).reset_index().sort_values(\"feature\", ignore_index=True)\n",
    "fi_org[\"null_imp\"] = null_imp_mean[\"importance\"]\n",
    "selected_features = fi_org[fi_org[\"importance\"]>fi_org[\"null_imp\"]][\"feature\"].tolist()\n",
    "\n",
    "null_imp_score = 0.0\n",
    "for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "    tr_x, tr_y = train.iloc[tr_idx][selected_features], train.iloc[tr_idx][target]\n",
    "    vl_x, vl_y = train.iloc[vl_idx][selected_features], train.iloc[vl_idx][target]\n",
    "\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(tr_x, tr_y, eval_set=[(vl_x, vl_y)], verbose=0)\n",
    "    print(f\"fold{i} score\", format(model.best_score, \".6f\"))\n",
    "    null_imp_score += model.best_score / cfg.n_splits\n",
    "print(\"null_imp\", format(total_score, \".6f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|room_fqidの系列|3097|0.534676|\n"
     ]
    }
   ],
   "source": [
    "print(f\"|{memo}|{str(len(fi))}|{org_score}|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if cfg.mode == \"local_cv\":\n",
    "#    valid_train_test_process_identity()\n",
    "#    run_train()\n",
    "#inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
