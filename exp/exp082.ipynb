{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp082"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantile追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cudf\n",
    "import polars as pl\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp082\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "    base_exp = None # 特徴量重要度を使う元のexp\n",
    "    n_features = 500 # 特徴量削減の数\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.01, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    with open(cfg.prep_dir + 'cat_col_lists_v2.pkl', 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) \n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    with open(\"/kaggle/input/psp-cat-col-lists/cat_col_lists_v2.pkl\", 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile0_1(s):\n",
    "    return s.quantile(0.1)\n",
    "def quantile0_2(s):\n",
    "    return s.quantile(0.2)\n",
    "def quantile0_3(s):\n",
    "    return s.quantile(0.3)\n",
    "def quantile0_4(s):\n",
    "    return s.quantile(0.4)\n",
    "def quantile0_6(s):\n",
    "    return s.quantile(0.6)\n",
    "def quantile0_7(s):\n",
    "    return s.quantile(0.7)\n",
    "def quantile0_8(s):\n",
    "    return s.quantile(0.8)\n",
    "def quantile0_9(s):\n",
    "    return s.quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTrain:\n",
    "    def __init__(self, sessions_df, labels):\n",
    "        self.sessions_df = sessions_df.sort_values([\"session_id\", \"level_group\", \"index\"], ignore_index=True)\n",
    "        self.features = self.sessions_df[[\"session_id\", \"level_group\"]].drop_duplicates().copy()\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].shift(1)\n",
    "        self.sessions_df[\"time_diff\"] = np.where(self.sessions_df[\"time_diff\"]<0, 0, self.sessions_df[\"time_diff\"])\n",
    "        self.sessions_df[\"time_diff\"] = np.nan_to_num(self.sessions_df[\"time_diff\"], 0)\n",
    "        self.sessions_df[\"event_name+name\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"name\"]\n",
    "        self.sessions_df[\"event_name+room_fqid\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"room_fqid\"]\n",
    "        self.sessions_df[\"event_name+fqid\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"fqid\"]\n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_record_cnt\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[\"elapsed_time\"].agg([max,min]).reset_index()\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[\"max\"] - add_features[\"min\"]\n",
    "        add_features[f\"{self.group}_group_elapsed_time\"] = add_features[f\"{self.group}_group_elapsed_time\"].astype(np.float32)\n",
    "        add_features = add_features[[\"session_id\", \"level_group\", f\"{self.group}_group_elapsed_time\"]].copy()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[\"index\"].count().reset_index().rename(columns={\"index\":\"cnt\"})\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            tmp = add_features[add_features[cat_col]==cat][[\"session_id\", \"level_group\", \"cnt\"]].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp = tmp.rename(columns={\"cnt\": feat_name})\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[feat_name] = self.features[feat_name].fillna(0).astype(int)\n",
    "            else:\n",
    "                self.features[feat_name] = int(0)\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        add_features = self.sessions_df.dropna(subset=[cat_col]).drop_duplicates([\"session_id\", \"level_group\", cat_col])\n",
    "        add_features = add_features.groupby([\"session_id\", \"level_group\"])[\"index\"].count().reset_index().rename(columns={\"index\":f\"{self.group}_{cat_col}_nunique\"})\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")        \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        new_cols = [f\"{self.group}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{self.group}_{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, aggs)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "    def _agg_features_quantile(self, val_cols):\n",
    "        quantiles_str = [\"quantile0_1\", \"quantile0_2\", \"quantile0_3\", \"quantile0_4\", \"quantile0_6\", \"quantile0_7\", \"quantile0_8\", \"quantile0_9\"]\n",
    "        aggs = [quantile0_1, quantile0_2, quantile0_3, quantile0_4, quantile0_6, quantile0_7, quantile0_8, quantile0_9]\n",
    "        new_cols = [f\"{self.group}_{v}_{a}\" for v,a in itertools.product(val_cols, quantiles_str)]\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\"])[val_cols].agg(aggs).reset_index()\n",
    "        add_features.columns = [\"session_id\", \"level_group\"] + new_cols\n",
    "        add_features[new_cols] = add_features[new_cols].astype(np.float32)\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "    def _cat_agg_features_quantile(self, val_cols, cat_col, not_use_cats=None):\n",
    "        quantiles_str = [\"quantile0_1\", \"quantile0_2\", \"quantile0_3\", \"quantile0_4\", \"quantile0_6\", \"quantile0_7\", \"quantile0_8\", \"quantile0_9\"]\n",
    "        aggs = [quantile0_1, quantile0_2, quantile0_3, quantile0_4, quantile0_6, quantile0_7, quantile0_8, quantile0_9]\n",
    "\n",
    "        add_features = self.sessions_df.groupby([\"session_id\", \"level_group\", cat_col])[val_cols].agg(aggs).reset_index()\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            new_cols = [f\"{self.group}_{cat_col}_{cat}_{v}_{a}\" for v,a in itertools.product(val_cols, quantiles_str)]\n",
    "            tmp = add_features[add_features[cat_col]==cat].copy()\n",
    "            if len(tmp) > 0:\n",
    "                tmp.columns = [\"session_id\", \"level_group\", cat_col] + new_cols\n",
    "                tmp = tmp.drop(columns=[cat_col])\n",
    "                self.features = self.features.merge(tmp, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "                self.features[new_cols] = self.features[new_cols].fillna(-1)\n",
    "            else:\n",
    "                self.features[new_cols] = -1\n",
    "            self.features[new_cols] = self.features[new_cols].astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "    def _cat_change_cnt(self, cat_col):\n",
    "        \"\"\"cat_colの変化回数\n",
    "        \"\"\"\n",
    "        tmp = self.sessions_df[[\"session_id\", \"level_group\", cat_col]].copy()\n",
    "        tmp[cat_col] = tmp[cat_col].fillna(\"nan\")\n",
    "        tmp[f\"{self.group}_{cat_col}_change_cnt\"] = (tmp[cat_col] != tmp.groupby([\"session_id\", \"level_group\"])[cat_col].shift(1)).astype(int)\n",
    "        add_features = tmp.groupby([\"session_id\", \"level_group\"])[f\"{self.group}_{cat_col}_change_cnt\"].sum().reset_index()\n",
    "        self.features = self.features.merge(add_features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "\n",
    "\n",
    "    def _add_minigame_features(self, start_fqid, end_fqid):\n",
    "        game_name = start_fqid\n",
    "        dfs = []\n",
    "        for session_id in tqdm(self.sessions_df[\"session_id\"].unique()):\n",
    "            tmp = self.sessions_df[self.sessions_df[\"session_id\"]==session_id].copy()\n",
    "            start_indexes = tmp[(tmp[\"event_name\"]==\"navigate_click\")&(tmp[\"fqid\"]==start_fqid)][\"index\"].values\n",
    "            end_indexes = tmp[(tmp[\"event_name\"]==\"object_click\")&(tmp[\"fqid\"]==end_fqid)][\"index\"].values\n",
    "            if len(start_indexes) > 0:\n",
    "                start_index = start_indexes[0]\n",
    "            else:\n",
    "                start_index = np.nan\n",
    "            if len(end_indexes) > 0:\n",
    "                end_index = end_indexes[0]\n",
    "            else:\n",
    "                end_index = np.nan\n",
    "\n",
    "            if start_index < end_index:\n",
    "                mini_game_sessions = tmp[(tmp[\"index\"]>start_index)&(tmp[\"index\"]<=end_index)].copy()\n",
    "                record_cnt = len(mini_game_sessions)\n",
    "                total_duration = mini_game_sessions[\"time_diff\"].sum()\n",
    "                total_hover_duration = mini_game_sessions[\"hover_duration\"].sum()\n",
    "\n",
    "                hover_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_hover\"].copy()\n",
    "                if len(hover_sessions) > 0:\n",
    "                    hover_cnt = len(hover_sessions)\n",
    "                else:\n",
    "                    hover_cnt = 0\n",
    "\n",
    "                click_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_click\"].copy()\n",
    "                if len(click_sessions) > 0:\n",
    "                    click_cnt = len(click_sessions)\n",
    "                else:\n",
    "                    click_cnt = 0\n",
    "\n",
    "                feature_tmp = pd.DataFrame([[session_id, record_cnt, total_duration, total_hover_duration, hover_cnt, click_cnt]],\n",
    "                                            columns=[\"session_id\", f\"{self.group}_minigame_{game_name}_record_cnt\", f\"{self.group}_minigame_{game_name}_total_duration\", f\"{self.group}_minigame_{game_name}_total_hover_duration\",\n",
    "                                                    f\"{self.group}_minigame_{game_name}_hover_cnt\", f\"{self.group}_minigame_{game_name}_click_cnt\"]\n",
    "                                        )\n",
    "            else:\n",
    "                feature_tmp = pd.DataFrame([[session_id, 0, 0, 0, 0, 0]],\n",
    "                                            columns=[\"session_id\", f\"{self.group}_minigame_{game_name}_record_cnt\", f\"{self.group}_minigame_{game_name}_total_duration\", f\"{self.group}_minigame_{game_name}_total_hover_duration\",\n",
    "                                                    f\"{self.group}_minigame_{game_name}_hover_cnt\", f\"{self.group}_minigame_{game_name}_click_cnt\"]\n",
    "                                        )\n",
    "            dfs.append(feature_tmp)\n",
    "        add_features = pd.concat(dfs, ignore_index=True)\n",
    "        self.features = self.features.merge(add_features, on=\"session_id\", how=\"left\")\n",
    "\n",
    "\n",
    "    def get_train(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_record_cnt(\"event_name+name\")\n",
    "        self._cat_record_cnt(\"event_name+room_fqid\")\n",
    "        self._cat_record_cnt(\"event_name+fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "        self._cat_col_nunique(\"event_name+name\")\n",
    "        self._cat_col_nunique(\"event_name+room_fqid\")\n",
    "        self._cat_col_nunique(\"event_name+fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"])\n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        self._cat_agg_features(val_cols=[\"hover_duration\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['cutscene_click', 'person_click', 'navigate_click',\n",
    "                                             'observation_click', 'notification_click', 'object_click',\n",
    "                                             'map_click', 'checkpoint', 'notebook_click'])\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+room_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"hover_duration\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+fqid\")    \n",
    "\n",
    "\n",
    "        self._agg_features_quantile(val_cols=[\"time_diff\", \"hover_duration\"])\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"event_name\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"room_fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"level\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"hover_duration\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['cutscene_click', 'person_click', 'navigate_click',\n",
    "                                             'observation_click', 'notification_click', 'object_click',\n",
    "                                             'map_click', 'checkpoint', 'notebook_click'])\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"event_name+name\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"event_name+room_fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"event_name+fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"hover_duration\"], cat_col=\"event_name+fqid\")    \n",
    "\n",
    "        self._cat_change_cnt(\"text_fqid\")\n",
    "        self._cat_change_cnt(\"room_fqid\")\n",
    "\n",
    "        if self.group == \"0-4\":\n",
    "            self._add_minigame_features(\"tunic\", \"tunic.hub.slip\")\n",
    "            self._add_minigame_features(\"plaque\", \"plaque.face.date\")\n",
    "        \n",
    "        elif self.group == \"5-12\":\n",
    "            self._add_minigame_features(\"businesscards\", \"businesscards.card_bingo.bingo\")\n",
    "            self._add_minigame_features(\"logbook\", \"logbook.page.bingo\")\n",
    "            self._add_minigame_features(\"reader\", \"reader.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals\", \"journals.pic_2.bingo\")\n",
    "        \n",
    "        elif self.group == \"13-22\":\n",
    "            self._add_minigame_features(\"tracks\", \"tracks.hub.deer\")\n",
    "            self._add_minigame_features(\"reader_flag\", \"reader_flag.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals_flag\", \"journals_flag.pic_0.bingo\")\n",
    "        \n",
    "        self.result = self.result.merge(self.features, on=[\"session_id\", \"level_group\"], how=\"left\")\n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesInf:\n",
    "    def __init__(self, sessions_df, labels, feature_select=False, need_create_features=[]):\n",
    "        self.sessions_df = sessions_df.sort_values([\"index\"], ignore_index=True)\n",
    "        self.result = labels\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.use_cols = [\n",
    "            \"elapsed_time\", \"event_name\", \"name\", \"level\", \"page\", \"index\",\n",
    "            \"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\",\n",
    "            \"hover_duration\", \"text\", \"fqid\", \"room_fqid\", \"text_fqid\", \"event_name+name\", \"event_name+room_fqid\", \"time_diff\", \"event_name+fqid\"\n",
    "        ]\n",
    "        self.feature_select = feature_select\n",
    "        self.need_create_features = need_create_features\n",
    "\n",
    "\n",
    "    def _prep(self):\n",
    "        self.sessions_df[\"event_name+name\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"name\"]\n",
    "        self.sessions_df[\"event_name+room_fqid\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"room_fqid\"]\n",
    "        self.sessions_df[\"event_name+fqid\"] = self.sessions_df[\"event_name\"] + \"_\" + self.sessions_df[\"fqid\"]\n",
    "        self.sessions_df[\"time_diff\"] = self.sessions_df[\"elapsed_time\"] - self.sessions_df[\"elapsed_time\"].shift(1).values\n",
    "        self.sessions_df[\"time_diff\"] = np.where(self.sessions_df[\"time_diff\"]<0, 0, self.sessions_df[\"time_diff\"])\n",
    "        self.sessions_df[\"time_diff\"] = np.nan_to_num(self.sessions_df[\"time_diff\"], 0)\n",
    "        # dataframeの各列をnumpy arrayで保持\n",
    "        self.sessions = {}\n",
    "        for c in self.use_cols:\n",
    "            self.sessions[c] = self.sessions_df[c].values\n",
    "        \n",
    "\n",
    "    def _total_record_cnt(self):\n",
    "        \"\"\"level_groupごとのレコード数\n",
    "        \"\"\"\n",
    "        feat_name = f\"{self.group}_record_cnt\"\n",
    "        if self.feature_select & (feat_name not in self.need_create_features):\n",
    "            pass\n",
    "        else:\n",
    "            add_feature = len(self.sessions[\"elapsed_time\"])\n",
    "            self.result[feat_name] = add_feature\n",
    "\n",
    "    def _group_elapsed_time(self):\n",
    "        \"\"\"level_groupごと、epapsed_timeのmax - min（経過時間）\n",
    "        \"\"\"\n",
    "        feat_name = f\"{self.group}_group_elapsed_time\"\n",
    "        if self.feature_select & (feat_name not in self.need_create_features):\n",
    "            pass\n",
    "        else:\n",
    "            add_feature = np.max(self.sessions[\"elapsed_time\"]) - np.min(self.sessions[\"elapsed_time\"])\n",
    "            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_record_cnt(self, cat_col):\n",
    "        \"\"\"level_groupごと、各{cat}のレコード数\n",
    "        \"\"\"\n",
    "        cat_list = cat_col_lists[self.group][cat_col]\n",
    "        for cat in cat_list:\n",
    "            feat_name = f\"{self.group}_{cat_col}_{str(cat)}_record_cnt\"\n",
    "            if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                pass\n",
    "            else:\n",
    "                add_feature = (self.sessions[cat_col] == cat).astype(int).sum()\n",
    "                self.result[feat_name] = add_feature\n",
    "\n",
    "    def _cat_col_nunique(self, cat_col):\n",
    "        \"\"\"level_groupごと、[col]のユニーク数\n",
    "        \"\"\"\n",
    "        feat_name = f\"{self.group}_{cat_col}_nunique\"\n",
    "        if self.feature_select & (feat_name not in self.need_create_features):\n",
    "            pass\n",
    "        else:\n",
    "            self.result[feat_name] = self.sessions_df[cat_col].dropna().nunique()     \n",
    "\n",
    "    def _agg_features(self, val_cols, aggs):\n",
    "        for val_col, agg in itertools.product(val_cols, aggs):\n",
    "            feat_name = f\"{self.group}_{val_col}_{agg}\"\n",
    "            if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                pass\n",
    "            else:\n",
    "                if agg == \"mean\":\n",
    "                    add_feature = np.nanmean(self.sessions[val_col])\n",
    "                elif agg == \"max\":\n",
    "                    add_feature = np.nanmax(self.sessions[val_col])\n",
    "                elif agg == \"min\":\n",
    "                    add_feature = np.nanmin(self.sessions[val_col])\n",
    "                elif agg == \"std\":\n",
    "                    add_feature = np.nanstd(self.sessions[val_col], ddof=1)\n",
    "                elif agg == \"sum\":\n",
    "                    add_feature = np.nansum(self.sessions[val_col])\n",
    "                elif agg == \"median\":\n",
    "                    add_feature = np.nanmedian(self.sessions[val_col])\n",
    "                self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features(self, val_cols, aggs, cat_col, not_use_cats=None):\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, aggs):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp = self.sessions[val_col][idx]\n",
    "                        if agg == \"mean\":\n",
    "                            add_feature = np.nanmean(tmp)\n",
    "                        elif agg == \"max\":\n",
    "                            add_feature = np.nanmax(tmp)\n",
    "                        elif agg == \"min\":\n",
    "                            add_feature = np.nanmin(tmp)\n",
    "                        elif agg == \"std\":\n",
    "                            add_feature = np.nanstd(tmp, ddof=1)\n",
    "                        elif agg == \"sum\":\n",
    "                            add_feature = np.nansum(tmp)\n",
    "                        elif agg == \"median\":\n",
    "                            add_feature = np.nanmedian(tmp)\n",
    "                        if np.isnan(add_feature):\n",
    "                            self.result[feat_name] = np.float32(-1)\n",
    "                        else:\n",
    "                            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _agg_features_quantile(self, val_cols):\n",
    "        quantiles_str = [\"quantile0_1\", \"quantile0_2\", \"quantile0_3\", \"quantile0_4\", \"quantile0_6\", \"quantile0_7\", \"quantile0_8\", \"quantile0_9\"]\n",
    "        quantile_dict = {\"quantile0_1\":0.1, \"quantile0_2\":0.2, \"quantile0_3\":0.3, \"quantile0_4\":0.4, \"quantile0_6\":0.6, \"quantile0_7\":0.7, \"quantile0_8\":0.8, \"quantile0_9\":0.9}\n",
    "\n",
    "        for val_col, agg in itertools.product(val_cols, quantiles_str):\n",
    "            feat_name = f\"{self.group}_{val_col}_{agg}\"\n",
    "            if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                pass\n",
    "            else:\n",
    "                add_feature = np.nanquantile(self.sessions[val_col], q=quantile_dict[agg])\n",
    "                self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "    def _cat_agg_features_quantile(self, val_cols, cat_col, not_use_cats=None):\n",
    "        quantiles_str = [\"quantile0_1\", \"quantile0_2\", \"quantile0_3\", \"quantile0_4\", \"quantile0_6\", \"quantile0_7\", \"quantile0_8\", \"quantile0_9\"]\n",
    "        quantile_dict = {\"quantile0_1\":0.1, \"quantile0_2\":0.2, \"quantile0_3\":0.3, \"quantile0_4\":0.4, \"quantile0_6\":0.6, \"quantile0_7\":0.7, \"quantile0_8\":0.8, \"quantile0_9\":0.9}\n",
    "\n",
    "        if not_use_cats is not None:\n",
    "            cat_list = [c for c in cat_col_lists[self.group][cat_col] if c not in not_use_cats]\n",
    "        else:\n",
    "            cat_list = cat_col_lists[self.group][cat_col]\n",
    "\n",
    "        for cat in cat_list:\n",
    "            idx = self.sessions[cat_col] == cat\n",
    "        \n",
    "            if idx.sum() == 0:\n",
    "                for val_col, agg in itertools.product(val_cols, quantiles_str):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.result[feat_name] = np.float32(-1)\n",
    "            else:\n",
    "                for val_col, agg in itertools.product(val_cols, quantiles_str):\n",
    "                    feat_name = f\"{self.group}_{cat_col}_{cat}_{val_col}_{agg}\"\n",
    "                    if self.feature_select & (feat_name not in self.need_create_features):\n",
    "                        pass\n",
    "                    else:\n",
    "                        tmp = self.sessions[val_col][idx]\n",
    "                        add_feature = np.nanquantile(tmp, q=quantile_dict[agg])\n",
    "                        if np.isnan(add_feature):\n",
    "                            self.result[feat_name] = np.float32(-1)\n",
    "                        else:\n",
    "                            self.result[feat_name] = np.float32(add_feature)\n",
    "\n",
    "\n",
    "    def _cat_change_cnt(self, cat_col):\n",
    "        \"\"\"cat_colの変化回数\n",
    "        \"\"\"\n",
    "        feat_name = f\"{self.group}_{cat_col}_change_cnt\"\n",
    "        if self.feature_select & (feat_name not in self.need_create_features):\n",
    "            pass\n",
    "        else:\n",
    "            tmp = self.sessions_df[cat_col].copy()\n",
    "            tmp = tmp.fillna(\"nan\")\n",
    "            self.result[feat_name] = (tmp != tmp.shift(1)).sum()\n",
    "\n",
    "\n",
    "    def _add_minigame_features(self, start_fqid, end_fqid):\n",
    "        game_name = start_fqid\n",
    "        start_indexes = self.sessions_df[(self.sessions_df[\"event_name\"]==\"navigate_click\")&(self.sessions_df[\"fqid\"]==start_fqid)][\"index\"].values\n",
    "        end_indexes = self.sessions_df[(self.sessions_df[\"event_name\"]==\"object_click\")&(self.sessions_df[\"fqid\"]==end_fqid)][\"index\"].values\n",
    "        if len(start_indexes) > 0:\n",
    "            start_index = start_indexes[0]\n",
    "        else:\n",
    "            start_index = np.nan\n",
    "        if len(end_indexes) > 0:\n",
    "            end_index = end_indexes[0]\n",
    "        else:\n",
    "            end_index = np.nan\n",
    "\n",
    "        if start_index < end_index:\n",
    "            mini_game_sessions = self.sessions_df[(self.sessions_df[\"index\"]>start_index)&(self.sessions_df[\"index\"]<=end_index)].copy()\n",
    "            record_cnt = len(mini_game_sessions)\n",
    "            total_duration = mini_game_sessions[\"time_diff\"].sum()\n",
    "            total_hover_duration = mini_game_sessions[\"hover_duration\"].sum()\n",
    "\n",
    "            hover_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_hover\"].copy()\n",
    "            if len(hover_sessions) > 0:\n",
    "                hover_cnt = len(hover_sessions)\n",
    "            else:\n",
    "                hover_cnt = 0\n",
    "\n",
    "            click_sessions = mini_game_sessions[mini_game_sessions[\"event_name\"]==\"object_click\"].copy()\n",
    "            if len(click_sessions) > 0:\n",
    "                click_cnt = len(click_sessions)\n",
    "            else:\n",
    "                click_cnt = 0\n",
    "                                    \n",
    "        else:\n",
    "            record_cnt = 0\n",
    "            total_duration = 0\n",
    "            total_hover_duration = 0\n",
    "            hover_cnt = 0\n",
    "            click_cnt = 0\n",
    "        \n",
    "        self.result[f\"{self.group}_minigame_{game_name}_record_cnt\"] = record_cnt\n",
    "        self.result[f\"{self.group}_minigame_{game_name}_total_duration\"] = total_duration\n",
    "        self.result[f\"{self.group}_minigame_{game_name}_total_hover_duration\"] = total_hover_duration\n",
    "        self.result[f\"{self.group}_minigame_{game_name}_hover_cnt\"] = hover_cnt\n",
    "        self.result[f\"{self.group}_minigame_{game_name}_click_cnt\"] = click_cnt\n",
    "            \n",
    "\n",
    "    def get_test(self):\n",
    "        self._prep()\n",
    "        self._total_record_cnt()\n",
    "        self._group_elapsed_time()\n",
    "        self._cat_record_cnt(\"event_name\")\n",
    "        self._cat_record_cnt(\"name\")\n",
    "        self._cat_record_cnt(\"page\")\n",
    "        self._cat_record_cnt(\"level\")\n",
    "        self._cat_record_cnt(\"room_fqid\")\n",
    "        self._cat_record_cnt(\"fqid\")\n",
    "        self._cat_record_cnt(\"text_fqid\")\n",
    "        self._cat_record_cnt(\"event_name+name\")\n",
    "        self._cat_record_cnt(\"event_name+room_fqid\")\n",
    "        self._cat_record_cnt(\"event_name+fqid\")\n",
    "        self._cat_col_nunique(\"text\")\n",
    "        self._cat_col_nunique(\"text_fqid\")\n",
    "        self._cat_col_nunique(\"room_fqid\")\n",
    "        self._cat_col_nunique(\"fqid\")\n",
    "        self._cat_col_nunique(\"event_name+name\")\n",
    "        self._cat_col_nunique(\"event_name+room_fqid\")\n",
    "        self._cat_col_nunique(\"event_name+fqid\")\n",
    "\n",
    "        self._agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"], \n",
    "                           aggs=[\"mean\"])\n",
    "        self._agg_features(val_cols=[\"time_diff\", \"hover_duration\"], \n",
    "                           aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"])\n",
    "        self._agg_features(val_cols=[\"elapsed_time\", \"index\"], \n",
    "                           aggs=[\"max\", \"min\"])\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"room_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"elapsed_time\", \"index\"],\n",
    "                               aggs=[\"max\", \"min\"],\n",
    "                               cat_col=\"level\")\n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['checkpoint', 'map_hover', 'object_hover'])        \n",
    "        self._cat_agg_features(val_cols=[\"room_coor_x\", \"room_coor_y\", \"screen_coor_x\", \"screen_coor_y\"],\n",
    "                               aggs=[\"mean\"],\n",
    "                               cat_col=\"name\")\n",
    "        self._cat_agg_features(val_cols=[\"hover_duration\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['cutscene_click', 'person_click', 'navigate_click',\n",
    "                                             'observation_click', 'notification_click', 'object_click',\n",
    "                                             'map_click', 'checkpoint', 'notebook_click'])\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+name\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+room_fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"time_diff\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+fqid\")\n",
    "        self._cat_agg_features(val_cols=[\"hover_duration\"],\n",
    "                               aggs=[\"mean\", \"max\", \"min\", \"std\", \"sum\", \"median\"],\n",
    "                               cat_col=\"event_name+fqid\")    \n",
    "\n",
    "\n",
    "        self._agg_features_quantile(val_cols=[\"time_diff\", \"hover_duration\"])\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"event_name\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"room_fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"text_fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"level\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"hover_duration\"],\n",
    "                               cat_col=\"event_name\",\n",
    "                               not_use_cats=['cutscene_click', 'person_click', 'navigate_click',\n",
    "                                             'observation_click', 'notification_click', 'object_click',\n",
    "                                             'map_click', 'checkpoint', 'notebook_click'])\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"event_name+name\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"event_name+room_fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"time_diff\"], cat_col=\"event_name+fqid\")\n",
    "        self._cat_agg_features_quantile(val_cols=[\"hover_duration\"], cat_col=\"event_name+fqid\")    \n",
    "\n",
    "        self._cat_change_cnt(\"text_fqid\")\n",
    "        self._cat_change_cnt(\"room_fqid\")\n",
    "\n",
    "        if self.group == \"0-4\":\n",
    "            self._add_minigame_features(\"tunic\", \"tunic.hub.slip\")\n",
    "            self._add_minigame_features(\"plaque\", \"plaque.face.date\")\n",
    "        \n",
    "        elif self.group == \"5-12\":\n",
    "            self._add_minigame_features(\"businesscards\", \"businesscards.card_bingo.bingo\")\n",
    "            self._add_minigame_features(\"logbook\", \"logbook.page.bingo\")\n",
    "            self._add_minigame_features(\"reader\", \"reader.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals\", \"journals.pic_2.bingo\")\n",
    "        \n",
    "        elif self.group == \"13-22\":\n",
    "            self._add_minigame_features(\"tracks\", \"tracks.hub.deer\")\n",
    "            self._add_minigame_features(\"reader_flag\", \"reader_flag.paper2.bingo\")\n",
    "            self._add_minigame_features(\"journals_flag\", \"journals_flag.pic_0.bingo\")\n",
    "        \n",
    "        return self.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesTrain(sessions, labels)\n",
    "    train = feat.get_train()\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels, feature_select=False, need_create_features=[]):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    feat = FeaturesInf(sessions, labels, feature_select, need_create_features)\n",
    "    test = feat.get_test()\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesSelect:\n",
    "    def __init__(self, df, init_features, corr_th=0.99):\n",
    "        self.init_features = init_features\n",
    "        self.df = cudf.from_pandas(df)\n",
    "        self.corr_th = corr_th\n",
    "        self.drop_cols = []\n",
    "    \n",
    "    def _high_corr_features_drop(self):\n",
    "        num_cols = self.df[self.init_features].select_dtypes(include=\"number\").columns\n",
    "\n",
    "        # 特徴量間の相関行列を計算\n",
    "        corr_matrix = self.df[num_cols].fillna(-1).corr().abs().to_pandas()\n",
    "        # 相関行列の上三角行列を取得します。（相関行列が対称であるため、重複する相関を取り除くため）\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        drop_cols = []\n",
    "        for c in num_cols:\n",
    "            if any(upper[c] > self.corr_th):\n",
    "                drop_cols.append(c)\n",
    "                upper = upper.drop(index=c)\n",
    "        print(f\"特徴量間の相関性が高い特徴量を{str(len(drop_cols))}個抽出\")\n",
    "        self.df = self.df.drop(columns=drop_cols)\n",
    "        self.drop_cols = list(set(self.drop_cols + drop_cols))\n",
    "\n",
    "    def features_select(self):\n",
    "        self._high_corr_features_drop()\n",
    "        selected_features = list(set(self.init_features) - set(self.drop_cols))\n",
    "        print(f\"{str(len(self.init_features))} -> {str(len(selected_features))}\")\n",
    "\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}_cleaned.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "    \n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "        # 特徴量選択\n",
    "        if cfg.base_exp is None:\n",
    "            features = FeaturesSelect(train, features).features_select()\n",
    "        else:\n",
    "            # 使用する特徴量の抽出\n",
    "            features = pd.read_csv(cfg.output_dir + f\"{cfg.base_exp}/fi_{group}.csv\").head(cfg.n_features)[\"feature\"].tolist()\n",
    "\n",
    "        gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "        fis = []\n",
    "        \n",
    "        oof_groups = []\n",
    "        for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "            model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\"\n",
    "            \n",
    "            print(f\"fold : {i}\")\n",
    "            tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "            vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "            tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "            if os.path.exists(model_path):\n",
    "                print(f\"modelが既に存在するのでロード : {model_path}\")\n",
    "                model = lgb.Booster(model_file=model_path)\n",
    "            else:\n",
    "                model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                                num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)\n",
    "            # モデル出力\n",
    "            model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\")\n",
    "        \n",
    "            # valid_pred\n",
    "            oof_fold = train.iloc[vl_idx].copy()\n",
    "            oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "            oof_groups.append(oof_fold)\n",
    "\n",
    "            # 特徴量重要度\n",
    "            fi_fold = pd.DataFrame()\n",
    "            fi_fold[\"feature\"] = model.feature_name()\n",
    "            fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "            fi_fold[\"fold\"] = i\n",
    "            fis.append(fi_fold)\n",
    "\n",
    "        fi = pd.concat(fis)    \n",
    "        fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "        fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "        fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi_{group}.csv\", index=False)\n",
    "\n",
    "        oof_group = pd.concat(oof_groups)\n",
    "        oofs.append(oof_group)\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\", \"level_group\"])\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = oof_group.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{group}_pred_mean\", \"max\":f\"{group}_pred_max\", \"min\":f\"{group}_pred_min\", \"std\":f\"{group}_pred_std\"})\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"level_group2\"] = test[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"level_group2\"] = sub[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in test.groupby(\"level_group2\")]\n",
    "    subs = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in sub.groupby(\"level_group2\")]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    model_dict = {}\n",
    "    features_dict = {}\n",
    "    for g in level_group_list:\n",
    "        if mode == \"local_cv\":\n",
    "            model_paths = [cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_paths = [f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        model_dict[g] = [lgb.Booster(model_file=p) for p in model_paths]\n",
    "        features_dict[g] = model_dict[g][0].feature_name()\n",
    "    need_create_features = features_dict[\"0-4\"] + features_dict[\"5-12\"] + features_dict[\"13-22\"]\n",
    "    not_drop_cols = [\"0-4_elapsed_time_max\", \"0-4_index_max\", \"5-12_elapsed_time_max\", \"5-12_index_max\", \"13-22_elapsed_time_max\", \"13-22_index_max\",\n",
    "                     \"0-4_elapsed_time_min\", \"0-4_index_min\", \"5-12_elapsed_time_min\", \"5-12_index_min\", \"13-22_elapsed_time_min\", \"13-22_index_min\"]\n",
    "    need_create_features = need_create_features + not_drop_cols\n",
    "    need_create_features = list(set(need_create_features))\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission, feature_select=True, need_create_features=need_create_features)\n",
    "        features = features_dict[level_group]\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "\n",
    "        prev_features_df = test.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\"])\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = model_dict[level_group][i]\n",
    "            preds += model.predict(test[features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        test[\"pred\"] = preds\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = test.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{level_group}_pred_mean\", \"max\":f\"{level_group}_pred_max\", \"min\":f\"{level_group}_pred_min\", \"std\":f\"{level_group}_pred_std\"})\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_train_test_process_identity():\n",
    "    iter_train = get_mock_iter_train()\n",
    "    iter_test = get_mock_iter_test()\n",
    "\n",
    "    print(\"train_iter\")\n",
    "    train_df_dict = {}\n",
    "    train_features_dict = {}\n",
    "    prev_features_df = None\n",
    "    for (sessions, sub) in iter_train:\n",
    "        group = sessions[\"level_group\"].values[0]\n",
    "        print(group)\n",
    "        train = get_train_dataset(sessions, sub)\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "            # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "        train_df_dict[group] = train[[\"session_id\"]+features].sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "        prev_features_df = train[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        train_features_dict[group] = features\n",
    "\n",
    "\n",
    "    print(\"test_iter\")\n",
    "    test_dfs_0_4 = []\n",
    "    test_dfs_5_12 = []\n",
    "    test_dfs_13_22 = []\n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        session_id = test_sessions[\"session_id\"].values[0]\n",
    "        print(session_id, level_group)\n",
    "        features = train_features_dict[level_group]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in test.columns if c not in not_use_cols]\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        if level_group == \"0-4\":\n",
    "            test_dfs_0_4.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"5-12\":\n",
    "            test_dfs_5_12.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"13-22\":\n",
    "            test_dfs_13_22.append(test[[\"session_id\"]+features])\n",
    "        \n",
    "\n",
    "    test_dfs_0_4 = pd.concat(test_dfs_0_4, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_5_12 = pd.concat(test_dfs_5_12, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_13_22 = pd.concat(test_dfs_13_22, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "\n",
    "    assert train_df_dict[\"0-4\"][train_features_dict[\"0-4\"]].equals(test_dfs_0_4[train_features_dict[\"0-4\"]])\n",
    "    assert train_df_dict[\"5-12\"][train_features_dict[\"5-12\"]].equals(test_dfs_5_12[train_features_dict[\"5-12\"]])\n",
    "    assert train_df_dict[\"13-22\"][train_features_dict[\"13-22\"]].equals(test_dfs_13_22[train_features_dict[\"13-22\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iter\n",
      "0-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 369.07it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 387.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 345.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 365.06it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 333.88it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 376.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13-22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 381.62it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 371.86it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 414.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_iter\n",
      "20090109393214576 0-4\n",
      "20090109393214576 5-12\n",
      "20090109393214576 13-22\n",
      "20090312143683264 0-4\n",
      "20090312143683264 5-12\n",
      "20090312143683264 13-22\n",
      "20090312331414616 0-4\n",
      "20090312331414616 5-12\n",
      "20090312331414616 13-22\n",
      "0-4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlocal_cv\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     valid_train_test_process_identity()\n\u001b[0;32m----> 3\u001b[0m     run_train()\n\u001b[1;32m      4\u001b[0m inference(cfg\u001b[39m.\u001b[39mmode)\n",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m train_sessions \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(cfg\u001b[39m.\u001b[39mprep_dir \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m{\u001b[39;00mgroup\u001b[39m}\u001b[39;00m\u001b[39m_cleaned.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m labels \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(cfg\u001b[39m.\u001b[39mprep_dir \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_labels\u001b[39m\u001b[39m{\u001b[39;00mgroup\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m train \u001b[39m=\u001b[39m get_train_dataset(train_sessions, labels)\n\u001b[1;32m     11\u001b[0m \u001b[39m# 一つ前のlevel_groupの特徴量を追加\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m prev_features_df \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m, in \u001b[0;36mget_train_dataset\u001b[0;34m(sessions, labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# 特徴量生成\u001b[39;00m\n\u001b[1;32m      6\u001b[0m feat \u001b[39m=\u001b[39m FeaturesTrain(sessions, labels)\n\u001b[0;32m----> 7\u001b[0m train \u001b[39m=\u001b[39m feat\u001b[39m.\u001b[39;49mget_train()\n\u001b[1;32m      8\u001b[0m train[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m train[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m train\n",
      "Cell \u001b[0;32mIn[26], line 255\u001b[0m, in \u001b[0;36mFeaturesTrain.get_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cat_agg_features_quantile(val_cols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtime_diff\u001b[39m\u001b[39m\"\u001b[39m], cat_col\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mroom_fqid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cat_agg_features_quantile(val_cols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtime_diff\u001b[39m\u001b[39m\"\u001b[39m], cat_col\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfqid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 255\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cat_agg_features_quantile(val_cols\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mtime_diff\u001b[39;49m\u001b[39m\"\u001b[39;49m], cat_col\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext_fqid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cat_agg_features_quantile(val_cols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtime_diff\u001b[39m\u001b[39m\"\u001b[39m], cat_col\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cat_agg_features_quantile(val_cols\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mhover_duration\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    258\u001b[0m                        cat_col\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mevent_name\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    259\u001b[0m                        not_use_cats\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcutscene_click\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mperson_click\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnavigate_click\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    260\u001b[0m                                      \u001b[39m'\u001b[39m\u001b[39mobservation_click\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnotification_click\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mobject_click\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    261\u001b[0m                                      \u001b[39m'\u001b[39m\u001b[39mmap_click\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcheckpoint\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnotebook_click\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[26], line 93\u001b[0m, in \u001b[0;36mFeaturesTrain._cat_agg_features_quantile\u001b[0;34m(self, val_cols, cat_col, not_use_cats)\u001b[0m\n\u001b[1;32m     90\u001b[0m quantiles_str \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mquantile0_1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquantile0_2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquantile0_3\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquantile0_4\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquantile0_6\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquantile0_7\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquantile0_8\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mquantile0_9\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     91\u001b[0m aggs \u001b[39m=\u001b[39m [quantile0_1, quantile0_2, quantile0_3, quantile0_4, quantile0_6, quantile0_7, quantile0_8, quantile0_9]\n\u001b[0;32m---> 93\u001b[0m add_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msessions_df\u001b[39m.\u001b[39;49mgroupby([\u001b[39m\"\u001b[39;49m\u001b[39msession_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlevel_group\u001b[39;49m\u001b[39m\"\u001b[39;49m, cat_col])[val_cols]\u001b[39m.\u001b[39;49magg(aggs)\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m not_use_cats \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     cat_list \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m cat_col_lists[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup][cat_col] \u001b[39mif\u001b[39;00m c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m not_use_cats]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/generic.py:895\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m func \u001b[39m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[1;32m    894\u001b[0m op \u001b[39m=\u001b[39m GroupByApply(\u001b[39mself\u001b[39m, func, args, kwargs)\n\u001b[0;32m--> 895\u001b[0m result \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39;49magg()\n\u001b[1;32m    896\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dict_like(func) \u001b[39mand\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    897\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:175\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magg_dict_like()\n\u001b[1;32m    173\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(arg):\n\u001b[1;32m    174\u001b[0m     \u001b[39m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magg_list_like()\n\u001b[1;32m    177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(arg):\n\u001b[1;32m    178\u001b[0m     f \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mget_cython_func(arg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:401\u001b[0m, in \u001b[0;36mApply.agg_list_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[39m# Capture and suppress any warnings emitted by us in the call\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[39m# to agg below, but pass through any warnings that were\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[39m# generated otherwise.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     \u001b[39m# This is necessary because of https://bugs.python.org/issue29672\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     \u001b[39m# See GH #43741 for more details\u001b[39;00m\n\u001b[1;32m    400\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings(record\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m record:\n\u001b[0;32m--> 401\u001b[0m         new_res \u001b[39m=\u001b[39m colg\u001b[39m.\u001b[39;49maggregate(arg)\n\u001b[1;32m    402\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(record) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    403\u001b[0m         match \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(depr_nuisance_columns_msg\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39m.*\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/generic.py:281\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, abc\u001b[39m.\u001b[39mIterable):\n\u001b[1;32m    278\u001b[0m     \u001b[39m# Catch instances of lists / tuples\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[39m# but not the class list / tuple itself.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     func \u001b[39m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[0;32m--> 281\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aggregate_multiple_funcs(func)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m relabeling:\n\u001b[1;32m    283\u001b[0m         \u001b[39m# columns is not narrowed by mypy from relabeling flag\u001b[39;00m\n\u001b[1;32m    284\u001b[0m         \u001b[39massert\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# for mypy\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/generic.py:336\u001b[0m, in \u001b[0;36mSeriesGroupBy._aggregate_multiple_funcs\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mfor\u001b[39;00m idx, (name, func) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(arg):\n\u001b[1;32m    335\u001b[0m     key \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mOutputKey(label\u001b[39m=\u001b[39mname, position\u001b[39m=\u001b[39midx)\n\u001b[0;32m--> 336\u001b[0m     results[key] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maggregate(func)\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, DataFrame) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    339\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m concat\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/generic.py:294\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, cyfunc)()\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39mnkeys \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_agg_general(func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    296\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_agg_general(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1682\u001b[0m, in \u001b[0;36mGroupBy._python_agg_general\u001b[0;34m(self, func, raise_on_typeerror, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1678\u001b[0m name \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mname\n\u001b[1;32m   1680\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[39m# if this function is invalid for this dtype, we will ignore it.\u001b[39;00m\n\u001b[0;32m-> 1682\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49magg_series(obj, f)\n\u001b[1;32m   1683\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     \u001b[39mif\u001b[39;00m raise_on_typeerror:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/ops.py:1081\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m   1078\u001b[0m     preserve_dtype \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1081\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aggregate_series_pure_python(obj, func)\n\u001b[1;32m   1083\u001b[0m npvalues \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/ops.py:1104\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m   1101\u001b[0m splitter \u001b[39m=\u001b[39m get_splitter(obj, ids, ngroups, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   1103\u001b[0m \u001b[39mfor\u001b[39;00m i, group \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(splitter):\n\u001b[0;32m-> 1104\u001b[0m     res \u001b[39m=\u001b[39m func(group)\n\u001b[1;32m   1105\u001b[0m     res \u001b[39m=\u001b[39m libreduction\u001b[39m.\u001b[39mextract_result(res)\n\u001b[1;32m   1107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m initialized:\n\u001b[1;32m   1108\u001b[0m         \u001b[39m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1668\u001b[0m, in \u001b[0;36mGroupBy._python_agg_general.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m   1666\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_python_agg_general\u001b[39m(\u001b[39mself\u001b[39m, func, \u001b[39m*\u001b[39margs, raise_on_typeerror\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1667\u001b[0m     func \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mis_builtin_func(func)\n\u001b[0;32m-> 1668\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: func(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1670\u001b[0m     \u001b[39m# iterate through \"columns\" ex exclusions to populate output dict\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m     output: \u001b[39mdict\u001b[39m[base\u001b[39m.\u001b[39mOutputKey, ArrayLike] \u001b[39m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m, in \u001b[0;36mquantile0_8\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquantile0_8\u001b[39m(s):\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m s\u001b[39m.\u001b[39;49mquantile(\u001b[39m0.8\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:2683\u001b[0m, in \u001b[0;36mSeries.quantile\u001b[0;34m(self, q, interpolation)\u001b[0m\n\u001b[1;32m   2679\u001b[0m \u001b[39m# We dispatch to DataFrame so that core.internals only has to worry\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \u001b[39m#  about 2D cases.\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m-> 2683\u001b[0m result \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mquantile(q\u001b[39m=\u001b[39;49mq, interpolation\u001b[39m=\u001b[39;49minterpolation, numeric_only\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   2684\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   2685\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:11282\u001b[0m, in \u001b[0;36mDataFrame.quantile\u001b[0;34m(self, q, axis, numeric_only, interpolation, method)\u001b[0m\n\u001b[1;32m  11274\u001b[0m res_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantile(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m  11275\u001b[0m     [q],\n\u001b[1;32m  11276\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11279\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m  11280\u001b[0m )\n\u001b[1;32m  11281\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msingle\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m> 11282\u001b[0m     res \u001b[39m=\u001b[39m res_df\u001b[39m.\u001b[39;49miloc[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m  11283\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m  11284\u001b[0m     \u001b[39m# cannot directly iloc over sparse arrays\u001b[39;00m\n\u001b[1;32m  11285\u001b[0m     res \u001b[39m=\u001b[39m res_df\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1627\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[39m# validate the location\u001b[39;00m\n\u001b[1;32m   1625\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1627\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_ixs(key, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3720\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3718\u001b[0m \u001b[39m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3719\u001b[0m copy \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(new_mgr\u001b[39m.\u001b[39marray, np\u001b[39m.\u001b[39mndarray) \u001b[39mand\u001b[39;00m new_mgr\u001b[39m.\u001b[39marray\u001b[39m.\u001b[39mbase \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 3720\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_constructor_sliced(new_mgr, name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex[i])\u001b[39m.\u001b[39;49m__finalize__(\n\u001b[1;32m   3721\u001b[0m     \u001b[39mself\u001b[39;49m\n\u001b[1;32m   3722\u001b[0m )\n\u001b[1;32m   3723\u001b[0m result\u001b[39m.\u001b[39m_set_is_copy(\u001b[39mself\u001b[39m, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m   3724\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:5868\u001b[0m, in \u001b[0;36mNDFrame.__finalize__\u001b[0;34m(self, other, method, **kwargs)\u001b[0m\n\u001b[1;32m   5865\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m other\u001b[39m.\u001b[39mattrs:\n\u001b[1;32m   5866\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattrs[name] \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39mattrs[name]\n\u001b[0;32m-> 5868\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflags\u001b[39m.\u001b[39;49mallows_duplicate_labels \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mallows_duplicate_labels\n\u001b[1;32m   5869\u001b[0m \u001b[39m# For subclasses using _metadata.\u001b[39;00m\n\u001b[1;32m   5870\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata) \u001b[39m&\u001b[39m \u001b[39mset\u001b[39m(other\u001b[39m.\u001b[39m_metadata):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/flags.py:88\u001b[0m, in \u001b[0;36mFlags.allows_duplicate_labels\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m@allows_duplicate_labels\u001b[39m\u001b[39m.\u001b[39msetter\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mallows_duplicate_labels\u001b[39m(\u001b[39mself\u001b[39m, value: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(value)\n\u001b[0;32m---> 88\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_obj()\n\u001b[1;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThis flag\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms object has been deleted.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    valid_train_test_process_identity()\n",
    "    run_train()\n",
    "inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
