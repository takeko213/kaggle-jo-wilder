{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp103"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minigameの正解座標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    mode = \"local_cv\" # \"local_cv\" or \"kaggle_inf\" \n",
    "    exp_name = \"exp103\"\n",
    "    input_dir = \"/mnt/predict-student-performance-from-game-play/input/\"\n",
    "    output_dir = \"/mnt/predict-student-performance-from-game-play/output/\"\n",
    "    prep_dir = \"/mnt/predict-student-performance-from-game-play/prep/\"\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    best_threshold = 0.630 # local_cvの結果を入れる\n",
    "    base_exp = None # 特徴量重要度を使う元のexp\n",
    "    n_features = 500 # 特徴量削減の数\n",
    "cfg = Cfg()\n",
    "\n",
    "if cfg.mode == \"local_cv\":\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "    import cudf\n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    import jo_wilder_310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary', \n",
    "    'boosting': 'gbdt', \n",
    "    'learning_rate': 0.01, \n",
    "    'metric': 'binary_logloss', \n",
    "    'seed': cfg.seed, \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 4.134488140102331, \n",
    "    'lambda_l2': 0.007775200046481757, \n",
    "    'num_leaves': 75, \n",
    "    'feature_fraction': 0.5, \n",
    "    'bagging_fraction': 0.7036110805680353, \n",
    "    'bagging_freq': 3, \n",
    "    'min_data_in_leaf': 50, \n",
    "    'min_child_samples': 100\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_group_list = ['0-4', '5-12', '13-22']\n",
    "level_group_map = {\n",
    "    \"q1\":\"0-4\", \"q2\":\"0-4\", \"q3\":\"0-4\",\n",
    "    \"q4\":\"5-12\", \"q5\":\"5-12\", \"q6\":\"5-12\", \"q7\":\"5-12\", \"q8\":\"5-12\", \"q9\":\"5-12\", \"q10\":\"5-12\", \"q11\":\"5-12\", \"q12\":\"5-12\", \"q13\":\"5-12\",\n",
    "    \"q14\":\"13-22\", \"q15\":\"13-22\", \"q16\":\"13-22\", \"q17\":\"13-22\", \"q18\":\"13-22\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_seqs = {}\n",
    "ideal_seqs[\"0-4\"] = \"a b c d c f g\"\n",
    "ideal_seqs[\"5-12\"] = \"g c b h b c e i e c j k l m l c e i e c n\"\n",
    "ideal_seqs[\"13-22\"] = \"n c b o b c e i e c b o b c p c q r l m l c e i e c s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = {}\n",
    "n_features[\"0-4\"] = 600\n",
    "n_features[\"5-12\"] = 1000\n",
    "n_features[\"13-22\"] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    with open(cfg.prep_dir + 'cat_col_lists_v3.pkl', 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) \n",
    "    with open(cfg.prep_dir + 'room_fqid_encoder.pkl', 'rb') as f:\n",
    "        room_fqid_encoder = pickle.load(f) \n",
    "\n",
    "elif cfg.mode == \"kaggle_inf\":\n",
    "    with open(\"/kaggle/input/psp-cat-col-lists/cat_col_lists_v3.pkl\", 'rb') as f:\n",
    "        cat_col_lists = pickle.load(f) \n",
    "    with open(\"/kaggle/input/room-fqid-encoder/room_fqid_encoder.pkl\", 'rb') as f:\n",
    "        room_fqid_encoder = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels_df_train(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    # trainの特徴量と結合するためにquestionに対応するlabel_groupを列として設けておく\n",
    "    labels[\"level_group\"] = \"\"\n",
    "    labels.loc[labels[\"question\"]<=3, \"level_group\"] = \"0-4\"\n",
    "    labels.loc[(labels[\"question\"]>=4)&(labels[\"question\"]<=13), \"level_group\"] = \"5-12\"\n",
    "    labels.loc[labels[\"question\"]>=14, \"level_group\"] = \"13-22\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def transform_labels_df_inf(labels_):\n",
    "    \"\"\"\n",
    "    labelsデータを整形する\n",
    "    \"\"\"\n",
    "    labels = labels_.copy()\n",
    "    labels[\"question\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[1].replace(\"q\", \"\")).astype(int)\n",
    "    labels[\"session_id\"] = labels[\"session_id\"].apply(lambda x: x.split(\"_\")[0]).astype(int)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_maxmin(s):\n",
    "    try:\n",
    "        return s.max() - s.min()\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Features:\n",
    "    def __init__(self, sessions_df, need_create_features=None):\n",
    "        self.sessions_df = pl.from_pandas(sessions_df).sort([\"session_id\", \"index\"])\n",
    "        self.group = sessions_df[\"level_group\"].values[0]\n",
    "        self.need_create_features = need_create_features\n",
    "\n",
    "    def prep(self):\n",
    "        self.sessions_df = self.sessions_df.with_columns(\n",
    "            [(pl.col(\"elapsed_time\") - pl.col(\"elapsed_time\").shift(1)).clip(0, 1e9).fill_null(0).over([\"session_id\"]).alias(\"time_diff\"),\n",
    "             (pl.col(\"elapsed_time\").shift(-1)-pl.col(\"elapsed_time\")).clip(0, 1e9).fill_null(0).over([\"session_id\"]).alias(\"time_diff2\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"name\")).alias(\"event_name+name\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"room_fqid\")).alias(\"event_name+room_fqid\"),\n",
    "             (pl.col(\"event_name\") + \"_\" + pl.col(\"fqid\")).alias(\"event_name+fqid\"),\n",
    "             (pl.col(\"level\").cast(pl.Utf8) + \"_\" + pl.col(\"room_fqid\")).alias(\"level+room_fqid\"),\n",
    "             (pl.col(\"room_fqid\").map_dict(room_fqid_encoder).alias(\"room_fqid_encode\"))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "    def get_aggs(self):\n",
    "        g = self.group\n",
    "        cats = cat_col_lists[g]\n",
    "        aggs = []\n",
    "\n",
    "        # トータルレコード数\n",
    "        aggs += [pl.col(\"index\").count().alias(f\"{g}_record_cnt\")]\n",
    "\n",
    "        # グループ全体の経過時間\n",
    "        aggs += [pl.col(\"elapsed_time\").apply(lambda s:s.max() - s.min()).alias(f\"{g}_elapsed_time\")]\n",
    "\n",
    "        # 各categoryごとのレコード数\n",
    "        for c in [\"event_name\", \"name\", \"page\", \"level\", \"room_fqid\", \"fqid\", \"event_name+name\", \"event_name+room_fqid\", \"event_name+fqid\", \"level+room_fqid\"]:\n",
    "            aggs += [pl.col(\"index\").filter(pl.col(c)==v).count().fill_null(0).alias(f\"{g}_{c}_{str(v)}_record_cnt\") for v in cats[c]]\n",
    "        \n",
    "        # 各categoryごとのユニーク数\n",
    "        for c in [\"event_name\", \"name\", \"page\", \"level\", \"room_fqid\", \"fqid\", \"event_name+name\", \"event_name+room_fqid\", \"event_name+fqid\", \"level+room_fqid\"]:\n",
    "            aggs += [pl.col(c).drop_nulls().n_unique().fill_null(0).alias(f\"{g}_{c}_nunique\")]\n",
    "\n",
    "        # 各level - categoryごとのユニーク数\n",
    "        for c in [\"event_name\", \"name\", \"page\", \"level\", \"room_fqid\", \"fqid\", \"event_name+name\", \"event_name+room_fqid\", \"event_name+fqid\", \"level+room_fqid\"]:\n",
    "            aggs += [pl.col(c).filter(pl.col(\"level\")==l).drop_nulls().n_unique().fill_null(0).alias(f\"{g}_leve{l}_{c}_nunique\") for l in cats[\"level\"]]\n",
    "\n",
    "        # 集計量\n",
    "        for v in [\"elapsed_time\", \"index\"]:\n",
    "            aggs += [pl.col(v).max().fill_null(-1).alias(f\"{g}_{v}_max\").cast(pl.Float32), \n",
    "                     pl.col(v).max().fill_null(-1).alias(f\"{g}_{v}_min\").cast(pl.Float32)]\n",
    "\n",
    "        for v in [\"time_diff\", \"time_diff2\", \"hover_duration\"]:\n",
    "            aggs += [pl.col(v).max().fill_null(-1).alias(f\"{g}_{v}_max\").cast(pl.Float32), \n",
    "                     pl.col(v).min().fill_null(-1).alias(f\"{g}_{v}_min\").cast(pl.Float32), \n",
    "                     pl.col(v).std().fill_null(-1).alias(f\"{g}_{v}_std\").cast(pl.Float32),\n",
    "                     pl.col(v).mean().fill_null(-1).alias(f\"{g}_{v}_mean\").cast(pl.Float32), \n",
    "                     pl.col(v).sum().fill_null(-1).alias(f\"{g}_{v}_sum\").cast(pl.Float32), \n",
    "                     pl.col(v).median().fill_null(-1).alias(f\"{g}_{v}_median\").cast(pl.Float32)]\n",
    "            \n",
    "            aggs += [pl.col(v).quantile(0.25, \"nearest\").fill_null(-1).alias(f\"{g}_{v}_quantile025\"),\n",
    "                     pl.col(v).quantile(0.75, \"nearest\").fill_null(-1).alias(f\"{g}_{v}_quantile075\")\n",
    "            ]\n",
    "            \n",
    "        # カテゴリ×集計量\n",
    "        cs = [\"event_name\", \"room_fqid\", \"fqid\", \"text_fqid\", \"level\", \"name\", \"event_name+name\", \"event_name+room_fqid\", \"level+room_fqid\"]\n",
    "        vs = [\"time_diff\", \"time_diff2\"]\n",
    "        for c, v in itertools.product(cs, vs):\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).max().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_max\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).min().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_min\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).std().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_std\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).mean().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_mean\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).median().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_median\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).sum().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_sum\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).quantile(0.25, \"nearest\").fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_quantile025\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).quantile(0.75, \"nearest\").fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_quantile075\").cast(pl.Float32) for cat in cats[c]]\n",
    "\n",
    "        cs = [\"event_name\", \"room_fqid\", \"fqid\", \"text_fqid\", \"level\", \"name\", \"event_name+fqid\", \"event_name+room_fqid\", \"level+room_fqid\"]\n",
    "        vs = [\"elapsed_time\", \"index\"]\n",
    "        for c, v in itertools.product(cs, vs):\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).max().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_max\").cast(pl.Float32) for cat in cats[c]]\n",
    "            aggs += [pl.col(v).filter(pl.col(c)==cat).min().fill_null(-1).alias(f\"{g}_{c}_{cat}_{v}_min\").cast(pl.Float32) for cat in cats[c]]\n",
    "\n",
    "        # カテゴリの変化回数\n",
    "        for c in [\"room_fqid\", \"text_fqid\"]:\n",
    "            aggs += [(pl.col(c) != pl.col(c).shift(1)).sum().alias(f\"{g}_{c}_change_cnt\")]\n",
    "\n",
    "        # levelごとのカテゴリ変化回数\n",
    "        for c in [\"room_fqid\", \"text_fqid\"]:\n",
    "            aggs += [pl.col(c).filter((pl.col(\"level\")==l)&(pl.col(c) != pl.col(c).shift(1))).count().alias(f\"{g}_level{l}_{c}_change_cnt\") for l in cats[\"level\"]]\n",
    "\n",
    "        # object_hoverのduration関連の特徴量（各fqidごと）\n",
    "        fqids = [c.removeprefix(\"object_hover_\") for c in cats[\"event_name+fqid\"] if \"object_hover\" in c]\n",
    "        for fqid in fqids:\n",
    "            aggs += [pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).max().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_max\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).min().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_min\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_std\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_mean\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).median().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_median\"),\n",
    "                     pl.col(\"hover_duration\").filter((pl.col(\"event_name\")==\"object_hover\") & (pl.col(\"fqid\")==fqid)).sum().fill_null(-1).cast(pl.Float32).alias(f\"{g}_object_hover_{fqid}_hover_duration_sum\")\n",
    "                    ]\n",
    "\n",
    "        # miniゲームの所要時間\n",
    "        if g == \"0-4\":\n",
    "            for fqid_start, fqid_end in zip([\"tunic\", \"plaque\"],[\"tunic.hub.slip\", \"plaque.face.date\"]):\n",
    "                aggs += [\n",
    "                    pl.col(\"index\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_index_cnt\"),\n",
    "                    pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_duration\")\n",
    "                ]\n",
    "        elif g == \"5-12\":\n",
    "            for fqid_start, fqid_end in zip([\"businesscards\", \"logbook\", \"reader\", \"journals\"],[\"businesscards.card_bingo.bingo\", \"logbook.page.bingo\", \"reader.paper2.bingo\", \"journals.pic_2.bingo\"]):\n",
    "                aggs += [\n",
    "                    pl.col(\"index\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_index_cnt\"),\n",
    "                    pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_duration\")\n",
    "                ]\n",
    "        elif g == \"13-22\":\n",
    "            for fqid_start, fqid_end in zip([\"tracks\", \"reader_flag\", \"journals_flag\", \"directory\"],[\"tracks.hub.deer\", \"reader_flag.paper2.bingo\", \"journals_flag.pic_0.bingo\", \"directory.closeup.archivist\"]):\n",
    "                aggs += [\n",
    "                    pl.col(\"index\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_index_cnt\"),\n",
    "                    pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")==\"navigate_click\")&(pl.col(\"fqid\")==fqid_start))|((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid_end))).apply(diff_maxmin).alias(f\"{g}_minigame_{fqid_start}_duration\")\n",
    "                ]\n",
    "\n",
    "        # miniゲーム関連の集計特徴量\n",
    "        if g == \"0-4\":\n",
    "            event_fqids = [\n",
    "            'object_click_tunic', 'object_click_tunic.hub.slip',\n",
    "            'object_click_plaque.face.date', 'object_click_plaque'\n",
    "            ]\n",
    "        \n",
    "        elif g == \"5-12\":\n",
    "            event_fqids = [\n",
    "                'object_click_businesscards.card_0.next', 'object_click_businesscards.card_1.next', 'object_click_businesscards', 'object_click_businesscards.card_bingo.bingo', 'object_click_businesscards.card_bingo.next',\n",
    "                'object_click_logbook', 'object_click_logbook.page.bingo',\n",
    "                'object_click_reader.paper0.next', 'object_click_reader.paper1.next', 'object_click_reader.paper2.bingo', 'object_click_reader', 'object_click_reader_flag', 'object_click_reader_flag.paper0.next', 'object_click_reader_flag.paper1.next',\n",
    "                'object_click_reader_flag.paper2.bingo', 'object_click_reader.paper2.next', 'object_click_reader.paper1.prev', 'object_click_reader.paper0.prev', 'object_click_reader_flag.paper2.next', 'object_click_reader_flag.paper1.prev',\n",
    "                'object_click_reader_flag.paper0.prev', 'object_click_reader.paper2.prev', 'object_click_reader_flag.paper2.prev',\n",
    "                'object_click_journals', 'object_click_journals.hub.topics', 'object_click_journals.pic_0.next', 'object_click_journals.pic_1.next', 'object_click_journals.pic_2.bingo', 'object_click_journals_flag.hub.topics', 'object_click_journals_flag.pic_0.bingo',\n",
    "                'object_click_journals_flag', 'object_click_journals.pic_2.next', 'object_click_journals_flag.hub.topics_old', 'object_click_journals_flag.pic_0_old.next', 'object_click_journals_flag.pic_1_old.next', 'object_click_journals_flag.pic_0.next',\n",
    "                'object_click_journals_flag.pic_1.bingo', 'object_click_journals_flag.pic_1.next', 'object_click_journals_flag.pic_2.bingo', 'object_click_journals_flag.pic_2.next', 'object_click_journals_flag.pic_2_old.next'\n",
    "            ]\n",
    "\n",
    "        elif g == \"13-22\":\n",
    "            event_fqids = [\n",
    "                'object_click_tracks.hub.deer', 'object_click_tracks',\n",
    "                'object_click_reader_flag',\n",
    "                'object_click_reader_flag.paper0.next',\n",
    "                'object_click_reader_flag.paper1.next',\n",
    "                'object_click_reader_flag.paper2.bingo',\n",
    "                'object_click_reader_flag.paper2.next',\n",
    "                'object_click_reader_flag.paper1.prev',\n",
    "                'object_click_reader_flag.paper0.prev',\n",
    "                'object_click_reader_flag.paper2.prev',\n",
    "                'object_click_journals_flag.hub.topics',\n",
    "                'object_click_journals_flag.pic_0.bingo',\n",
    "                'object_click_journals_flag',\n",
    "                'object_click_journals_flag.hub.topics_old',\n",
    "                'object_click_journals_flag.pic_0_old.next',\n",
    "                'object_click_journals_flag.pic_1_old.next',\n",
    "                'object_click_journals_flag.pic_0.next',\n",
    "                'object_click_journals_flag.pic_1.bingo',\n",
    "                'object_click_journals_flag.pic_1.next',\n",
    "                'object_click_journals_flag.pic_2.bingo',\n",
    "                'object_click_journals_flag.pic_2.next',\n",
    "                'object_click_journals_flag.pic_2_old.next',\n",
    "                'object_click_directory', 'object_click_directory.closeup.archivist'\n",
    "                ]\n",
    "        vs = [\"time_diff\", \"time_diff2\"]\n",
    "        for event_fqid, v in itertools.product(event_fqids, vs):\n",
    "            aggs += [pl.col(v).filter(pl.col(\"event_name+fqid\")==event_fqid).max().fill_null(-1).cast(pl.Float32).alias(f\"{g}_event_name+fqid_{event_fqid}_{v}_max\"),\n",
    "                     pl.col(v).filter(pl.col(\"event_name+fqid\")==event_fqid).min().fill_null(-1).cast(pl.Float32).alias(f\"{g}_event_name+fqid_{event_fqid}_{v}_min\"),\n",
    "                     pl.col(v).filter(pl.col(\"event_name+fqid\")==event_fqid).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_event_name+fqid_{event_fqid}_{v}_std\"),\n",
    "                     pl.col(v).filter(pl.col(\"event_name+fqid\")==event_fqid).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_event_name+fqid_{event_fqid}_{v}_mean\"),\n",
    "                     pl.col(v).filter(pl.col(\"event_name+fqid\")==event_fqid).median().fill_null(-1).cast(pl.Float32).alias(f\"{g}_event_name+fqid_{event_fqid}_{v}_median\"),\n",
    "                     pl.col(v).filter(pl.col(\"event_name+fqid\")==event_fqid).sum().fill_null(-1).cast(pl.Float32).alias(f\"{g}_event_name+fqid_{event_fqid}_{v}_sum\"),\n",
    "                     pl.col(v).filter(pl.col(\"event_name+fqid\")==event_fqid).quantile(0.25, \"nearest\").fill_null(-1).cast(pl.Float32).alias(f\"{g}_event_name+fqid_{event_fqid}_{v}_quantile025\"),\n",
    "                     pl.col(v).filter(pl.col(\"event_name+fqid\")==event_fqid).quantile(0.75, \"nearest\").fill_null(-1).cast(pl.Float32).alias(f\"{g}_event_name+fqid_{event_fqid}_{v}_quantile075\")\n",
    "            ]\n",
    "\n",
    "\n",
    "        # miniゲーム中のクリック座標\n",
    "        if g == \"0-4\":\n",
    "            fqids = [\"tunic\", \"plaque\"]\n",
    "        elif g == \"5-12\":\n",
    "            fqids = [\"businesscards\", \"logbook\", \"reader\", \"journals\"]\n",
    "        elif g == \"13-22\":\n",
    "            fqids = [\"tracks\", \"reader_flag\", \"journals_flag\", \"directory\"]\n",
    "\n",
    "        for fqid in fqids:\n",
    "            aggs += [\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).first().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_first_click_room_coor_x\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).first().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_first_click_room_coor_y\"),\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).last().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_last_click_room_coor_x\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).last().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_last_click_room_coor_y\"),\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_x_mean\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).mean().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_y_mean\"),\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_x_std\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).std().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_click_room_coor_y_std\"),\n",
    "            ]\n",
    "\n",
    "            # miniゲーム中の正解時のクリック座標\n",
    "        if g == \"0-4\":\n",
    "            fqids = [\"tunic.hub.slip\", \"plaque.face.date\"]\n",
    "        elif g == \"5-12\":\n",
    "            fqids = [\"businesscards.card_bingo.bingo\", \"logbook.page.bingo\", \"reader.paper2.bingo\", \"journals.pic_2.bingo\"]\n",
    "        elif g == \"13-22\":\n",
    "            fqids = [\"tracks.hub.deer\", \"reader_flag.paper2.bingo\", \"journals_flag.pic_0.bingo\", \"directory.closeup.archivist\"]\n",
    "\n",
    "        for fqid in fqids:\n",
    "            aggs += [\n",
    "                pl.col(\"room_coor_x\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).first().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_first_click_room_coor_x\"),\n",
    "                pl.col(\"room_coor_y\").filter((pl.col(\"event_name\")==\"object_click\")&(pl.col(\"fqid\")==fqid)).first().fill_null(-1).cast(pl.Float32).alias(f\"{g}_{fqid}_first_click_room_coor_y\"),\n",
    "            ]\n",
    "        \n",
    "        # 理想系列からのLevenshtein距離\n",
    "        aggs += [pl.col(\"room_fqid_encode\").filter(pl.col(\"room_fqid_encode\")!=pl.col(\"room_fqid_encode\").shift(1)).str.concat(\" \").apply(lambda x: Levenshtein.distance(x, ideal_seqs[g])).alias(f\"{g}_room_fqid_leven_dist\")]\n",
    "\n",
    "        # 生成する特徴量を限定\n",
    "        if self.need_create_features is not None:\n",
    "            feats = [re.findall(r'alias\\(\"(.*)\"\\)', str(a))[0] for a in aggs]\n",
    "            aggs = [aggs[i] for i, f in enumerate(feats) if f in self.need_create_features]\n",
    "\n",
    "        return aggs\n",
    "\n",
    "    def get_features(self):\n",
    "        self.prep()\n",
    "        aggs = self.get_aggs()\n",
    "        features = self.sessions_df.groupby([\"session_id\"], maintain_order=True).agg(aggs)\n",
    "        return features.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset(sessions, labels):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_train(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    features = Features(sessions).get_features()\n",
    "    train = labels.merge(features, on=[\"session_id\"], how=\"left\")\n",
    "    train[\"question\"] = train[\"question\"].astype(\"category\")\n",
    "\n",
    "    return train\n",
    "\n",
    "def get_test_dataset(sessions, labels, need_create_features=None):\n",
    "    # labelデータの整形\n",
    "    labels = transform_labels_df_inf(labels)\n",
    "\n",
    "    # 特徴量生成\n",
    "    features = Features(sessions, need_create_features).get_features()\n",
    "    test = labels.merge(features, on=[\"session_id\"], how=\"left\")\n",
    "    test[\"question\"] = test[\"question\"].astype(\"category\")\n",
    "\n",
    "    return test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(oof):\n",
    "    logloss = log_loss(oof[\"correct\"], oof[\"pred\"])\n",
    "\n",
    "    # find best th\n",
    "    scores = []; thresholds = []\n",
    "    best_score = 0; best_threshold = 0\n",
    "\n",
    "    for threshold in np.arange(0.4,0.81,0.01):\n",
    "        preds = (oof[\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[\"correct\"].values, preds, average='macro')   \n",
    "        scores.append(m)\n",
    "        thresholds.append(threshold)\n",
    "        if m>best_score:\n",
    "            best_score = m\n",
    "            best_threshold = threshold\n",
    "    print(\"logloss\", format(logloss, \".6f\"))\n",
    "    print(\"best_score\", format(best_score, \".6f\"))\n",
    "    print(\"best_threshold\", format(best_threshold, \".3f\"))\n",
    "\n",
    "    # Q別スコア\n",
    "    print(\"---\"*10)\n",
    "    for q in range(18):\n",
    "        q = q + 1\n",
    "        preds = (oof[oof[\"question\"]==q][\"pred\"].values>threshold).astype(int)\n",
    "        m = f1_score(oof[oof[\"question\"]==q][\"correct\"].values, preds, average='macro')\n",
    "        print(f\"Q{q} : F1 = {format(m, '.6f')}\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesSelect:\n",
    "    def __init__(self, df, init_features, corr_th=0.99):\n",
    "        self.init_features = init_features\n",
    "        self.df = cudf.from_pandas(df)\n",
    "        self.corr_th = corr_th\n",
    "        self.drop_cols = []\n",
    "    \n",
    "    def _high_corr_features_drop(self):\n",
    "        num_cols = self.df[self.init_features].select_dtypes(include=\"number\").columns\n",
    "\n",
    "        # 特徴量間の相関行列を計算\n",
    "        corr_matrix = self.df[num_cols].fillna(-1).corr().abs().to_pandas()\n",
    "        # 相関行列の上三角行列を取得します。（相関行列が対称であるため、重複する相関を取り除くため）\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        drop_cols = []\n",
    "        for c in num_cols:\n",
    "            if any(upper[c] > self.corr_th):\n",
    "                drop_cols.append(c)\n",
    "                upper = upper.drop(index=c)\n",
    "        print(f\"特徴量間の相関性が高い特徴量を{str(len(drop_cols))}個抽出\")\n",
    "        self.df = self.df.drop(columns=drop_cols)\n",
    "        self.drop_cols = list(set(self.drop_cols + drop_cols))\n",
    "\n",
    "    def features_select(self):\n",
    "        self._high_corr_features_drop()\n",
    "        selected_features = list(set(self.init_features) - set(self.drop_cols))\n",
    "        print(f\"{str(len(self.init_features))} -> {str(len(selected_features))}\")\n",
    "\n",
    "        return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_importance_feature_select(df, features, n):\n",
    "    null_imp_params = {\n",
    "        'objective': 'binary', \n",
    "        'boosting': 'gbdt', \n",
    "        'learning_rate': 0.1, \n",
    "        'metric': 'binary_logloss', \n",
    "        'seed': cfg.seed, \n",
    "        'feature_pre_filter': False, \n",
    "        'lambda_l1': 4.134488140102331, \n",
    "        'lambda_l2': 0.007775200046481757, \n",
    "        'num_leaves': 75, \n",
    "        'feature_fraction': 0.5, \n",
    "        'bagging_fraction': 0.7036110805680353, \n",
    "        'bagging_freq': 3, \n",
    "        'min_data_in_leaf': 50, \n",
    "        'min_child_samples': 100\n",
    "        }\n",
    "    \n",
    "    model = lgb.train(null_imp_params, lgb.Dataset(df[features], label=df[\"correct\"]), num_boost_round=100, verbose_eval=0)\n",
    "    fi_org = pd.DataFrame()\n",
    "    fi_org[\"feature\"] = model.feature_name()\n",
    "    fi_org[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "    fi_org = fi_org.sort_values(\"feature\", ignore_index=True)\n",
    "\n",
    "    null_imps = []\n",
    "    for i in range(10):\n",
    "        model = lgb.train(null_imp_params, lgb.Dataset(df[features], label=df[\"correct\"].sample(frac=1, random_state=i).values), num_boost_round=100, verbose_eval=0)\n",
    "        fi_tmp = pd.DataFrame()\n",
    "        fi_tmp[\"feature\"] = model.feature_name()\n",
    "        fi_tmp[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "        null_imps.append(fi_tmp)\n",
    "    null_imp = pd.concat(null_imps)\n",
    "    null_imp_mean = null_imp.groupby(\"feature\")[\"importance\"].quantile(0.8).reset_index().sort_values(\"feature\", ignore_index=True)\n",
    "    fi_org[\"null_imp\"] = null_imp_mean[\"importance\"]\n",
    "    fi_org[\"gain\"] = np.log(1e-10 + fi_org[\"importance\"] / (1 + fi_org[\"null_imp\"]))\n",
    "    selected_features = fi_org.sort_values(\"gain\", ascending=False).head(n)[\"feature\"].tolist()\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    oofs = []\n",
    "    prev_features_df = None # 次のlevel_groupで特徴量を使うための保持データ。0-4は前のlevel_groupがないので初期値はNone\n",
    "    for group in level_group_list:\n",
    "        print(group)\n",
    "        # データ読み込み\n",
    "        train_sessions = pd.read_csv(cfg.prep_dir + f\"train{group}_cleaned.csv\")\n",
    "        labels = pd.read_csv(cfg.prep_dir + f\"train_labels{group}.csv\")\n",
    "        train = get_train_dataset(train_sessions, labels)\n",
    "\n",
    "        # 一つ前のlevel_groupの特徴量を追加\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "\n",
    "            train[\"5-12_0-4_pred_max_diff\"] = train[\"5-12_pred_max\"] - train[\"0-4_pred_max\"]\n",
    "            train[\"5-12_0-4_pred_min_diff\"] = train[\"5-12_pred_min\"] - train[\"0-4_pred_min\"]\n",
    "            train[\"5-12_0-4_pred_mean_diff\"] = train[\"5-12_pred_mean\"] - train[\"0-4_pred_mean\"]\n",
    "            train[\"5-12_0-4_pred_std_diff\"] = train[\"5-12_pred_std\"] - train[\"0-4_pred_std\"]\n",
    "            train[\"5-12_0-4_pred_maxmin_diff\"] = train[\"5-12_pred_maxmin\"] - train[\"0-4_pred_maxmin\"]\n",
    "\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "        # 特徴量選択\n",
    "        if cfg.base_exp is None:\n",
    "            # features = FeaturesSelect(train, features).features_select()\n",
    "            features = null_importance_feature_select(train, features, n=n_features[group])\n",
    "        else:\n",
    "            # 使用する特徴量の抽出\n",
    "            features = pd.read_csv(cfg.output_dir + f\"{cfg.base_exp}/fi_{group}.csv\").head(cfg.n_features)[\"feature\"].tolist()\n",
    "\n",
    "        gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "        fis = []\n",
    "        \n",
    "        oof_groups = []\n",
    "        for i, (tr_idx, vl_idx) in enumerate(gkf.split(train[features], train[target], train[\"session_id\"])):\n",
    "            model_path = cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\"\n",
    "            \n",
    "            print(f\"fold : {i}\")\n",
    "            tr_x, tr_y = train.iloc[tr_idx][features], train.iloc[tr_idx][target]\n",
    "            vl_x, vl_y = train.iloc[vl_idx][features], train.iloc[vl_idx][target]\n",
    "            tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "            vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "            if os.path.exists(model_path):\n",
    "                print(f\"modelが既に存在するのでロード : {model_path}\")\n",
    "                model = lgb.Booster(model_file=model_path)\n",
    "            else:\n",
    "                model = lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                                num_boost_round=20000, early_stopping_rounds=1000, verbose_eval=100)\n",
    "            # モデル出力\n",
    "            model.save_model(cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{group}_{i}.lgb\")\n",
    "        \n",
    "            # valid_pred\n",
    "            oof_fold = train.iloc[vl_idx].copy()\n",
    "            oof_fold[\"pred\"] = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "            oof_groups.append(oof_fold)\n",
    "\n",
    "            # 特徴量重要度\n",
    "            fi_fold = pd.DataFrame()\n",
    "            fi_fold[\"feature\"] = model.feature_name()\n",
    "            fi_fold[\"importance\"] = model.feature_importance(importance_type=\"gain\")\n",
    "            fi_fold[\"fold\"] = i\n",
    "            fis.append(fi_fold)\n",
    "\n",
    "        fi = pd.concat(fis)    \n",
    "        fi = fi.groupby(\"feature\")[\"importance\"].mean().reset_index()\n",
    "        fi = fi.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "        fi.to_csv(cfg.output_dir + f\"{cfg.exp_name}/fi_{group}.csv\", index=False)\n",
    "\n",
    "        oof_group = pd.concat(oof_groups)\n",
    "        oofs.append(oof_group)\n",
    "\n",
    "        # 次のlevel_groupで使う用に特徴量を保持\n",
    "        prev_features_df = train.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\", \"level_group\"])\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = oof_group.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{group}_pred_mean\", \"max\":f\"{group}_pred_max\", \"min\":f\"{group}_pred_min\", \"std\":f\"{group}_pred_std\"})\n",
    "        meta_df[f\"{group}_pred_maxmin\"] = meta_df[f\"{group}_pred_max\"] - meta_df[f\"{group}_pred_min\"]\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # cv\n",
    "    oof = pd.concat(oofs)\n",
    "    best_threshold = calc_metrics(oof)\n",
    "    cfg.best_threshold = best_threshold\n",
    "    oof[[\"session_id\", \"question\", \"pred\", \"correct\"]].to_csv(cfg.output_dir + f\"{cfg.exp_name}/oof.csv.gz\", compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mock_iter_train():\n",
    "    \"\"\"trainデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    sub[\"level_group\"] = sub[\"session_level\"].apply(lambda x: x.split(\"_\")[-1])\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"level_group2\"] = test[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"level_group2\"] = sub[\"level_group\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in test.groupby(\"level_group2\")]\n",
    "    subs = [df[1].drop(columns=[\"session_level\", \"level_group2\"]).reset_index(drop=True) for df in sub.groupby(\"level_group2\")]\n",
    "    return zip(tests, subs)\n",
    "\n",
    "def get_mock_iter_test():\n",
    "    \"\"\"testデータのiter分割を適用したtest_sample\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(cfg.input_dir + \"_old/test.csv\")\n",
    "    sub = pd.read_csv(cfg.input_dir + \"_old/sample_submission.csv\")\n",
    "    \n",
    "    # groupbyでiter作るときにgroup_levelの順番が崩れないように\n",
    "    test[\"session_level\"] = test[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "    sub[\"session_level\"] = sub[\"session_level\"].str.replace(\"13-22\", \"6\")\n",
    "\n",
    "    tests = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in test.groupby(\"session_level\")]\n",
    "    subs = [df[1].drop(columns=\"session_level\").reset_index(drop=True) for df in sub.groupby(\"session_level\")]\n",
    "    return zip(tests, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(mode):\n",
    "    if mode == \"local_cv\":\n",
    "        # time series apiを模したiterをモックとして用意する\n",
    "        iter_test = get_mock_iter_test()\n",
    "        start_time = time.time()\n",
    "    elif mode == \"kaggle_inf\":\n",
    "        env = jo_wilder_310.make_env()\n",
    "        iter_test = env.iter_test()\n",
    "        \n",
    "    model_dict = {}\n",
    "    features_dict = {}\n",
    "    for g in level_group_list:\n",
    "        if mode == \"local_cv\":\n",
    "            model_paths = [cfg.output_dir + f\"{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            model_paths = [f\"/kaggle/input/jo-wilder-{cfg.exp_name}/{cfg.exp_name}_model_{g}_{i}.lgb\" for i in range(cfg.n_splits)]\n",
    "        model_dict[g] = [lgb.Booster(model_file=p) for p in model_paths]\n",
    "        features_dict[g] = model_dict[g][0].feature_name()\n",
    "    need_create_features = features_dict[\"0-4\"] + features_dict[\"5-12\"] + features_dict[\"13-22\"]\n",
    "    not_drop_cols = [\"0-4_elapsed_time_max\", \"0-4_index_max\", \"5-12_elapsed_time_max\", \"5-12_index_max\", \"13-22_elapsed_time_max\", \"13-22_index_max\",\n",
    "                     \"0-4_elapsed_time_min\", \"0-4_index_min\", \"5-12_elapsed_time_min\", \"5-12_index_min\", \"13-22_elapsed_time_min\", \"13-22_index_min\"]\n",
    "    need_create_features = need_create_features + not_drop_cols\n",
    "    need_create_features = list(set(need_create_features))\n",
    "    \n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        test = get_test_dataset(test_sessions, sample_submission, need_create_features=need_create_features)\n",
    "        features = features_dict[level_group]\n",
    "        preds = np.zeros(len(test))\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "\n",
    "            test[\"5-12_0-4_pred_max_diff\"] = test[\"5-12_pred_max\"] - test[\"0-4_pred_max\"]\n",
    "            test[\"5-12_0-4_pred_min_diff\"] = test[\"5-12_pred_min\"] - test[\"0-4_pred_min\"]\n",
    "            test[\"5-12_0-4_pred_mean_diff\"] = test[\"5-12_pred_mean\"] - test[\"0-4_pred_mean\"]\n",
    "            test[\"5-12_0-4_pred_std_diff\"] = test[\"5-12_pred_std\"] - test[\"0-4_pred_std\"]\n",
    "            test[\"5-12_0-4_pred_maxmin_diff\"] = test[\"5-12_pred_maxmin\"] - test[\"0-4_pred_maxmin\"]\n",
    "\n",
    "        prev_features_df = test.groupby(\"session_id\").head(1).drop(columns=[\"question\", \"correct\"])\n",
    "\n",
    "        for i in range(cfg.n_splits):\n",
    "            model = model_dict[level_group][i]\n",
    "            preds += model.predict(test[features], num_iteration=model.best_iteration) / cfg.n_splits\n",
    "        test[\"pred\"] = preds\n",
    "        preds = (preds>cfg.best_threshold).astype(int)\n",
    "        sample_submission[\"correct\"] = preds\n",
    "\n",
    "        # meta_featureの付与\n",
    "        meta_df = test.groupby(\"session_id\")[\"pred\"].agg([\"mean\", \"max\", \"min\", \"std\"]).reset_index()\n",
    "        meta_df = meta_df.rename(columns={\"mean\":f\"{level_group}_pred_mean\", \"max\":f\"{level_group}_pred_max\", \"min\":f\"{level_group}_pred_min\", \"std\":f\"{level_group}_pred_std\"})\n",
    "        meta_df[f\"{level_group}_pred_maxmin\"] = meta_df[f\"{level_group}_pred_max\"] - meta_df[f\"{level_group}_pred_min\"]\n",
    "        prev_features_df = prev_features_df.merge(meta_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "        if mode == \"local_cv\":\n",
    "            print(sample_submission[\"correct\"].values)\n",
    "        elif mode == \"kaggle_inf\":\n",
    "            env.predict(sample_submission)\n",
    "    if mode == \"local_cv\":\n",
    "        process_time = format(time.time() - start_time, \".1f\")\n",
    "        print(\"sample_inf処理時間 : \", process_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_train_test_process_identity():\n",
    "    iter_train = get_mock_iter_train()\n",
    "    iter_test = get_mock_iter_test()\n",
    "\n",
    "    print(\"train_iter\")\n",
    "    train_df_dict = {}\n",
    "    train_features_dict = {}\n",
    "    prev_features_df = None\n",
    "    for (sessions, sub) in iter_train:\n",
    "        group = sessions[\"level_group\"].values[0]\n",
    "        print(group)\n",
    "        train = get_train_dataset(sessions, sub)\n",
    "        if prev_features_df is not None:\n",
    "            train = train.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "        else:\n",
    "            pass\n",
    "            # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if group == \"5-12\":\n",
    "            train[\"0-4_question_duration_time\"] = train[\"5-12_elapsed_time_min\"] - train[\"0-4_elapsed_time_max\"]\n",
    "            train[\"0-4_question_duration_index\"] = train[\"5-12_index_min\"] - train[\"0-4_index_max\"]\n",
    "        elif group == \"13-22\":\n",
    "            train[\"5-12_question_duration_time\"] = train[\"13-22_elapsed_time_min\"] - train[\"5-12_elapsed_time_max\"]\n",
    "            train[\"5-12_question_duration_index\"] = train[\"13-22_index_min\"] - train[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in train.columns if c not in not_use_cols]\n",
    "        train_df_dict[group] = train[[\"session_id\"]+features].sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "        prev_features_df = train[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        train_features_dict[group] = features\n",
    "\n",
    "\n",
    "    print(\"test_iter\")\n",
    "    test_dfs_0_4 = []\n",
    "    test_dfs_5_12 = []\n",
    "    test_dfs_13_22 = []\n",
    "    prev_features_df = None\n",
    "    for (test_sessions, sample_submission) in iter_test:\n",
    "        level_group = test_sessions[\"level_group\"].values[0]\n",
    "        session_id = test_sessions[\"session_id\"].values[0]\n",
    "        print(session_id, level_group)\n",
    "        features = train_features_dict[level_group]\n",
    "        test = get_test_dataset(test_sessions, sample_submission)\n",
    "\n",
    "        if level_group == \"0-4\":\n",
    "            pass\n",
    "        else:\n",
    "            test = test.merge(prev_features_df, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "        # 前のlevel_groupのquestionパートの経過時間特徴量\n",
    "        if level_group == \"5-12\":\n",
    "            test[\"0-4_question_duration_time\"] = test[\"5-12_elapsed_time_min\"] - test[\"0-4_elapsed_time_max\"]\n",
    "            test[\"0-4_question_duration_index\"] = test[\"5-12_index_min\"] - test[\"0-4_index_max\"]\n",
    "        elif level_group == \"13-22\":\n",
    "            test[\"5-12_question_duration_time\"] = test[\"13-22_elapsed_time_min\"] - test[\"5-12_elapsed_time_max\"]\n",
    "            test[\"5-12_question_duration_index\"] = test[\"13-22_index_min\"] - test[\"5-12_index_max\"]\n",
    "        target = \"correct\"\n",
    "        not_use_cols = [target, \"session_id\", \"level_group\"]\n",
    "        features = [c for c in test.columns if c not in not_use_cols]\n",
    "        prev_features_df = test[[\"session_id\"]+features].groupby(\"session_id\").head(1).drop(columns=\"question\")\n",
    "        if level_group == \"0-4\":\n",
    "            test_dfs_0_4.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"5-12\":\n",
    "            test_dfs_5_12.append(test[[\"session_id\"]+features])\n",
    "        elif level_group == \"13-22\":\n",
    "            test_dfs_13_22.append(test[[\"session_id\"]+features])\n",
    "        \n",
    "\n",
    "    test_dfs_0_4 = pd.concat(test_dfs_0_4, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_5_12 = pd.concat(test_dfs_5_12, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "    test_dfs_13_22 = pd.concat(test_dfs_13_22, ignore_index=True).sort_values([\"session_id\", \"question\"], ignore_index=True)\n",
    "\n",
    "    assert train_df_dict[\"0-4\"][train_features_dict[\"0-4\"]].equals(test_dfs_0_4[train_features_dict[\"0-4\"]])\n",
    "    assert train_df_dict[\"5-12\"][train_features_dict[\"5-12\"]].equals(test_dfs_5_12[train_features_dict[\"5-12\"]])\n",
    "    assert train_df_dict[\"13-22\"][train_features_dict[\"13-22\"]].equals(test_dfs_13_22[train_features_dict[\"13-22\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iter\n",
      "0-4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-12\n",
      "13-22\n",
      "test_iter\n",
      "20090109393214576 0-4\n",
      "20090109393214576 5-12\n",
      "20090109393214576 13-22\n",
      "20090312143683264 0-4\n",
      "20090312143683264 5-12\n",
      "20090312143683264 13-22\n",
      "20090312331414616 0-4\n",
      "20090312331414616 5-12\n",
      "20090312331414616 13-22\n",
      "0-4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.822779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.134606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.375769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.815375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.774802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.822452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.823739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.788883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.798646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.769250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 62211, number of negative: 8475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 1.853643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 936358\n",
      "[LightGBM] [Info] Number of data points in the train set: 70686, number of used features: 4168\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880104 -> initscore=1.993411\n",
      "[LightGBM] [Info] Start training from score 1.993411\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49821, number of negative: 6726\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 144574\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 600\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.881055 -> initscore=2.002456\n",
      "[LightGBM] [Info] Start training from score 2.002456\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.28332\tvalid_1's binary_logloss: 0.305312\n",
      "[200]\ttraining's binary_logloss: 0.250209\tvalid_1's binary_logloss: 0.282511\n",
      "[300]\ttraining's binary_logloss: 0.231805\tvalid_1's binary_logloss: 0.27417\n",
      "[400]\ttraining's binary_logloss: 0.217906\tvalid_1's binary_logloss: 0.269667\n",
      "[500]\ttraining's binary_logloss: 0.206779\tvalid_1's binary_logloss: 0.267825\n",
      "[600]\ttraining's binary_logloss: 0.197131\tvalid_1's binary_logloss: 0.267003\n",
      "[700]\ttraining's binary_logloss: 0.18858\tvalid_1's binary_logloss: 0.266589\n",
      "[800]\ttraining's binary_logloss: 0.180905\tvalid_1's binary_logloss: 0.266531\n",
      "[900]\ttraining's binary_logloss: 0.173935\tvalid_1's binary_logloss: 0.266525\n",
      "[1000]\ttraining's binary_logloss: 0.167665\tvalid_1's binary_logloss: 0.266756\n",
      "[1100]\ttraining's binary_logloss: 0.161808\tvalid_1's binary_logloss: 0.267023\n",
      "[1200]\ttraining's binary_logloss: 0.156398\tvalid_1's binary_logloss: 0.267132\n",
      "[1300]\ttraining's binary_logloss: 0.151366\tvalid_1's binary_logloss: 0.267298\n",
      "[1400]\ttraining's binary_logloss: 0.146585\tvalid_1's binary_logloss: 0.267541\n",
      "[1500]\ttraining's binary_logloss: 0.142299\tvalid_1's binary_logloss: 0.26791\n",
      "[1600]\ttraining's binary_logloss: 0.138239\tvalid_1's binary_logloss: 0.268315\n",
      "[1700]\ttraining's binary_logloss: 0.134372\tvalid_1's binary_logloss: 0.268851\n",
      "[1800]\ttraining's binary_logloss: 0.130778\tvalid_1's binary_logloss: 0.269388\n",
      "Early stopping, best iteration is:\n",
      "[885]\ttraining's binary_logloss: 0.174948\tvalid_1's binary_logloss: 0.26646\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49692, number of negative: 6855\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 144614\n",
      "[LightGBM] [Info] Number of data points in the train set: 56547, number of used features: 600\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.878773 -> initscore=1.980866\n",
      "[LightGBM] [Info] Start training from score 1.980866\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.286076\tvalid_1's binary_logloss: 0.292881\n",
      "[200]\ttraining's binary_logloss: 0.252308\tvalid_1's binary_logloss: 0.271972\n",
      "[300]\ttraining's binary_logloss: 0.233562\tvalid_1's binary_logloss: 0.264325\n",
      "[400]\ttraining's binary_logloss: 0.219408\tvalid_1's binary_logloss: 0.260595\n",
      "[500]\ttraining's binary_logloss: 0.208169\tvalid_1's binary_logloss: 0.259292\n",
      "[600]\ttraining's binary_logloss: 0.1985\tvalid_1's binary_logloss: 0.258581\n",
      "[700]\ttraining's binary_logloss: 0.189795\tvalid_1's binary_logloss: 0.258282\n",
      "[800]\ttraining's binary_logloss: 0.182066\tvalid_1's binary_logloss: 0.258262\n",
      "[900]\ttraining's binary_logloss: 0.174873\tvalid_1's binary_logloss: 0.258244\n",
      "[1000]\ttraining's binary_logloss: 0.168324\tvalid_1's binary_logloss: 0.258235\n",
      "[1100]\ttraining's binary_logloss: 0.162317\tvalid_1's binary_logloss: 0.258446\n",
      "[1200]\ttraining's binary_logloss: 0.156867\tvalid_1's binary_logloss: 0.258529\n",
      "[1300]\ttraining's binary_logloss: 0.151773\tvalid_1's binary_logloss: 0.258816\n",
      "[1400]\ttraining's binary_logloss: 0.147092\tvalid_1's binary_logloss: 0.259038\n",
      "[1500]\ttraining's binary_logloss: 0.142656\tvalid_1's binary_logloss: 0.259325\n",
      "[1600]\ttraining's binary_logloss: 0.13839\tvalid_1's binary_logloss: 0.259754\n",
      "[1700]\ttraining's binary_logloss: 0.134659\tvalid_1's binary_logloss: 0.25997\n",
      "[1800]\ttraining's binary_logloss: 0.131073\tvalid_1's binary_logloss: 0.260431\n",
      "Early stopping, best iteration is:\n",
      "[860]\ttraining's binary_logloss: 0.17769\tvalid_1's binary_logloss: 0.258182\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49772, number of negative: 6778\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 144601\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 600\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880141 -> initscore=1.993771\n",
      "[LightGBM] [Info] Start training from score 1.993771\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.285343\tvalid_1's binary_logloss: 0.297913\n",
      "[200]\ttraining's binary_logloss: 0.252194\tvalid_1's binary_logloss: 0.274675\n",
      "[300]\ttraining's binary_logloss: 0.233657\tvalid_1's binary_logloss: 0.26579\n",
      "[400]\ttraining's binary_logloss: 0.219634\tvalid_1's binary_logloss: 0.261191\n",
      "[500]\ttraining's binary_logloss: 0.208399\tvalid_1's binary_logloss: 0.259351\n",
      "[600]\ttraining's binary_logloss: 0.198803\tvalid_1's binary_logloss: 0.258458\n",
      "[700]\ttraining's binary_logloss: 0.190179\tvalid_1's binary_logloss: 0.258009\n",
      "[800]\ttraining's binary_logloss: 0.182413\tvalid_1's binary_logloss: 0.257678\n",
      "[900]\ttraining's binary_logloss: 0.175349\tvalid_1's binary_logloss: 0.257502\n",
      "[1000]\ttraining's binary_logloss: 0.16899\tvalid_1's binary_logloss: 0.257607\n",
      "[1100]\ttraining's binary_logloss: 0.163067\tvalid_1's binary_logloss: 0.257743\n",
      "[1200]\ttraining's binary_logloss: 0.157653\tvalid_1's binary_logloss: 0.257818\n",
      "[1300]\ttraining's binary_logloss: 0.152515\tvalid_1's binary_logloss: 0.258017\n",
      "[1400]\ttraining's binary_logloss: 0.147918\tvalid_1's binary_logloss: 0.258332\n",
      "[1500]\ttraining's binary_logloss: 0.143379\tvalid_1's binary_logloss: 0.258503\n",
      "[1600]\ttraining's binary_logloss: 0.139299\tvalid_1's binary_logloss: 0.258662\n",
      "[1700]\ttraining's binary_logloss: 0.135565\tvalid_1's binary_logloss: 0.258946\n",
      "[1800]\ttraining's binary_logloss: 0.131929\tvalid_1's binary_logloss: 0.25943\n",
      "Early stopping, best iteration is:\n",
      "[883]\ttraining's binary_logloss: 0.176519\tvalid_1's binary_logloss: 0.257401\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49755, number of negative: 6795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 144636\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 600\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.879841 -> initscore=1.990924\n",
      "[LightGBM] [Info] Start training from score 1.990924\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.284321\tvalid_1's binary_logloss: 0.299806\n",
      "[200]\ttraining's binary_logloss: 0.250872\tvalid_1's binary_logloss: 0.278818\n",
      "[300]\ttraining's binary_logloss: 0.232331\tvalid_1's binary_logloss: 0.271196\n",
      "[400]\ttraining's binary_logloss: 0.218297\tvalid_1's binary_logloss: 0.267627\n",
      "[500]\ttraining's binary_logloss: 0.207099\tvalid_1's binary_logloss: 0.266101\n",
      "[600]\ttraining's binary_logloss: 0.19745\tvalid_1's binary_logloss: 0.265529\n",
      "[700]\ttraining's binary_logloss: 0.188914\tvalid_1's binary_logloss: 0.265311\n",
      "[800]\ttraining's binary_logloss: 0.181145\tvalid_1's binary_logloss: 0.265226\n",
      "[900]\ttraining's binary_logloss: 0.174088\tvalid_1's binary_logloss: 0.265311\n",
      "[1000]\ttraining's binary_logloss: 0.167722\tvalid_1's binary_logloss: 0.265461\n",
      "[1100]\ttraining's binary_logloss: 0.16189\tvalid_1's binary_logloss: 0.265636\n",
      "[1200]\ttraining's binary_logloss: 0.156524\tvalid_1's binary_logloss: 0.26575\n",
      "[1300]\ttraining's binary_logloss: 0.151395\tvalid_1's binary_logloss: 0.266118\n",
      "[1400]\ttraining's binary_logloss: 0.146688\tvalid_1's binary_logloss: 0.266441\n",
      "[1500]\ttraining's binary_logloss: 0.142312\tvalid_1's binary_logloss: 0.26674\n",
      "[1600]\ttraining's binary_logloss: 0.138261\tvalid_1's binary_logloss: 0.267159\n",
      "[1700]\ttraining's binary_logloss: 0.134409\tvalid_1's binary_logloss: 0.267507\n",
      "Early stopping, best iteration is:\n",
      "[767]\ttraining's binary_logloss: 0.183661\tvalid_1's binary_logloss: 0.265177\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 49804, number of negative: 6746\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 144607\n",
      "[LightGBM] [Info] Number of data points in the train set: 56550, number of used features: 600\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.880707 -> initscore=1.999146\n",
      "[LightGBM] [Info] Start training from score 1.999146\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.284541\tvalid_1's binary_logloss: 0.301327\n",
      "[200]\ttraining's binary_logloss: 0.251725\tvalid_1's binary_logloss: 0.277667\n",
      "[300]\ttraining's binary_logloss: 0.233448\tvalid_1's binary_logloss: 0.268422\n",
      "[400]\ttraining's binary_logloss: 0.219552\tvalid_1's binary_logloss: 0.263599\n",
      "[500]\ttraining's binary_logloss: 0.208374\tvalid_1's binary_logloss: 0.261591\n",
      "[600]\ttraining's binary_logloss: 0.198681\tvalid_1's binary_logloss: 0.260584\n",
      "[700]\ttraining's binary_logloss: 0.190043\tvalid_1's binary_logloss: 0.259884\n",
      "[800]\ttraining's binary_logloss: 0.182335\tvalid_1's binary_logloss: 0.259536\n",
      "[900]\ttraining's binary_logloss: 0.175322\tvalid_1's binary_logloss: 0.259358\n",
      "[1000]\ttraining's binary_logloss: 0.168963\tvalid_1's binary_logloss: 0.259286\n",
      "[1100]\ttraining's binary_logloss: 0.163133\tvalid_1's binary_logloss: 0.259423\n",
      "[1200]\ttraining's binary_logloss: 0.157673\tvalid_1's binary_logloss: 0.25952\n",
      "[1300]\ttraining's binary_logloss: 0.152628\tvalid_1's binary_logloss: 0.25965\n",
      "[1400]\ttraining's binary_logloss: 0.147952\tvalid_1's binary_logloss: 0.259802\n",
      "[1500]\ttraining's binary_logloss: 0.143551\tvalid_1's binary_logloss: 0.260038\n",
      "[1600]\ttraining's binary_logloss: 0.139284\tvalid_1's binary_logloss: 0.260297\n",
      "[1700]\ttraining's binary_logloss: 0.135535\tvalid_1's binary_logloss: 0.26066\n",
      "[1800]\ttraining's binary_logloss: 0.13198\tvalid_1's binary_logloss: 0.260968\n",
      "[1900]\ttraining's binary_logloss: 0.12852\tvalid_1's binary_logloss: 0.261403\n",
      "Early stopping, best iteration is:\n",
      "[963]\ttraining's binary_logloss: 0.171239\tvalid_1's binary_logloss: 0.259232\n",
      "5-12\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 23.900287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 26.129246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 28.997875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 51.122242 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 42.591633 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 34.015897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 43.602015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 36.425735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 34.919741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 42.654490 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 153132, number of negative: 82488\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 33.111885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2724057\n",
      "[LightGBM] [Info] Number of data points in the train set: 235620, number of used features: 12502\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649911 -> initscore=0.618647\n",
      "[LightGBM] [Info] Start training from score 0.618647\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122655, number of negative: 65835\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.395435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238191\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650724 -> initscore=0.622224\n",
      "[LightGBM] [Info] Start training from score 0.622224\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572818\tvalid_1's binary_logloss: 0.580269\n",
      "[200]\ttraining's binary_logloss: 0.538452\tvalid_1's binary_logloss: 0.551001\n",
      "[300]\ttraining's binary_logloss: 0.523106\tvalid_1's binary_logloss: 0.541197\n",
      "[400]\ttraining's binary_logloss: 0.512687\tvalid_1's binary_logloss: 0.536005\n",
      "[500]\ttraining's binary_logloss: 0.504839\tvalid_1's binary_logloss: 0.533311\n",
      "[600]\ttraining's binary_logloss: 0.498405\tvalid_1's binary_logloss: 0.531851\n",
      "[700]\ttraining's binary_logloss: 0.492742\tvalid_1's binary_logloss: 0.53093\n",
      "[800]\ttraining's binary_logloss: 0.487664\tvalid_1's binary_logloss: 0.530166\n",
      "[900]\ttraining's binary_logloss: 0.482964\tvalid_1's binary_logloss: 0.529649\n",
      "[1000]\ttraining's binary_logloss: 0.478702\tvalid_1's binary_logloss: 0.529362\n",
      "[1100]\ttraining's binary_logloss: 0.474762\tvalid_1's binary_logloss: 0.529105\n",
      "[1200]\ttraining's binary_logloss: 0.470993\tvalid_1's binary_logloss: 0.528908\n",
      "[1300]\ttraining's binary_logloss: 0.46747\tvalid_1's binary_logloss: 0.52874\n",
      "[1400]\ttraining's binary_logloss: 0.464136\tvalid_1's binary_logloss: 0.528543\n",
      "[1500]\ttraining's binary_logloss: 0.461053\tvalid_1's binary_logloss: 0.528378\n",
      "[1600]\ttraining's binary_logloss: 0.458177\tvalid_1's binary_logloss: 0.528329\n",
      "[1700]\ttraining's binary_logloss: 0.455309\tvalid_1's binary_logloss: 0.52818\n",
      "[1800]\ttraining's binary_logloss: 0.452647\tvalid_1's binary_logloss: 0.528106\n",
      "[1900]\ttraining's binary_logloss: 0.45015\tvalid_1's binary_logloss: 0.528055\n",
      "[2000]\ttraining's binary_logloss: 0.447752\tvalid_1's binary_logloss: 0.528007\n",
      "[2100]\ttraining's binary_logloss: 0.445285\tvalid_1's binary_logloss: 0.527951\n",
      "[2200]\ttraining's binary_logloss: 0.442843\tvalid_1's binary_logloss: 0.52787\n",
      "[2300]\ttraining's binary_logloss: 0.440642\tvalid_1's binary_logloss: 0.527864\n",
      "[2400]\ttraining's binary_logloss: 0.438482\tvalid_1's binary_logloss: 0.527838\n",
      "[2500]\ttraining's binary_logloss: 0.436558\tvalid_1's binary_logloss: 0.527858\n",
      "[2600]\ttraining's binary_logloss: 0.434406\tvalid_1's binary_logloss: 0.527861\n",
      "[2700]\ttraining's binary_logloss: 0.432465\tvalid_1's binary_logloss: 0.527872\n",
      "[2800]\ttraining's binary_logloss: 0.430474\tvalid_1's binary_logloss: 0.527842\n",
      "[2900]\ttraining's binary_logloss: 0.428617\tvalid_1's binary_logloss: 0.527893\n",
      "[3000]\ttraining's binary_logloss: 0.426822\tvalid_1's binary_logloss: 0.527883\n",
      "[3100]\ttraining's binary_logloss: 0.425075\tvalid_1's binary_logloss: 0.527948\n",
      "[3200]\ttraining's binary_logloss: 0.423397\tvalid_1's binary_logloss: 0.527931\n",
      "[3300]\ttraining's binary_logloss: 0.421505\tvalid_1's binary_logloss: 0.527988\n",
      "[3400]\ttraining's binary_logloss: 0.419722\tvalid_1's binary_logloss: 0.528062\n",
      "Early stopping, best iteration is:\n",
      "[2428]\ttraining's binary_logloss: 0.437908\tvalid_1's binary_logloss: 0.527819\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122423, number of negative: 66067\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.395882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238164\n",
      "[LightGBM] [Info] Number of data points in the train set: 188490, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.649493 -> initscore=0.616813\n",
      "[LightGBM] [Info] Start training from score 0.616813\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572758\tvalid_1's binary_logloss: 0.579596\n",
      "[200]\ttraining's binary_logloss: 0.538133\tvalid_1's binary_logloss: 0.551475\n",
      "[300]\ttraining's binary_logloss: 0.522683\tvalid_1's binary_logloss: 0.54191\n",
      "[400]\ttraining's binary_logloss: 0.512186\tvalid_1's binary_logloss: 0.537021\n",
      "[500]\ttraining's binary_logloss: 0.5044\tvalid_1's binary_logloss: 0.534606\n",
      "[600]\ttraining's binary_logloss: 0.498001\tvalid_1's binary_logloss: 0.533344\n",
      "[700]\ttraining's binary_logloss: 0.492351\tvalid_1's binary_logloss: 0.532537\n",
      "[800]\ttraining's binary_logloss: 0.487236\tvalid_1's binary_logloss: 0.531988\n",
      "[900]\ttraining's binary_logloss: 0.482527\tvalid_1's binary_logloss: 0.531647\n",
      "[1000]\ttraining's binary_logloss: 0.478255\tvalid_1's binary_logloss: 0.531398\n",
      "[1100]\ttraining's binary_logloss: 0.474201\tvalid_1's binary_logloss: 0.53119\n",
      "[1200]\ttraining's binary_logloss: 0.470391\tvalid_1's binary_logloss: 0.531035\n",
      "[1300]\ttraining's binary_logloss: 0.466866\tvalid_1's binary_logloss: 0.530887\n",
      "[1400]\ttraining's binary_logloss: 0.463502\tvalid_1's binary_logloss: 0.530808\n",
      "[1500]\ttraining's binary_logloss: 0.460415\tvalid_1's binary_logloss: 0.530753\n",
      "[1600]\ttraining's binary_logloss: 0.457525\tvalid_1's binary_logloss: 0.530674\n",
      "[1700]\ttraining's binary_logloss: 0.454695\tvalid_1's binary_logloss: 0.5307\n",
      "[1800]\ttraining's binary_logloss: 0.451944\tvalid_1's binary_logloss: 0.530724\n",
      "[1900]\ttraining's binary_logloss: 0.449374\tvalid_1's binary_logloss: 0.530707\n",
      "[2000]\ttraining's binary_logloss: 0.447008\tvalid_1's binary_logloss: 0.530676\n",
      "[2100]\ttraining's binary_logloss: 0.444505\tvalid_1's binary_logloss: 0.53068\n",
      "[2200]\ttraining's binary_logloss: 0.442073\tvalid_1's binary_logloss: 0.530629\n",
      "[2300]\ttraining's binary_logloss: 0.439872\tvalid_1's binary_logloss: 0.530642\n",
      "[2400]\ttraining's binary_logloss: 0.437681\tvalid_1's binary_logloss: 0.530663\n",
      "[2500]\ttraining's binary_logloss: 0.435662\tvalid_1's binary_logloss: 0.530638\n",
      "[2600]\ttraining's binary_logloss: 0.433475\tvalid_1's binary_logloss: 0.530641\n",
      "[2700]\ttraining's binary_logloss: 0.4315\tvalid_1's binary_logloss: 0.530665\n",
      "[2800]\ttraining's binary_logloss: 0.429514\tvalid_1's binary_logloss: 0.530661\n",
      "[2900]\ttraining's binary_logloss: 0.427671\tvalid_1's binary_logloss: 0.530741\n",
      "[3000]\ttraining's binary_logloss: 0.425804\tvalid_1's binary_logloss: 0.530798\n",
      "[3100]\ttraining's binary_logloss: 0.424111\tvalid_1's binary_logloss: 0.530858\n",
      "[3200]\ttraining's binary_logloss: 0.422441\tvalid_1's binary_logloss: 0.530941\n",
      "[3300]\ttraining's binary_logloss: 0.420647\tvalid_1's binary_logloss: 0.531031\n",
      "[3400]\ttraining's binary_logloss: 0.418894\tvalid_1's binary_logloss: 0.531126\n",
      "[3500]\ttraining's binary_logloss: 0.417124\tvalid_1's binary_logloss: 0.531133\n",
      "Early stopping, best iteration is:\n",
      "[2547]\ttraining's binary_logloss: 0.434609\tvalid_1's binary_logloss: 0.530607\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122249, number of negative: 66251\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.404802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238223\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.648536 -> initscore=0.612609\n",
      "[LightGBM] [Info] Start training from score 0.612609\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.573278\tvalid_1's binary_logloss: 0.577627\n",
      "[200]\ttraining's binary_logloss: 0.538547\tvalid_1's binary_logloss: 0.549511\n",
      "[300]\ttraining's binary_logloss: 0.523095\tvalid_1's binary_logloss: 0.540359\n",
      "[400]\ttraining's binary_logloss: 0.512563\tvalid_1's binary_logloss: 0.535551\n",
      "[500]\ttraining's binary_logloss: 0.504735\tvalid_1's binary_logloss: 0.533072\n",
      "[600]\ttraining's binary_logloss: 0.498325\tvalid_1's binary_logloss: 0.531836\n",
      "[700]\ttraining's binary_logloss: 0.492778\tvalid_1's binary_logloss: 0.531206\n",
      "[800]\ttraining's binary_logloss: 0.487672\tvalid_1's binary_logloss: 0.530821\n",
      "[900]\ttraining's binary_logloss: 0.483022\tvalid_1's binary_logloss: 0.530509\n",
      "[1000]\ttraining's binary_logloss: 0.478703\tvalid_1's binary_logloss: 0.530233\n",
      "[1100]\ttraining's binary_logloss: 0.474741\tvalid_1's binary_logloss: 0.5301\n",
      "[1200]\ttraining's binary_logloss: 0.471007\tvalid_1's binary_logloss: 0.529867\n",
      "[1300]\ttraining's binary_logloss: 0.467499\tvalid_1's binary_logloss: 0.529771\n",
      "[1400]\ttraining's binary_logloss: 0.464208\tvalid_1's binary_logloss: 0.529658\n",
      "[1500]\ttraining's binary_logloss: 0.461207\tvalid_1's binary_logloss: 0.529577\n",
      "[1600]\ttraining's binary_logloss: 0.458295\tvalid_1's binary_logloss: 0.529547\n",
      "[1700]\ttraining's binary_logloss: 0.455451\tvalid_1's binary_logloss: 0.529488\n",
      "[1800]\ttraining's binary_logloss: 0.45275\tvalid_1's binary_logloss: 0.529481\n",
      "[1900]\ttraining's binary_logloss: 0.450163\tvalid_1's binary_logloss: 0.529481\n",
      "[2000]\ttraining's binary_logloss: 0.447803\tvalid_1's binary_logloss: 0.529457\n",
      "[2100]\ttraining's binary_logloss: 0.445337\tvalid_1's binary_logloss: 0.529437\n",
      "[2200]\ttraining's binary_logloss: 0.443013\tvalid_1's binary_logloss: 0.529407\n",
      "[2300]\ttraining's binary_logloss: 0.440866\tvalid_1's binary_logloss: 0.529431\n",
      "[2400]\ttraining's binary_logloss: 0.438601\tvalid_1's binary_logloss: 0.529389\n",
      "[2500]\ttraining's binary_logloss: 0.436651\tvalid_1's binary_logloss: 0.52947\n",
      "[2600]\ttraining's binary_logloss: 0.434529\tvalid_1's binary_logloss: 0.529486\n",
      "[2700]\ttraining's binary_logloss: 0.43259\tvalid_1's binary_logloss: 0.529542\n",
      "[2800]\ttraining's binary_logloss: 0.430681\tvalid_1's binary_logloss: 0.529565\n",
      "[2900]\ttraining's binary_logloss: 0.428884\tvalid_1's binary_logloss: 0.529577\n",
      "[3000]\ttraining's binary_logloss: 0.426997\tvalid_1's binary_logloss: 0.529645\n",
      "[3100]\ttraining's binary_logloss: 0.425363\tvalid_1's binary_logloss: 0.529743\n",
      "[3200]\ttraining's binary_logloss: 0.423715\tvalid_1's binary_logloss: 0.529697\n",
      "[3300]\ttraining's binary_logloss: 0.421841\tvalid_1's binary_logloss: 0.529717\n",
      "Early stopping, best iteration is:\n",
      "[2396]\ttraining's binary_logloss: 0.438701\tvalid_1's binary_logloss: 0.529382\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122566, number of negative: 65934\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.419441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238299\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650218 -> initscore=0.619995\n",
      "[LightGBM] [Info] Start training from score 0.619995\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572904\tvalid_1's binary_logloss: 0.5801\n",
      "[200]\ttraining's binary_logloss: 0.53851\tvalid_1's binary_logloss: 0.550644\n",
      "[300]\ttraining's binary_logloss: 0.523239\tvalid_1's binary_logloss: 0.54081\n",
      "[400]\ttraining's binary_logloss: 0.51278\tvalid_1's binary_logloss: 0.535377\n",
      "[500]\ttraining's binary_logloss: 0.505001\tvalid_1's binary_logloss: 0.532646\n",
      "[600]\ttraining's binary_logloss: 0.498584\tvalid_1's binary_logloss: 0.531135\n",
      "[700]\ttraining's binary_logloss: 0.492969\tvalid_1's binary_logloss: 0.530195\n",
      "[800]\ttraining's binary_logloss: 0.487857\tvalid_1's binary_logloss: 0.529524\n",
      "[900]\ttraining's binary_logloss: 0.483231\tvalid_1's binary_logloss: 0.529121\n",
      "[1000]\ttraining's binary_logloss: 0.478943\tvalid_1's binary_logloss: 0.528895\n",
      "[1100]\ttraining's binary_logloss: 0.474979\tvalid_1's binary_logloss: 0.528696\n",
      "[1200]\ttraining's binary_logloss: 0.471231\tvalid_1's binary_logloss: 0.528459\n",
      "[1300]\ttraining's binary_logloss: 0.467731\tvalid_1's binary_logloss: 0.528281\n",
      "[1400]\ttraining's binary_logloss: 0.46443\tvalid_1's binary_logloss: 0.528174\n",
      "[1500]\ttraining's binary_logloss: 0.461317\tvalid_1's binary_logloss: 0.528116\n",
      "[1600]\ttraining's binary_logloss: 0.458399\tvalid_1's binary_logloss: 0.528056\n",
      "[1700]\ttraining's binary_logloss: 0.455635\tvalid_1's binary_logloss: 0.527976\n",
      "[1800]\ttraining's binary_logloss: 0.452951\tvalid_1's binary_logloss: 0.5279\n",
      "[1900]\ttraining's binary_logloss: 0.450365\tvalid_1's binary_logloss: 0.527887\n",
      "[2000]\ttraining's binary_logloss: 0.447994\tvalid_1's binary_logloss: 0.527877\n",
      "[2100]\ttraining's binary_logloss: 0.445577\tvalid_1's binary_logloss: 0.52791\n",
      "[2200]\ttraining's binary_logloss: 0.443125\tvalid_1's binary_logloss: 0.527848\n",
      "[2300]\ttraining's binary_logloss: 0.440882\tvalid_1's binary_logloss: 0.527922\n",
      "[2400]\ttraining's binary_logloss: 0.438759\tvalid_1's binary_logloss: 0.527892\n",
      "[2500]\ttraining's binary_logloss: 0.436742\tvalid_1's binary_logloss: 0.527872\n",
      "[2600]\ttraining's binary_logloss: 0.434593\tvalid_1's binary_logloss: 0.527848\n",
      "[2700]\ttraining's binary_logloss: 0.432679\tvalid_1's binary_logloss: 0.527876\n",
      "[2800]\ttraining's binary_logloss: 0.430728\tvalid_1's binary_logloss: 0.527896\n",
      "[2900]\ttraining's binary_logloss: 0.428882\tvalid_1's binary_logloss: 0.527985\n",
      "[3000]\ttraining's binary_logloss: 0.427057\tvalid_1's binary_logloss: 0.528004\n",
      "[3100]\ttraining's binary_logloss: 0.425356\tvalid_1's binary_logloss: 0.527957\n",
      "[3200]\ttraining's binary_logloss: 0.423709\tvalid_1's binary_logloss: 0.527993\n",
      "[3300]\ttraining's binary_logloss: 0.42183\tvalid_1's binary_logloss: 0.527981\n",
      "[3400]\ttraining's binary_logloss: 0.420133\tvalid_1's binary_logloss: 0.52801\n",
      "[3500]\ttraining's binary_logloss: 0.418324\tvalid_1's binary_logloss: 0.528036\n",
      "[3600]\ttraining's binary_logloss: 0.416698\tvalid_1's binary_logloss: 0.528074\n",
      "Early stopping, best iteration is:\n",
      "[2663]\ttraining's binary_logloss: 0.433398\tvalid_1's binary_logloss: 0.52783\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 122635, number of negative: 65865\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.400551 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238151\n",
      "[LightGBM] [Info] Number of data points in the train set: 188500, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.650584 -> initscore=0.621605\n",
      "[LightGBM] [Info] Start training from score 0.621605\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.572386\tvalid_1's binary_logloss: 0.581615\n",
      "[200]\ttraining's binary_logloss: 0.537941\tvalid_1's binary_logloss: 0.552549\n",
      "[300]\ttraining's binary_logloss: 0.522652\tvalid_1's binary_logloss: 0.542715\n",
      "[400]\ttraining's binary_logloss: 0.512235\tvalid_1's binary_logloss: 0.53749\n",
      "[500]\ttraining's binary_logloss: 0.504473\tvalid_1's binary_logloss: 0.534663\n",
      "[600]\ttraining's binary_logloss: 0.498021\tvalid_1's binary_logloss: 0.533105\n",
      "[700]\ttraining's binary_logloss: 0.492346\tvalid_1's binary_logloss: 0.532193\n",
      "[800]\ttraining's binary_logloss: 0.487256\tvalid_1's binary_logloss: 0.531601\n",
      "[900]\ttraining's binary_logloss: 0.482564\tvalid_1's binary_logloss: 0.531208\n",
      "[1000]\ttraining's binary_logloss: 0.478291\tvalid_1's binary_logloss: 0.530917\n",
      "[1100]\ttraining's binary_logloss: 0.4743\tvalid_1's binary_logloss: 0.530725\n",
      "[1200]\ttraining's binary_logloss: 0.470561\tvalid_1's binary_logloss: 0.530549\n",
      "[1300]\ttraining's binary_logloss: 0.467055\tvalid_1's binary_logloss: 0.530384\n",
      "[1400]\ttraining's binary_logloss: 0.463735\tvalid_1's binary_logloss: 0.530247\n",
      "[1500]\ttraining's binary_logloss: 0.460618\tvalid_1's binary_logloss: 0.530165\n",
      "[1600]\ttraining's binary_logloss: 0.457724\tvalid_1's binary_logloss: 0.530115\n",
      "[1700]\ttraining's binary_logloss: 0.454919\tvalid_1's binary_logloss: 0.530077\n",
      "[1800]\ttraining's binary_logloss: 0.452213\tvalid_1's binary_logloss: 0.529986\n",
      "[1900]\ttraining's binary_logloss: 0.449648\tvalid_1's binary_logloss: 0.529984\n",
      "[2000]\ttraining's binary_logloss: 0.447309\tvalid_1's binary_logloss: 0.529995\n",
      "[2100]\ttraining's binary_logloss: 0.444858\tvalid_1's binary_logloss: 0.529931\n",
      "[2200]\ttraining's binary_logloss: 0.442384\tvalid_1's binary_logloss: 0.529864\n",
      "[2300]\ttraining's binary_logloss: 0.440205\tvalid_1's binary_logloss: 0.529903\n",
      "[2400]\ttraining's binary_logloss: 0.43811\tvalid_1's binary_logloss: 0.52993\n",
      "[2500]\ttraining's binary_logloss: 0.43616\tvalid_1's binary_logloss: 0.529914\n",
      "[2600]\ttraining's binary_logloss: 0.43406\tvalid_1's binary_logloss: 0.529862\n",
      "[2700]\ttraining's binary_logloss: 0.432138\tvalid_1's binary_logloss: 0.529913\n",
      "[2800]\ttraining's binary_logloss: 0.430118\tvalid_1's binary_logloss: 0.529895\n",
      "[2900]\ttraining's binary_logloss: 0.428383\tvalid_1's binary_logloss: 0.529974\n",
      "[3000]\ttraining's binary_logloss: 0.426562\tvalid_1's binary_logloss: 0.530005\n",
      "[3100]\ttraining's binary_logloss: 0.424843\tvalid_1's binary_logloss: 0.530032\n",
      "[3200]\ttraining's binary_logloss: 0.423225\tvalid_1's binary_logloss: 0.530038\n",
      "[3300]\ttraining's binary_logloss: 0.421396\tvalid_1's binary_logloss: 0.530044\n",
      "[3400]\ttraining's binary_logloss: 0.419633\tvalid_1's binary_logloss: 0.530014\n",
      "[3500]\ttraining's binary_logloss: 0.417873\tvalid_1's binary_logloss: 0.530042\n",
      "[3600]\ttraining's binary_logloss: 0.416212\tvalid_1's binary_logloss: 0.530129\n",
      "Early stopping, best iteration is:\n",
      "[2605]\ttraining's binary_logloss: 0.433969\tvalid_1's binary_logloss: 0.529853\n",
      "13-22\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 23.524872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 23.357196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 25.834606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 26.496931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 24.889932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 25.184305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 22.719347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 25.204406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 29.000024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 25.533137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 83928, number of negative: 33882\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 24.543461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5230661\n",
      "[LightGBM] [Info] Number of data points in the train set: 117810, number of used features: 23764\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.712401 -> initscore=0.907075\n",
      "[LightGBM] [Info] Start training from score 0.907075\n",
      "fold : 0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67313, number of negative: 26932\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.662742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238315\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.714234 -> initscore=0.916038\n",
      "[LightGBM] [Info] Start training from score 0.916038\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.530231\tvalid_1's binary_logloss: 0.545652\n",
      "[200]\ttraining's binary_logloss: 0.501283\tvalid_1's binary_logloss: 0.524367\n",
      "[300]\ttraining's binary_logloss: 0.481343\tvalid_1's binary_logloss: 0.512244\n",
      "[400]\ttraining's binary_logloss: 0.468453\tvalid_1's binary_logloss: 0.507303\n",
      "[500]\ttraining's binary_logloss: 0.45836\tvalid_1's binary_logloss: 0.504689\n",
      "[600]\ttraining's binary_logloss: 0.449807\tvalid_1's binary_logloss: 0.503435\n",
      "[700]\ttraining's binary_logloss: 0.441899\tvalid_1's binary_logloss: 0.502638\n",
      "[800]\ttraining's binary_logloss: 0.434799\tvalid_1's binary_logloss: 0.502177\n",
      "[900]\ttraining's binary_logloss: 0.428253\tvalid_1's binary_logloss: 0.501864\n",
      "[1000]\ttraining's binary_logloss: 0.422219\tvalid_1's binary_logloss: 0.501711\n",
      "[1100]\ttraining's binary_logloss: 0.416723\tvalid_1's binary_logloss: 0.501613\n",
      "[1200]\ttraining's binary_logloss: 0.411603\tvalid_1's binary_logloss: 0.501519\n",
      "[1300]\ttraining's binary_logloss: 0.40689\tvalid_1's binary_logloss: 0.501512\n",
      "[1400]\ttraining's binary_logloss: 0.402284\tvalid_1's binary_logloss: 0.501651\n",
      "[1500]\ttraining's binary_logloss: 0.397968\tvalid_1's binary_logloss: 0.501599\n",
      "[1600]\ttraining's binary_logloss: 0.393888\tvalid_1's binary_logloss: 0.501497\n",
      "[1700]\ttraining's binary_logloss: 0.390025\tvalid_1's binary_logloss: 0.501528\n",
      "[1800]\ttraining's binary_logloss: 0.38607\tvalid_1's binary_logloss: 0.50172\n",
      "[1900]\ttraining's binary_logloss: 0.382517\tvalid_1's binary_logloss: 0.501761\n",
      "[2000]\ttraining's binary_logloss: 0.379048\tvalid_1's binary_logloss: 0.501821\n",
      "[2100]\ttraining's binary_logloss: 0.375685\tvalid_1's binary_logloss: 0.501864\n",
      "[2200]\ttraining's binary_logloss: 0.372391\tvalid_1's binary_logloss: 0.501907\n",
      "[2300]\ttraining's binary_logloss: 0.369277\tvalid_1's binary_logloss: 0.501954\n",
      "[2400]\ttraining's binary_logloss: 0.36642\tvalid_1's binary_logloss: 0.50204\n",
      "[2500]\ttraining's binary_logloss: 0.363745\tvalid_1's binary_logloss: 0.502195\n",
      "[2600]\ttraining's binary_logloss: 0.360879\tvalid_1's binary_logloss: 0.502396\n",
      "Early stopping, best iteration is:\n",
      "[1626]\ttraining's binary_logloss: 0.392878\tvalid_1's binary_logloss: 0.501473\n",
      "fold : 1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 66951, number of negative: 27294\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 4.997920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238346\n",
      "[LightGBM] [Info] Number of data points in the train set: 94245, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710393 -> initscore=0.897294\n",
      "[LightGBM] [Info] Start training from score 0.897294\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.53289\tvalid_1's binary_logloss: 0.534695\n",
      "[200]\ttraining's binary_logloss: 0.503736\tvalid_1's binary_logloss: 0.514501\n",
      "[300]\ttraining's binary_logloss: 0.483531\tvalid_1's binary_logloss: 0.502887\n",
      "[400]\ttraining's binary_logloss: 0.470745\tvalid_1's binary_logloss: 0.498259\n",
      "[500]\ttraining's binary_logloss: 0.460517\tvalid_1's binary_logloss: 0.495816\n",
      "[600]\ttraining's binary_logloss: 0.452085\tvalid_1's binary_logloss: 0.494599\n",
      "[700]\ttraining's binary_logloss: 0.4441\tvalid_1's binary_logloss: 0.493667\n",
      "[800]\ttraining's binary_logloss: 0.437001\tvalid_1's binary_logloss: 0.493241\n",
      "[900]\ttraining's binary_logloss: 0.430496\tvalid_1's binary_logloss: 0.492924\n",
      "[1000]\ttraining's binary_logloss: 0.424476\tvalid_1's binary_logloss: 0.492713\n",
      "[1100]\ttraining's binary_logloss: 0.418821\tvalid_1's binary_logloss: 0.492619\n",
      "[1200]\ttraining's binary_logloss: 0.413606\tvalid_1's binary_logloss: 0.492501\n",
      "[1300]\ttraining's binary_logloss: 0.408742\tvalid_1's binary_logloss: 0.492456\n",
      "[1400]\ttraining's binary_logloss: 0.404112\tvalid_1's binary_logloss: 0.492425\n",
      "[1500]\ttraining's binary_logloss: 0.399841\tvalid_1's binary_logloss: 0.492362\n",
      "[1600]\ttraining's binary_logloss: 0.395783\tvalid_1's binary_logloss: 0.492412\n",
      "[1700]\ttraining's binary_logloss: 0.391854\tvalid_1's binary_logloss: 0.492411\n",
      "[1800]\ttraining's binary_logloss: 0.388021\tvalid_1's binary_logloss: 0.492371\n",
      "[1900]\ttraining's binary_logloss: 0.384409\tvalid_1's binary_logloss: 0.492441\n",
      "[2000]\ttraining's binary_logloss: 0.380995\tvalid_1's binary_logloss: 0.49257\n",
      "[2100]\ttraining's binary_logloss: 0.377724\tvalid_1's binary_logloss: 0.492662\n",
      "[2200]\ttraining's binary_logloss: 0.374323\tvalid_1's binary_logloss: 0.492663\n",
      "[2300]\ttraining's binary_logloss: 0.371138\tvalid_1's binary_logloss: 0.492619\n",
      "[2400]\ttraining's binary_logloss: 0.368193\tvalid_1's binary_logloss: 0.492684\n",
      "[2500]\ttraining's binary_logloss: 0.365569\tvalid_1's binary_logloss: 0.492809\n",
      "Early stopping, best iteration is:\n",
      "[1509]\ttraining's binary_logloss: 0.399458\tvalid_1's binary_logloss: 0.492314\n",
      "fold : 2\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 66989, number of negative: 27261\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.522591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238327\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.710759 -> initscore=0.899071\n",
      "[LightGBM] [Info] Start training from score 0.899071\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.53289\tvalid_1's binary_logloss: 0.536004\n",
      "[200]\ttraining's binary_logloss: 0.503723\tvalid_1's binary_logloss: 0.515678\n",
      "[300]\ttraining's binary_logloss: 0.483615\tvalid_1's binary_logloss: 0.504314\n",
      "[400]\ttraining's binary_logloss: 0.470785\tvalid_1's binary_logloss: 0.49968\n",
      "[500]\ttraining's binary_logloss: 0.460596\tvalid_1's binary_logloss: 0.497106\n",
      "[600]\ttraining's binary_logloss: 0.452116\tvalid_1's binary_logloss: 0.495981\n",
      "[700]\ttraining's binary_logloss: 0.444104\tvalid_1's binary_logloss: 0.495173\n",
      "[800]\ttraining's binary_logloss: 0.43697\tvalid_1's binary_logloss: 0.494745\n",
      "[900]\ttraining's binary_logloss: 0.430302\tvalid_1's binary_logloss: 0.494415\n",
      "[1000]\ttraining's binary_logloss: 0.424328\tvalid_1's binary_logloss: 0.494258\n",
      "[1100]\ttraining's binary_logloss: 0.418782\tvalid_1's binary_logloss: 0.494072\n",
      "[1200]\ttraining's binary_logloss: 0.413624\tvalid_1's binary_logloss: 0.494099\n",
      "[1300]\ttraining's binary_logloss: 0.408822\tvalid_1's binary_logloss: 0.494072\n",
      "[1400]\ttraining's binary_logloss: 0.404278\tvalid_1's binary_logloss: 0.493936\n",
      "[1500]\ttraining's binary_logloss: 0.400002\tvalid_1's binary_logloss: 0.493919\n",
      "[1600]\ttraining's binary_logloss: 0.395937\tvalid_1's binary_logloss: 0.493961\n",
      "[1700]\ttraining's binary_logloss: 0.391905\tvalid_1's binary_logloss: 0.493926\n",
      "[1800]\ttraining's binary_logloss: 0.388045\tvalid_1's binary_logloss: 0.493877\n",
      "[1900]\ttraining's binary_logloss: 0.384494\tvalid_1's binary_logloss: 0.493936\n",
      "[2000]\ttraining's binary_logloss: 0.380797\tvalid_1's binary_logloss: 0.493941\n",
      "[2100]\ttraining's binary_logloss: 0.377582\tvalid_1's binary_logloss: 0.493944\n",
      "[2200]\ttraining's binary_logloss: 0.374357\tvalid_1's binary_logloss: 0.493938\n",
      "[2300]\ttraining's binary_logloss: 0.371271\tvalid_1's binary_logloss: 0.493926\n",
      "[2400]\ttraining's binary_logloss: 0.368353\tvalid_1's binary_logloss: 0.494039\n",
      "[2500]\ttraining's binary_logloss: 0.365653\tvalid_1's binary_logloss: 0.494091\n",
      "[2600]\ttraining's binary_logloss: 0.362766\tvalid_1's binary_logloss: 0.494104\n",
      "[2700]\ttraining's binary_logloss: 0.360113\tvalid_1's binary_logloss: 0.494129\n",
      "[2800]\ttraining's binary_logloss: 0.357578\tvalid_1's binary_logloss: 0.494168\n",
      "[2900]\ttraining's binary_logloss: 0.35496\tvalid_1's binary_logloss: 0.494265\n",
      "Early stopping, best iteration is:\n",
      "[1958]\ttraining's binary_logloss: 0.382283\tvalid_1's binary_logloss: 0.493858\n",
      "fold : 3\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67214, number of negative: 27036\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.578720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238370\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.713146 -> initscore=0.910712\n",
      "[LightGBM] [Info] Start training from score 0.910712\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.530469\tvalid_1's binary_logloss: 0.543464\n",
      "[200]\ttraining's binary_logloss: 0.501359\tvalid_1's binary_logloss: 0.523211\n",
      "[300]\ttraining's binary_logloss: 0.481183\tvalid_1's binary_logloss: 0.511497\n",
      "[400]\ttraining's binary_logloss: 0.468323\tvalid_1's binary_logloss: 0.506816\n",
      "[500]\ttraining's binary_logloss: 0.458154\tvalid_1's binary_logloss: 0.50472\n",
      "[600]\ttraining's binary_logloss: 0.449604\tvalid_1's binary_logloss: 0.503667\n",
      "[700]\ttraining's binary_logloss: 0.441702\tvalid_1's binary_logloss: 0.502888\n",
      "[800]\ttraining's binary_logloss: 0.434677\tvalid_1's binary_logloss: 0.502553\n",
      "[900]\ttraining's binary_logloss: 0.428206\tvalid_1's binary_logloss: 0.502333\n",
      "[1000]\ttraining's binary_logloss: 0.422199\tvalid_1's binary_logloss: 0.502153\n",
      "[1100]\ttraining's binary_logloss: 0.416738\tvalid_1's binary_logloss: 0.502164\n",
      "[1200]\ttraining's binary_logloss: 0.411599\tvalid_1's binary_logloss: 0.502176\n",
      "[1300]\ttraining's binary_logloss: 0.406689\tvalid_1's binary_logloss: 0.502126\n",
      "[1400]\ttraining's binary_logloss: 0.402189\tvalid_1's binary_logloss: 0.502147\n",
      "[1500]\ttraining's binary_logloss: 0.397941\tvalid_1's binary_logloss: 0.502244\n",
      "[1600]\ttraining's binary_logloss: 0.393782\tvalid_1's binary_logloss: 0.502335\n",
      "[1700]\ttraining's binary_logloss: 0.389664\tvalid_1's binary_logloss: 0.50235\n",
      "[1800]\ttraining's binary_logloss: 0.385795\tvalid_1's binary_logloss: 0.502487\n",
      "[1900]\ttraining's binary_logloss: 0.382286\tvalid_1's binary_logloss: 0.50251\n",
      "[2000]\ttraining's binary_logloss: 0.378801\tvalid_1's binary_logloss: 0.50251\n",
      "Early stopping, best iteration is:\n",
      "[1046]\ttraining's binary_logloss: 0.419556\tvalid_1's binary_logloss: 0.502048\n",
      "fold : 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] Number of positive: 67245, number of negative: 27005\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.546360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 238331\n",
      "[LightGBM] [Info] Number of data points in the train set: 94250, number of used features: 1000\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=100 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.713475 -> initscore=0.912321\n",
      "[LightGBM] [Info] Start training from score 0.912321\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.530522\tvalid_1's binary_logloss: 0.54464\n",
      "[200]\ttraining's binary_logloss: 0.501537\tvalid_1's binary_logloss: 0.523823\n",
      "[300]\ttraining's binary_logloss: 0.481484\tvalid_1's binary_logloss: 0.511786\n",
      "[400]\ttraining's binary_logloss: 0.468723\tvalid_1's binary_logloss: 0.506871\n",
      "[500]\ttraining's binary_logloss: 0.458573\tvalid_1's binary_logloss: 0.504332\n",
      "[600]\ttraining's binary_logloss: 0.450013\tvalid_1's binary_logloss: 0.502965\n",
      "[700]\ttraining's binary_logloss: 0.4422\tvalid_1's binary_logloss: 0.501968\n",
      "[800]\ttraining's binary_logloss: 0.435164\tvalid_1's binary_logloss: 0.501518\n",
      "[900]\ttraining's binary_logloss: 0.428523\tvalid_1's binary_logloss: 0.501142\n",
      "[1000]\ttraining's binary_logloss: 0.422674\tvalid_1's binary_logloss: 0.500992\n",
      "[1100]\ttraining's binary_logloss: 0.417072\tvalid_1's binary_logloss: 0.500728\n",
      "[1200]\ttraining's binary_logloss: 0.412017\tvalid_1's binary_logloss: 0.500693\n",
      "[1300]\ttraining's binary_logloss: 0.407185\tvalid_1's binary_logloss: 0.50062\n",
      "[1400]\ttraining's binary_logloss: 0.402562\tvalid_1's binary_logloss: 0.500538\n",
      "[1500]\ttraining's binary_logloss: 0.398397\tvalid_1's binary_logloss: 0.500522\n",
      "[1600]\ttraining's binary_logloss: 0.394338\tvalid_1's binary_logloss: 0.500513\n",
      "[1700]\ttraining's binary_logloss: 0.390447\tvalid_1's binary_logloss: 0.50052\n",
      "[1800]\ttraining's binary_logloss: 0.386609\tvalid_1's binary_logloss: 0.50058\n",
      "[1900]\ttraining's binary_logloss: 0.383118\tvalid_1's binary_logloss: 0.500637\n",
      "[2000]\ttraining's binary_logloss: 0.379633\tvalid_1's binary_logloss: 0.500691\n",
      "[2100]\ttraining's binary_logloss: 0.376279\tvalid_1's binary_logloss: 0.500665\n",
      "[2200]\ttraining's binary_logloss: 0.372934\tvalid_1's binary_logloss: 0.500628\n",
      "[2300]\ttraining's binary_logloss: 0.369939\tvalid_1's binary_logloss: 0.500684\n",
      "[2400]\ttraining's binary_logloss: 0.36718\tvalid_1's binary_logloss: 0.500795\n",
      "Early stopping, best iteration is:\n",
      "[1436]\ttraining's binary_logloss: 0.400987\tvalid_1's binary_logloss: 0.500473\n",
      "logloss 0.475834\n",
      "best_score 0.701523\n",
      "best_threshold 0.620\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mコードを実行できません。セッションは破棄されました。カーネルを再起動してください。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "if cfg.mode == \"local_cv\":\n",
    "    valid_train_test_process_identity()\n",
    "    run_train()\n",
    "inference(cfg.mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
